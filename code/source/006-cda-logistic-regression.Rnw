% !Rnw root = master.Rnw




<<cache=FALSE, echo=FALSE>>=
## trying this out to use num signif digits (which is what
## tidy print does by default) instead of decimal
options(knitr.digits.signif = TRUE)
options(digits = 2)
@


\hypertarget{cda-logistic-regression}{%
\chapter{Categorical data analysis and logistic regression}\label{chap:cda-logistic-regression}}

So far we have mostly considered data analysis where the outcome is a continuous variable, like reaction time or vowel duration. In this chapter we turn to \emph{categorical data analysis}, where the outcome is a categorical variable (Section~\ref{sec:cda-basics}--\ref{sec:odds-ors}), focusing on \emph{logistic regression}, which predicts a binary outcome (Section~\ref{sec:simple-logistic-regression}--\ref{sec:log-reg-reporting}).

\section{Preliminaries}

\subsection{Packages}

This chapter assumes that you have loaded several packages from previous chapters, as well as the 
%\ttt{performance},
%\ttt{caret}, 
{ModelMetrics} and {margins} packages \citep{ModelMetrics,margins}:
%hence the `ar' in their names.}
%a text on \textbf{a}pplied \textbf{r}egression \textbf{m}odeling which we will often refer to.}

<<message=F, error=F, warning=F, echo=1:13, cache=FALSE>>=
library(tidyverse)
library(broom)
library(languageR)
library(arm)
library(sjPlot)
library(performance)
library(ggeffects)
library(ModelMetrics)
library(margins)
library(rms)

## ensures that `rescale' is not the version from the `scales' package
rescale <- arm::rescale
library(patchwork)
@

\subsection{Data}

\label{sec:diatones-dataset}

\subsubsection*{The \ttt{diatones} dataset}

We also assume that you have loaded the \ttt{diatones} dataset:

<<>>=
diatones <- read.csv("data/diatones_rmld.csv", stringsAsFactors = TRUE)
@

The English lexicon contains many pairs of nouns and verbs which are spelled the same, such as `protest' and `decay'. 
%These pairs are interesting first, because
Some of these pairs are pronounced identically and some differ only in the placement of primary stress (e.g., initial/final-stressed `protest' = noun/verb).
%so these pairs are the only part of the English lexicon where word stress is contrastive
%second,  
The placement of stress for some words has changed over time, typically by the noun changing from final-stressed %(`oxytonic')
to initial-stressed. Pairs with initial stress on the noun and final stress on the verb are called `diatones'.
%(`diatonic').

This dataset, analyzed in \citet{sonderegger2010tfs}
and described in more detail there,
%Section~\ref{sec:diatones-data}, 
contains information about \Sexpr{nrow(diatones)}  pairs of disyllabic (two-syllable) noun/verb pairs, all of which had final-stressed noun and verb in British English in 1700 (e.g., `protest' noun/verb were pronounced `proTEST').  For a subset of noun/verb pairs (column \ttt{stress\_shifted}), the noun's stress stress shifted by 2005 (e.g., `PROtest' noun).  (We call noun/verb pairs `words' from here on for simplicity.)  


Several variables capture each word's \textbf{frequency} and aspects of its phonological \textbf{structure}, which might condition stress shift based on previous work:
\begin{itemize}
\item \ttt{frequency}: the word's frequency (log-transformed, from a large corpus)
\item \ttt{syll1\_coda} (levels \tsc{no}, \tsc{yes}): whether the first syllable ends in a vowel or a consonant (e.g., `debate', `disdain')
%\footnote{\tsc{yes} means a C coda for all words except one (`exchange').}
\item \ttt{syll2\_coda} (levels \tsc{0}, \tsc{c}, \tsc{cc}, \tsc{ccc}): whether the second syllable ends in a vowel or in 1--3 consonants
\item \ttt{syll2\_coda\_td} (levels \tsc{no}, \tsc{yes}): whether the second syllable ends in /t/ or /d/
\end{itemize}

Frequency and  structure are representative of two broad sources of pronunciation change (roughly, how a word is used vs. represented in the mind),  so it is of interest whether a word's likelihood of shifting stress is predicted by each, and their relative contributions.

% 
% \textbf{frequency} (column \ttt{frequency}) and its phonological \textbf{structure}, coded in three variables:


%which we simply call `words'.  The 

For simplicity, in this chapter we code all the `structure' factors as numeric, while renaming the original variables:

<<>>=
diatones <- diatones %>% mutate(
  syll1_coda_orig = syll1_coda,
  syll2_coda_orig = syll2_coda,
  syll2_td_orig = syll2_td,
  ## Change no/yes -> 0/1
  syll1_coda = ifelse(syll1_coda == "no", 0, 1),
  ## Change '0'/'C'/'CC'/'CCC' -> 0/1/2/3
  syll2_coda = str_count(syll2_coda_orig, "C"),
  syll2_td = ifelse(syll2_td == "no", 0, 1),
)
@

<<echo=FALSE>>=
diatones_old <- diatones
@


Coding \ttt{syll1\_coda} and \ttt{syll2\_td} as 0 and 1 is the same as we have previously done for  two-level factors (e.g., Section~\ref{sec:centering-scaling}). Coding \ttt{syll2\_coda} as numeric makes some sense because its levels have a clear order (number of consonants), and we haven't covered multi-level factors yet.

Figure~\ref{fig:diatones-empirical} shows the proportion of words with shifted stress as a function of each variable.
%noun/verb pairs with shifted stress as a function of each predictor.  

<<diatones-empirical, echo=FALSE, out.width='40%', fig.width=default_fig.width*.40/default_out.width, fig.cap='Proportion of words with shifted stress as a function of each predictor in the \\ttt{diatones} dataset.  Points are individual observations (at $y=0, 1$); dots/errorbars are means and exact 95\\% CIs; and the line/shading are predictions and 95\\% CIs from simple logistic regression.'>>=
## needed to calculate exact binomial confidence intervals
library(binom)

diatones %>% ggplot(aes(x = frequency, y = stress_shifted)) +
  geom_jitter(size = 0.75, width = 0.1, height = 0.02, alpha = .5) +
  geom_smooth(color = default_line_color, method = "glm", method.args = list(family = "binomial")) +
  xlab("Word frequency") +
  ylab(expression(P("stress shifted")))

## This empirical plot assumes a linear effect of frequency (x) -- it is
## analagous to using geom_smooth(method='lm') for an empirical plot
## of continuous y.  To allow for a nonlinear effect of x, analagous
## to geom_smooth() for continuous y, we need to tell geom_smooth
## to model y with a logistic regression of the *smooth* of x:

## In this case, the plots look very similar.
# diatones %>% ggplot(aes(x = frequency, y = stress_shifted)) +
#    geom_jitter(size = 0.5, width = 0.1, height = 0.02) +
#    geom_smooth(method = "gam", method.args = list(family = "binomial"), formula = y ~ s(x)) +
#    xlab("Word frequency") +
#    ylab(expression(P("stress shifted")))



## Calculate data summaries for plotting effects of categorical predictors, with exact 95% CIs

syll1_df <- diatones %>%  group_by(syll1_coda_orig) %>% summarise(n=n(), s=sum(stress_shifted==1)) %>% split(.$syll1_coda_orig) %>% map(~binom.confint(.$s, .$n, methods = c('exact'))) %>% bind_rows(.id='syll1_coda_orig')


syll2_coda_df <- diatones %>%  mutate(syll2_coda_fact = factor(syll2_coda_orig, labels = c("0", "1", "2", "3")))  %>% group_by(syll2_coda_fact) %>% summarise(n=n(), s=sum(stress_shifted==1)) %>% split(.$syll2_coda_fact) %>% map(~binom.confint(.$s, .$n, methods = c('exact'))) %>% bind_rows(.id='syll2_coda_fact')

syll2_td_df <- diatones %>%  group_by(syll2_td_orig) %>% summarise(n=n(), s=sum(stress_shifted==1)) %>% split(.$syll2_td_orig) %>% map(~binom.confint(.$s, .$n, methods = c('exact'))) %>% bind_rows(.id='syll2_td_orig')


syll1_df %>% ggplot(aes(x=syll1_coda_orig, y=mean)) +
  geom_pointrange(aes(ymin=lower, ymax=upper)) +
  geom_jitter(aes(x=syll1_coda_orig, y=stress_shifted), size=0.75, width=0.1, height=0.02, data=diatones, alpha = .5) +
  xlab("Syllable 1 coda") +
  ylab(expression(P("stress shifted")))



syll2_coda_df %>% ggplot(aes(x = syll2_coda_fact, y = mean)) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  geom_jitter(aes(x = syll2_coda_fact, y = stress_shifted), size = 0.75, width = 0.1, height = 0.02, alpha = .5, data = mutate(diatones, syll2_coda_fact = factor(syll2_coda_orig, labels = c("0", "1", "2", "3")))) +
  xlab("Syllable 2 coda") +
  ylab(expression(P("stress shifted")))


syll2_td_df %>% ggplot(aes(x=syll2_td_orig, y=mean)) +
  geom_pointrange(aes(ymin=lower, ymax=upper)) +
  geom_jitter(aes(x=syll2_td_orig, y=stress_shifted), size=0.75, width=0.1, height=0.02, alpha = .5, data=diatones) +  
  xlab("Syllable 2 coda = t/d") +
  ylab(expression(P("stress shifted")))

## The same plots with 95% bootstraped CITs. Note how different these are
## in some cases.

# diatones %>%
#   mutate(syll1_coda_fact = factor(syll1_coda, labels = c("0", "1"))) %>%
#   ggplot(aes(x = factor(syll1_coda_fact), y = stress_shifted)) +
#   geom_jitter(size = 0.5, width = 0.1, height = 0.02) +
#   stat_summary(fun.data = "mean_cl_boot") +
#   xlab("Syllable 1 coda") +
#   ylab(expression(P("stress shifted")))


# diatones %>%
#   mutate(syll2_coda_fact = factor(syll2_coda, labels = c("0", "1", "2", "3"))) %>%
#   ggplot(aes(x = factor(syll2_coda_fact), y = stress_shifted)) +
#   geom_jitter(size = 0.5, width = 0.1, height = 0.02) +
#   stat_summary(fun.data = "mean_cl_boot") +
#   xlab("Syllable 2 coda") +
#   ylab(expression(P("stress shifted")))


# diatones %>%
#   mutate(syll2_td_fact = factor(syll2_td, labels = c("no", "yes"))) %>%
#   ggplot(aes(x = factor(syll2_td_fact), y = stress_shifted)) +
#   geom_jitter(size = 0.5, width = 0.1, height = 0.02) +
#   stat_summary(fun.data = "mean_cl_boot") +
#   xlab("Syllable 2 coda = t/d") +
#   ylab(expression(P("stress shifted")))
@


\begin{boxedtext}{Practical note: Empirical plots for binomial data}
\label{box:empirical-binomial}


% In introducing logistic regression we saw several reasons why it is better for binomial data to model the probability (as log-odds) that $y=1$, rather than $y$ itself. By the same logic, 
% 
% When plotting empirical relationships for binomial data it is important to use visualizations that calculate probabilities rather than treating $y$ as 0 and 1.  

Empirical plots of the effect of a predictor $x$ for binomial data (where $y=0, 1$) should take into account that what is being plotted are (estimated) probabilities, not averages of a response $y$ that happens only take on values 0 and 1. This reflects the structure of logistic regressions, which will model the probability that $y=1$ rather than $y$ itself.

For continuous $x$, a simple logistic regression should be used to calculate the empirical effect (with 95\% CIs), which will look like part of an S-shaped curve. This is better than just showing the distributions of $x$ for $y=0$ and $y=1$ (e.g., using boxplots), which is common, but doesn't respect the fact that $x$ is a predictor. 

For categorical $x$, the empirical effect is just the proportion $p$ of cases with $y=1$ for each value of $x$, but the 95\% CIs should be calculated using an exact method (e.g., from \ttt{binom.test()}), which can be very different from bootstrapped CIs especially for small sample size  or $p$ near 0 and 1.

Fig.~\ref{fig:diatones-empirical} shows examples for continuous (top left) and categorical (other panels) predictors;  Figure~\ref{fig:reg-fam-size} in the next chapter shows more examples. The code files show how to make such plots.  

This topic seems technical, but it comes up a lot in practice and raises a more general point: empirical plots should summarize data in a way that matches the statistical models to be used, as much as possible.

\end{boxedtext}

\section{Categorical data analysis}
\label{sec:cda-basics}

This section reviews some basics of categorical data analysis.
%---contingency tables and tests of independence of two categorical variables---which we assume you have had some exposure to before.
%and odds/odds ratios---which we assume you have had some exposure to before  (except perhaps odds/odds ratios). 
Section~\ref{sec:other-reading-ch6} gives some places to read more if this is your first exposure to categorical data analysis.

% 
% 
% \begin{itemize}
% \item
%   \emph{Contingency tables} (e.g.,~\texttt{xtabs} in R)
% 
%   \begin{itemize}
%   \tightlist
%   \item
%     Including the notion of \textbf{observed} and \textbf{expected} values based on a contingency table, and ``observed/expected ratios'' (O/E ratios).
%   \end{itemize}
% \item
%   \emph{Tests of independence} of categorical variables
% 
%   \begin{itemize}
%   \item
%     \(\chi^2\) tests
%   \item
%     Fisher's exact test
%   \end{itemize}
% \end{itemize}
% 
% which we refresh briefly below. If you haven't seen these topics before, some places to read more are given \protect\hyperlink{cda-other-readings}{below}.


\subsection{Contingency tables}
\label{x2-contingency-tables}

\emph{Contingency tables}---a.k.a.\ \emph{cross-tabulation}, hence the R function \ttt{xtabs()}---show the number of observations for each combination of values of categorical variables. A 2x2 contingency table in particular shows the number of observations for each combination of two categorical variables, $x_1$ and $x_2$, each of which has two levels.

For example, for the \ttt{diatones} data consider the variables capturing whether the first syllable has a coda (\ttt{syll1\_coda}) and whether stress shifted (\ttt{stress\_shifted}), each of which has two levels. (In the dataframe these are coded as numeric variables with values 0 and 1, which is equivalent.)  A contingency table showing the number of observations for each combination of these variables is:
%\ttt{syll\_coda} and \ttt{stress\_shifted} is:

<<>>=
diatones_xtab <- xtabs(~ syll1_coda + stress_shifted, data = diatones)
diatones_xtab
@

%\subsubsection{Observed and expected values}

%In the contingency table above, 
It looks like stress shift isn't independent of coda presence: \texttt{stress\_shifted}=1 is much more likely relative to \texttt{stress\_shifted}=0 when \ttt{syll1\_coda}=1. 

We would like to formally test this hypothesis of non-independence, which we can do using the null hypothesis that  \texttt{stress\_shifted} and \ttt{syll1\_coda} occur independently in this data. Under this null hypothesis, the expected count in a cell with $x_1 = a$ and $x_2 = b$, given a total of $n$ observations, is:
\begin{equation*}
P(x_1 = a) \cdot P(x_2 = b) \cdot n
\end{equation*}


% 
% Under \(H_0\), we can work out:
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   The estimated P(\texttt{context} = \texttt{alternative})
% \item
%   The estimated P(\texttt{prominence} = \texttt{adjective})
% \item
%   The expected count in each cell, given the total number of observations \(n\).
% \end{enumerate}

The two probabilities can be estimated by just summing over the relevant row or column of the contingency table and dividing by $n$.
For example, for the contingency table above, we can calculate the expected count when \ttt{syll1\_coda} = 1 and \ttt{stress\_shifted} = 0, given that $n = 130$:

\begin{align*}
P(\ttt{syll1\_coda} = 1) \cdot P(\ttt{stress\_shifted}=0) \cdot n & =
\frac{(21+8)}{130} \frac{(88+21)}{130} \cdot 130 \\
& = 24.3
\end{align*}


% 
% <<>>=
% n <- nrow(diatones)
% 
% ## contingency table from above
% tab <- xtabs(~syll1_coda + stress_shifted, data=diatones)
% 
% ## P(syll1_coda = 1) estimate
% p1 <- sum(tab['1',])/n
% 
% 
% ## P(stress_shifted=0) estimate
% p2 <- sum(tab[,'0'])/n
% 
% ## estimated count
% p1*p2*n
% @
% % 
% The estimated count in the \texttt{context}=\texttt{Alternative} \& \texttt{prominence}=\texttt{Adjective} cell is then:
% \begin{align}
% n \cdot P(`context` = `Alternative`) \cdot P(`prominence` = `Adjective`) & = 
% 415 \cdot 0.4939759 \cdot 0.4216867 \\
% & = 86.4457831
% \end{align}
% 
% \hypertarget{exercise-1-1}{%
% \subsubsection*{Exercise 1}\label{exercise-1-1}}
% \addcontentsline{toc}{subsubsection}{Exercise 1}
% 
% Calculate the estimated counts for the other three cells.

% FUTURE: exercise? 
% do this in R and/or calculate the estimated counts for the other three cells.


\subsection{The chi-squared test}
\label{sec:chisq-ex}

The logic above applies to all four cells of a 2x2 contingency table, giving two numbers for the $i$th cell: the \emph{observed} count $O_i$, and the \emph{expected} count $E_i$ (under the null hypothesis of independence).  Let $m$ be the number of cells in the table; here $m=4$.

%We can then define
\emph{Pearson's test statistic} $X^2$,   measures how much the observed and expected values differ across the whole contingency table:
\begin{equation}
  X^2 = \sum^m_{i = 1} \frac{(O_i - E_i)^2}{E_i}
    \label{eq:pearson}
\end{equation}

 \(X^2\) is 0 when the observed and expected values are exactly the same, and increases the more the observed and expected values differ.

Pearson's test statistic for a 2x2 table approximately follows a \(\chi^2\) distribution (pronounced ``chi-squared'') with one degree of freedom, denoted \(\chi^2(1)\).
%the approximation is good under conditions described below. 
Thus, we can test the null hypothesis that \(x_1\) and \(x_2\) are independent by comparing the value of \(X^2\) to the \(\chi^2(1)\) distribution; this is a \emph{chi-squared test} (or $\chi^2$ test), one of the most widely-used hypothesis tests.

The same methodology applies for testing independence of two categorical variables with any number of levels. For $x_1$ and $x_2$ with $p$ and $q$ levels, \(X^2\) is calculated
using equation \eqref{eq:pearson}, $m = pq$, and the null hypothesis tested using a $\chi^2(df)$ distribution, where $df = (p-1)(q-1))$.  For example, a 2x2 contingency table has $df = 1$ and a 3x4 contingency table has $df = 6$.

The chi-squared test relies on the approximation that $X^2$ follows a chi-squared distribution (the `$\chi^2$ approximation'), which is only valid when the expected count in each cell is `big enough'---otherwise, $p$-values are anti-conservative. A common rule of thumb is 5 (expected) observations per cell, although for tables larger than 2x2 about 20\% of cells can have expected counts of 1--5 \citep[][\S5.3.4]{agresti2007}. 
%Today there is little reason to use a chi-squared test anyway (see below),but these rules of thumb are important for assessing previous work, where chi-squared tests are ubiquitous.


\paragraph{Examples}

\ttt{chisq.test()} applied to a contingency table carries out a chi-squared test. To test whether \ttt{syll1\_coda} and \ttt{stress\_shifted} are associated for the contingency table above:

<<>>=
diatones_xtab_test <- chisq.test(diatones_xtab)
diatones_xtab_test
@

Whether a word's first syllable has a coda is not significantly associated (at the $\alpha = 0.05$ level) with whether its stress shifted.
%% note to self, but FUTURE decide whether to include: Yates continuity correction is an adjustment to |O-E| differences to decrease error in chi-squared approximation, applied in R by default for 2x2 tables.

Executing the code above gives a warning message:

<<warning=TRUE, echo=FALSE, results=FALSE>>=
diatones_xtab_test <- chisq.test(diatones_xtab)
@

This message refers to the expected counts rule of thumb above, as one cell in the contingency table has expected count below 5:

<<>>=
diatones_xtab_test$expected
@

% As another example, this chi-squared test shows that whether a verb is irregular and what auxiliary class it belongs to are significantly associated, for the \ttt{regularity} data 
% 
% <<>>=
% regularity_test <- chisq.test(xtabs(~Auxiliary + Regularity, regularity))
% regularity_test
% @
% 
% For this test $df = 2$ because the contingency table is $2 \times 3$ ($df = (2-1) \cdot (3-1)$).


\begin{boxedtext}{Broader context: Whither chi-squared tests?}
\label{box:whither-chi-squared}

It is common to have fewer than 5 observations in one or more cells in a contingency table, making `chi-squared tests' in the usual sense of the term ($p$-value calculated assuming that $X^2$ follows a chi-squared distribution) frequently inappropriate. Widespread use of the chi-squared test is largely a holdover from when it was computationally difficult to compute `exact' tests (which don't require a large-sample approximation to compute $p$-values), and there is little reason to use the \(\chi^2\) approximation today. 

For example, you can run a `chi-squared test' for the \ttt{diatones\_xtab\_test} example above without assuming the \(\chi^2\) approximation by using the \texttt{simulate.p.value=TRUE} argument to  \texttt{chisq.test()}, which gets rid of the error.
% (not shown)function
% <<>>=
% chisq.test(diatones_xtab, simulate.p.value=TRUE)
% @
% which gets rid of the error.

It is still worth knowing about the chi-squared test and its limitations because it is still frequently used, and in older literature is very widely used. For example, if you are reading a paper where the crucial result relies on a chi-squared test with \(p=0.02\) for a 2x2 contingency table with 5 observations in one cell, you should be suspicious. If the contingency table has 20+ observations per cell, you shouldn't be.

\end{boxedtext}

\subsection{Fisher's exact test}

Given that the assumptions of a chi-squared test are often not met in practice (Box~\ref{box:whither-chi-squared}), 
an alternative which is a good default is
%to the \(\chi^2\) test is 
\emph{Fisher's exact test}, which as an `exact test' does not place any assumptions on counts per cell. Fisher's test asks a slightly different question from a chi-squared test---given the row and column totals in the contingency table, how likely would an arrangement of the data at least this extreme be if $x_1$ and $x_2$ are independent?---but in practice they can both just be thought of as testing independence of two categorical variables.
% 
% \begin{itemize}
% \tightlist
% \item
%   given the \emph{marginal counts} (row and column totals) in the contingency table, if \(X_1\) and \(X_2\) were independent, how likely would an arrangement of the data at least this extreme be?
% \end{itemize}


For example, let us test whether a verb's \ttt{Regularity} (2 levels) and its \ttt{Auxiliary} (3 levels) are independent, for the Dutch verb \ttt{regularity} data (Section~\ref{sec:nnd-regularity-data}).  The contingency table is:

<<>>=
regularity_xtab <- xtabs(~ Auxiliary + Regularity, regularity)
regularity_xtab
 @

Fisher's test asks: for \Sexpr{sum(regularity_xtab[,'irregular'])} irregular verbs, assuming independence, how likely would we be to have \(\geq\) \Sexpr{regularity_xtab['hebben','irregular']} \emph{hebben}, \(\leq\) \Sexpr{regularity_xtab['zijn','irregular']} \emph{zijn}, \(\leq\) \Sexpr{regularity_xtab['zijnheb','irregular']} \emph{zijnheb}, and so on.  
In R: 

<<>>=
fisher.test(regularity_xtab)
@

As expected, whether a verb is irregular and which \texttt{Auxiliary} it takes are significantly associated ($\alpha = 0.05$).

\subsection{Effect sizes and reporting}
\label{sec:cda-effect-size}

As for continuous $y$, it is essential to report {effect sizes} for categorical data analysis, and many options are available \citep[][chap.~6]{kline2013beyond}. For associations between two categorical variables $x_1$ and $x_2$,  an effect size which can be used for any $p \times q$ contingency  table (and hence chi-squared test) is \emph{Cramer's $V$}:
\begin{equation*}
V = \sqrt{
\frac{X^2}{df \cdot n}
},
\end{equation*}
<<echo=FALSE>>=
diatones_cramer <- sqrt(as.numeric(chisq.test(diatones_xtab, correct = F)$statistic) / sum(diatones_xtab))
regularity_cramer <- sqrt(as.numeric(chisq.test(regularity_xtab, correct = F)$statistic) / (1 * sum(regularity_xtab)))
@
where $df$ is the minimum of $p-1$ and $q-1$. Cramer's $V$ can be thought of as $X^2$ (equation~\ref{eq:pearson}) scaled so its maximum value is 1 \citep[][172]{kline2013beyond}.  So $V=0$ and $V=1$ mean `no association' and `perfect association'.  For a 2x2 contingency table ($df=1$), $V$ is the same as $|r|$, the absolute value of Pearson's correlation between $x_1$ and $x_2$ (coded as 0 and 1 for the two levels). So small/medium/large values of $V$ are the same as for correlations (Section~\ref{sec:effsize-rules-of-thumb}), divided by $\sqrt{df}$.

For the \ttt{diatones\_xtab} and \ttt{regularity\_xtab} tables, $V$ = \Sexpr{diatones_cramer} and \Sexpr{regularity_cramer} with $df=1$; both are `small' effects (Exercise~\ref{ex:cramer}).

It is also very common for 2x2 tables to just use an odds ratio (covered below) as an effect size measure, especially when reporting Fisher's test.



\begin{boxedtext}{Practical note: Interpreting and reporting tests of association}

When the null hypothesis is rejected, one must still interpret what the association `means'. This is straightforward for a 2x2 table (e.g., `stress shifting is more likely when there is a coda in syllable 1'), but harder for larger contingency tables. It can be useful to examine the \emph{observed/expected ratios} $O_i/E_i$ for intuition,
%(or just look at a plot of these values),
which give a sense of how often combinations of the two variables co-occur relative to what is expected by chance.   O/E ratios can be can be any number larger than 0, with O/E $<$ 1 meaning `co-occur less often than expected', O/E $\approx$ 1 meaning `about as often as expected', and O/E $>$ 1 meaning `more often than expected'.  O/E is commonly used to describe some kinds of linguistic data, such as co-occurrence of sounds in phonology \citep[e.g.,][]{frisch2004similarity}.

For the \ttt{regularity} example:
<<>>=
with(chisq.test(regularity_xtab), observed / expected)
@
<<echo=FALSE>>=
syll1_props <- diatones %>%
  group_by(syll1_coda) %>%
  tally(stress_shifted / n())
diatones_chisq_exact <- chisq.test(
  diatones_xtab, 
  simulate.p.value = TRUE, 
  B = 10000
)
@
Verbs are much more likely to be irregular for the auxiliary \tsc{zijn} than for the auxiliary \tsc{hebben}, with verbs which can take either auxiliary falling in between.

A test of association is reported using the same principles as for other hypothesis tests (Section~\ref{sec:reporting-hypothesis-tests}), bearing in mind that Fisher's test doesn't have a test statistic or $df$.  

A long report for the \ttt{diatones} chi-squared  test could be:
\begin{quote}
{\footnotesize Although a higher proportion of verbs with first-syllable codas shifted stress (\Sexpr{100*as.numeric(syll1_props[2,'n'])}\%) than verbs without codas (\Sexpr{100*as.numeric(syll1_props[1,'n'])}\%), the association was not significant at the $\alpha=0.05$ level by a chi-squared test  (Cramer's $V = \Sexpr{diatones_cramer}$, $X^2 = \Sexpr{glance(diatones_chisq_exact)$statistic}$, exact \Sexpr{formatP(glance(diatones_chisq_exact)$p.value)}).
}
\end{quote}

A short report for the  \ttt{regularity} Fisher's exact test could be:
\begin{quote}
{\footnotesize There was a small (Cramer's $V = \Sexpr{regularity_cramer}$) and significant association between verb regularity and auxiliary class (Fisher's exact test: \Sexpr{formatP(fisher.test(regularity_xtab)$p.value)}), primarily reflecting that verb irregularity is more likely for verbs which take auxiliary \tsc{zijn}.}
\end{quote}

\end{boxedtext}


\section{Odds and odds ratios}
\label{sec:odds-ors}

Intuitively, in logistic regression we will predict the probability that some event happens (e.g., stress shift) as a function of predictors, given data consisting of observations of whether the event happened or not ($y$ = 0 or 1).
%once (\(Y=\) 0 or 1). 

It would be tempting to just apply linear regression to this task, but this would not work.   We can't use $y$ as the response because we want to predict a probability (and $y$ only takes on the values 0 and 1).  We also can't do a linear regression using a probability as a response, because 
%(among other reasons) 
probabilities are bounded by 0 and 1 and linear regression assumes that $y$ isn't bounded.
%(We don't want the model to be able to predict ``probability = 2''.)


In order to predict probabilities using a regression model, we need
a way to think of probabilities on a continuous, unbounded scale, and a way to estimate these probabilities, such that the sample statistic is normally distributed.  The latter is necessary  so that we can apply all the statistical inference machinery we have used so far to do things like conduct hypothesis tests.

The answer turns out to be to use log-odds, for which we must first discuss odds.


\subsection{Odds}

\emph{Odds} are a way of thinking about probability, in terms of how likely an event is to happen versus not happen.  For example, if we say ``the odds are 3:1 that it will be sunny tomorrow'' (pronounced ``three-to-one''), it means the probability of sun is three times the probability of not-sun.

The odds corresponding to a probability $p$ is:
\begin{equation*}
\text{odds}(p) = \frac{p}{1 - p}
\end{equation*}

For example, if the probability that it is sunny tomorrow is 0.75, then the odds of it being sunny tomorrow are 3 (\(= 0.75/(1-0.75) \)). The odds of it not being sunny tomorrow are 1/3 (\(= 0.25/(1-0.25)\)).
%also written 1:3.
%(pronounced ``one-to-three'').
%or ``three-to-one against sun tomorrow.''

Figure \ref{fig:odds1} (left) shows odds as a function of probability.
Odds are more intuitive than probabilities, but they cannot be predicted in our regression model because they are always positive.

<<odds1, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap = 'Odds (left) and log-odds (right) as a function of probability (from 0.01 to 0.99).'>>=
p <- seq(0.01, 0.99, by = 0.01)

qplot(x = p, y = p / (1 - p), geom = "line") +
  xlab("Probability") +
  ylab("Odds")

qplot(x = p, y = log(p / (1 - p)), geom = "line") +
  xlab("Probability") +
  ylab("Log-odds")
@

%Odds may be more intuitive than probabilities, but they do not meet Goal 1 (unbounded scale) because odds are always positive.

\subsection{Log-odds}

The \emph{log-odds} corresponding to a probability \(p\) are:
\begin{equation*}
  \text{log-odds}(p) = \log \frac{p}{1-p}
\end{equation*}
Figure \ref{fig:odds1} (right) shows log-odds as a function of probability.

\begin{boxedtext}{Logarithm/exponentials refresher}
\label{box:log-exp}

Categorical data analysis and (especially) interpreting logistic regressions involve a lot of thinking in logarithms and exponentials.  If you haven't seen these for a while,
%(or ever), 
some important facts are:
\begin{itemize}

\item Logarithms transform division to subtraction:  $\log(x/y) = \log(x) - \log(y)$

\item Logarithms transform multiplication to addition: $\log(xy) = \log(x) + \log(y)$

\item Exponentials are the inverse of logarithms: if $y = \log(x)$, $x = e^{y}$.


\end{itemize}

Note that we use `log(x)' to refer to the \emph{natural logarithm}: the power to which $e$ (the `base') would need to be raised to get $x$, where \(e=2.718..\) is Euler's number.


You may have seen `log' used to mean something else (usually logarithm with base 10) or `ln' used to mean natural logarithm. 


% 
% Don't worry if thinking about logarithms and exponentials is slow at first---I find many students are embarassed that they don't remember this `basic' math, but it's natural to be rusty if you haven't used it for years!

% NB: checked with JT this is more helpful than patronizing -- but that included commented-out paragraph
\end{boxedtext}

Log-odds meet both desiderata as the response for a  regression: they
range from  \(-\infty\) to \(\infty\),
and they turn out to have normally-distributed sampling statistics (Section~\ref{sec:log-odds-sample-pop}). 
Log-odds have the drawback of being unintuitive, compared to odds or probability, but with practice you can think in terms of log-odds.

% and are nearly linear as a function of \(p\), so long as \(p\) isn't too close to 0 or 1. It also turns out that log-odds satisfy our Goal 2 (normally-distributed sampling statistic), as discussed below.
%Log-odds do have an important drawback: they are unintuitive, compared to odds or probability. They take getting used to, but with practice you can think in terms of log-odds.  

For example, here are some correspondences between $p$, odds (approximated), and log-odds:
%(after Winter p 230):

\begin{tabular}{ccc}
\toprule
$p$ & Odds & Log-odds \\
\midrule
% 0.26 & 0.35:1 & -1 \\
% 0.5 & 1:1 & 0 \\
% 0.73 & 2.7:1 & 1 \\
% 0.88 & 7.3:1 & 2 \\
% 0.953 & 20:1 & 3 \\
 0.27 & 1:2.7  & -1 \\
0.50 & 1:1 & 0 \\
0.73 & 2.7:1 & 1 \\
0.88 & 7.4:1 & 2 \\
0.95 & 20.1:1 & 3 \\
\bottomrule
\end{tabular}

This example illustrates a few intuitions about log-odds, which you can see in Figure~\ref{fig:odds1} (right):
\begin{itemize}
\item \textbf{Probability space is contracted} in log-odds, as we approach 0 or 1: a change of 1 in log-odds corresponds to a smaller and smaller change in \(p\).
\item \textbf{Log-odds of 3/-3 is big}: log-odds of -3 to 3 encompasses 90\% of probabilities ($p \approx 0.05, 0.95$).
\item \textbf{Log-odds of 5/-5 is huge}: $p \approx 0.99, 0.01$ .
\end{itemize}


% \begin{quote}
% \textbf{Questions}:
% 
% What is \(p\) corresponding to log-odds of -4?
% \end{quote}
% 
% <<echo=FALSE>>=
% invlogit(-4)
% @

\hypertarget{odds-ratios}{%
\subsection{Odds ratios}\label{sec:odds-ratios}}

Let us return to the \ttt{diatones} data example from above, 
%Let's start with an example from the \ttt{diatones} data, 
looking at the association between \ttt{syll1\_coda} and \ttt{stress\_shifted} as above (Section~\ref{sec:chisq-ex}), but now thinking of it as `how does the likelihood of stress shifting depend on the presence of a coda?'  That is, the response $y$ and predictor $x$ are:
\begin{itemize}
\item $y$ = \ttt{stress\_shifted} (1 = yes, 0 = no)
\item $x$ = \ttt{syll1\_coda} (1 = yes, 0 = no)
\end{itemize}

The proportion of cases in each cell for our data are:

<<>>=
diatones_ptab <- prop.table(diatones_xtab)
diatones_ptab
@
% 
% %(\ttt{prop.table(diatones\_xtab)}):
% <<echo=FALSE>>=
% diatones_ptab <- round(prop.table(xtabs(~syll1_coda + stress_shifted, data=diatones)),2)
% p_00 <-diatones_ptab[1,1]
% p_01 <- diatones_ptab[1,2]
% p10 <- diatones_ptab[2,1]
% p11 <- diatones_ptab[2,2]
% @
% 
% $$
% \begin{array}{c|cc}
% & y = 0 & y = 1 \\
% \hline
% x = 0 & \Sexpr{p_00} & \Sexpr{p_01} \\
% x = 1 & \Sexpr{p10} & \Sexpr{p11}
% \end{array}
% $$


(The \ttt{prop.table()} function calculates a table of proportions corresponding to a contingency table.)  For example, \Sexpr{diatones_ptab['0', '1']} of observations have $x=0$ and $y=1$.


%\ttt{prop.table(diatones\_xtab)}.)

We would like a measure of how much more likely $y=1$ (stress shifting) is when \(x=1\) than when \(x=0\), for which we
%One way to do this is to
calculate the odds of stress shifting in each case, then take their ratio:

<<>>=
## Odds(y=1 | x=0)
odds_y1x0 <- diatones_ptab[1, 2] / diatones_ptab[1, 1]
## Odds(y=1 | x=1)
odds_y1x1 <- diatones_ptab[2, 2] / diatones_ptab[2, 1]

## odds ratio
odds_y1x1 / odds_y1x0
@

So the odds of stress shifting are about 2.5 times higher when the first syllable has a coda relative to when it does not.

% % 
% % \begin{eqnarray}
% % Odds(y=1 | x=0)  = \frac{\Sexpr{p_01}}{\Sexpr{p_00}} = \Sepxr{p_01/p_00} %\\
% % %Odds(y=1 | x=1) & = \frac{\Sexpr{p11}}{\Sexpr{p10}} = \Sexpr{p11/p10}
% % \end{eqnarray}
% 
% \begin{itemize}
% \item
%   when \(X = 1\): \(\frac{0.462}{0.037} = 12.49\)
% \item
%   when \(X = 0\): \(\frac{0.43}{0.07} = 6.14\)
% \item
%   Ratio: \(\frac{12.49}{6.14}=\) \textbf{2.03}
% \end{itemize}

% Thus, the odds of tapping are roughly doubled for transitive verbs, relative to intransitive verbs.

To define this \emph{odds ratio} more generally, suppose that two binary random variables $x$ and $y$ co-occur with these (population) probabilities:
\begin{equation}
\begin{array}{c|cc}
& y = 1 & y = 0 \\
\hline
x = 1 & p_{11} & p_{10} \\
x = 0 & p_{01} & p_{00}
\end{array}
\label{eq:odds-notation}
\end{equation}

The odds that $y=1$ are then $p_{11}/p_{10}$ when $x=1$ and $p_{01}/p_{00}$ when $x=0$.  The  \emph{odds ratio}, as a population statistic, is defined as:
\begin{equation*}
\text{odds ratio} = \frac{p_{11}/p_{10}}{p_{01}/p_{00}}
\end{equation*}

The odds ratio describes how much the occurrence of one (binary) event depends on another (binary) event, and is often used as an intuitive measure, as in ``Your odds of developing lung cancer are halved if you quit smoking.''  Note that odds ratios are interpreted multiplicatively. It makes sense to say ``A raises the odds of B by a factor of 2'', but not ``A raises the odds of B by 2''.

<<echo=FALSE>>=
p_ex_1 <- odds_y1x0 / (1 + odds_y1x0)
p_ex_2 <- odds_y1x1 / (1 + odds_y1x1)
@

Odds ratios are in part a way of thinking about changes in probability. For the \ttt{diatones} example above, our estimates of the probability of stress shifting in each case are: 
\begin{align}
P(\text{\ttt{stress\_shifted} = 1} | \text{\ttt{syll1\_coda}=1}) & \approx \Sexpr{p_ex_1}
\nonumber \\
P(\text{\ttt{stress\_shifted} = 1} | \text{\ttt{syll1\_coda}=0})
 & \approx \Sexpr{p_ex_2}.
\label{eq:ss-approx}
\end{align}



% \[
% P(\,\text{tapped}~|~\text{transitive}\,) = 0.926, \quad
%   P(\,\text{tapped}~|~\text{intransitive}\,) = 0.86
% \]\\

Thus, the change in probability depending on whether a coda is present is \Sexpr{p_ex_2-p_ex_1}. However, odds ratios are not equivalent to changes in probability, because with odds ratios, what a change from \(p\) to \(p + \Delta p\) corresponds to depends on the probability (\(p\)) you start at.  You can see this in Figure \ref{fig:odds1} (right): the further $p$ is from 0.5 ($x$-axis), the bigger the change in log-odds ($y$-axis) corresponding to a change of one `square' in $p$ ($\Delta p = 0.125$). 
%Figure \ref{fig:oddsratio} shows the odds ratio corresponding to a change in probability of 0.15, as a function of the starting probability. 
For example, for a change in probability from 0.5 to 0.65, the odds increase about 1.8-fold, while for a change in probability from 0.8 to 0.95 the odds increase 4.8-fold.
% 
% <<oddsratio, echo=FALSE, warning=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap="Odds ratio when a probability ($p$) is increased by 0.15, as a function of $p$">>=
% p <- seq(0.001, 0.849, by = 0.01)
% oddsRat <- ((p+0.15)/(1-p-0.15))/(p/(1-p))
% 
% qplot(p, oddsRat, geom = "line") +  
%   xlab("Probability") + 
%   ylab("Odds ratio when p -> p+0.15") + xlim(0,1) + ylim(1.4,4)
% @


To summarize: odds correspond to probabilities, but changes in odds don't straightforwardly correspond to changes in probabilities.

% \begin{itemize}
% \item
%   Odds have a one-to-one relationship with probabilities
% \item
%   \textbf{Changes} in odds don't straightforwardly correspond to \textbf{changes} in probabilities.
% \end{itemize}

% The non-equivalence between \textbf{changes} in odds and probability is important
% for interpreting 
% %to remember when interpreting 
% logistic regressions, which are in log-odds space.

\subsection{Log odds: Sample and population}
\label{sec:log-odds-sample-pop}

To estimate the population value of the odds ratio, we need a sample statistic to estimate it from observed counts for
%Because we never actually observe the population value of the odds ratio---which is based on probabilities of each combination of \(X\) and \(Y\)---we need a \emph{sample statistic} to estimate it from observed counts for 
each \(x\)/\(y\) combination.  Using the same notation as in equation \eqref{eq:odds-notation}, let $n_{01}$ be the number of observations where $x=0$ and $y=1$, and so on.
% Suppose we observe the following counts:
% 
% \[
% \begin{array}{c|cc}
% & y = 1 & Y = 0 \\
% \hline
% X = 1 & n_{11} & n_{10} \\
% X = 0 & n_{01} & n_{00}
% \end{array}
% \]

To estimate the odds ratio, we estimate  each probability from the observed counts (as we did in equation~\ref{eq:ss-approx} above), then calculate the odds for each case. For example, for $x=1$:
\begin{align*}
P(y=1|x=1) \approx \frac{n_{11}}{n_{11}+n_{10}},  \quad P(y=0|x=1) \approx\frac{n_{10}}{n_{11}+n_{10}} \\
\Rightarrow \text{odds}(y=1|x=1) = \frac{n_{11}}{n_{10}}
\end{align*}

Similarly estimating \(\text{odds}(y=1|x=0)\), then taking the logarithm of \(\text{odds}(y=1|x=1)/\text{odds}(y=1|x=0)\), gives a sample statistic for the change in log odds:
\begin{equation*}
  \hat{L} = \log \left[ \frac{n_{11}/n_{10}}{n_{01}/n_{00}} \right]
\end{equation*}

%(Recall that $\log(a/b) = log(a) - log(b)$---see Box~\ref{box:log-exp}.)

It turns out that \(\hat{L}\) is (approximately) normally distributed with mean \(\log(OR)\), where \(OR\) is the population value of the odds ratio \citep[][\S2.3.3]{agresti2007}. Thus, if we want to do inferential statistics on the effect of binary \(x\) on binary \(y\), it makes sense to work with log-odds, rather than probabilities or odds, estimates of which are not normally distributed.

\subsection{Calculation and interpretation}

The function from probability to log-odds is called \emph{logit} (\(\text{logit}(p)\)), and %sometimes changes in log-odds are also called `logits' (e.g., `... increases by 2 logits').
  the function from log-odds to probability is called \emph{inverse logit} (\(\text{logit}^{-1}(x)\)), a.k.a. the `logistic function'
  (\(1/(1+e^{-x})\)).

Both can be calculated using  \ttt{qlogis()} and \ttt{plogis()} in base R, or
the more intuitively-named
%
\texttt{logit()} and \texttt{invlogit()} in the arm package.
%\texttt{arm} package (or \ttt{qlogis} and \ttt{plogis} in base R, which do the same thing).

Note that adding in the log-odds scale corresponds to multiplying in the odds scale, by a power of \(e\) (Box~\ref{box:log-exp}).  So an increase of $x$ in log-odds corresponds to multiplying the odds by $e^{x}$.

% 
% For example, changing log-odds by 1 corresponds to multiplying the odds ratio by \(e\):
% \begin{eqnarray*}
% \log(OR_1) = 0.69 & \Rightarrow & OR_1 = e^{0.69} = 2 \\
% \log(OR_2) = 1.69 & \Rightarrow & OR_2 = e^{1.69} = 5.42 = 2{\cdot}e
% \end{eqnarray*}

%\subsubsection{Example}

For instance, for the \ttt{diatones} example, the (estimated) odds of stress shifting for words without and with a first syllable coda are \Sexpr{odds_y1x0} and \Sexpr{odds_y1x1} (\ttt{odds\_y1x0}, \ttt{odds\_y1x1}). The change in log-odds is:

<<>>=
log(odds_y1x1) - log(odds_y1x0)
@

This change corresponds to multiplying the odds by  $e^{\Sexpr{log(odds_y1x1/odds_y1x0)}}$ (=\Sexpr{odds_y1x1/odds_y1x0}, the odds ratio).

% 
% \section{Other readings}
% \label{sec:other-reading-ch6}
% 
% Most introductory statistics books, and many of those written for linguists, cover basics of categorical data analysis, including contingency tables, chi-squared tests, and Fisher exact tests (e.g., those listed in Section~\ref{sec:other-reading-ch2}). Some discussion of odds and odds ratios is given by \citet{agresti2007} chap.~2, \citet{gelman2007data} 5.2, \citet{crawley2015statistics} chap.~14.
% %\citep[e.g.,][]{dalgaard2008introductory,field2012discovering,crawley2015statistics,NavarroOnline}. 
% \citet{agresti2007} is  a particularly good and accessible resource for categorical data analysis generally (chap.\ 1--2 for these topics). \citet{agresti2003categorical} is a more technical/comprehensive version.  

% 
% \begin{itemize}
% \item
%   General audience: \citet{dalgaard2008introductory} chap.~8, \citet{agresti2007} chap.~1-2
% \item
%   For psychologists and language scientists: \citet{NavarroOnline} chap.~12, \citet{johnson2008quantitative} chap.~3.1, 5.1-5.3
% \end{itemize}
% 
% The Agresti book is a particularly good and accessible resource for categorical data analysis generally. (\citet{agresti2003categorical} is Agresti's more technical/comprehensive book, which is the reference on CDA.)

% 
% \hypertarget{c3solns}{%
% \section{Solutions}\label{c3solns}}
% 
% \hypertarget{solutions-to-exercise-1}{%
% \subsection{Solutions to Exercise 1:}\label{solutions-to-exercise-1}}
% 
% Estimated Counts for:
% 
% \begin{itemize}
% \item
%   \texttt{Prominence} = \texttt{Noun}, \texttt{Context} = \texttt{Alternative} cell = 118.5542
% \item
%   \texttt{Prominence} = \texttt{Adjective}, \texttt{Context} = \texttt{NoAlternative} cell = 88.55422
% \item
%   \texttt{Prominence} = \texttt{Adjective}, \texttt{Context} = \texttt{NoAlternative} cell = 121.4458
% \end{itemize}

%\textbf{Preliminary code}
%
%This code is needed to make other code below work:
% 
% <<message=F, error=F, warning=F>>=
% library(gridExtra) # for grid.arrange() to print plots side-by-side
% library(languageR)
% library(ggplot2)
% library(dplyr)
% library(arm)
% library(boot)
% 
% ## loads givennessMcGillLing620.csv from OSF project for Wagner (2012) data
% givenness <- read.csv("data/givenness_rmld.csv")
% #givenness <- read.csv(url("https://osf.io/q9e3a/download"))
% 
% ## make standardized (numeric/centered/scaled) versions of `givenness' dataset predictors:
% givenness <- mutate(givenness,
%                     conditionLabel.williams = arm::rescale(conditionLabel),
%                     clabel.williams = arm::rescale(conditionLabel),
%                     npType.pronoun = arm::rescale(npType),
%                     npType.pron = arm::rescale(npType),
%                     voice.passive = arm::rescale(voice),
%                     order.std = arm::rescale(order),
%                     stressshift.num = (as.numeric(stressshift) - 1),
%                     acoustics.std = arm::rescale(acoustics)
% )
% @
% 
% \textbf{Note}: Answers to some questions/exercises not listed in text are in \protect\hyperlink{c4solns}{Solutions}


\section{Simple logistic regression}
\label{sec:simple-logistic-regression}

Analogously to `simple linear regression' (Section~\ref{sec:simple-linear-regression}) for continuous $y$,
%which is predicting a continuous $y$ from a single predictor $x$, 
in \emph{simple logistic regression} we predict a binary $y$ from a single predictor $x$, which may be continuous or discrete.

In this setting our sample consists of $n$ observations,  $(x_1, y_1), ..., (x_n, y_n)$, where each $y_i$ is 0 or 1.  We model the log-odds of $y=1$ for the \(i^{\text{th}}\) observation as a function of the single predictor:
% 
% \[  
% \log \left[ \frac{P(y_i = 1)}{P(y_i = 0)} \right] \equiv \text{logit}(P(y_i = 1))
% \]
%as a function of the single predictor:
\begin{equation}
  \text{logit}(p(y_i = 1)) = \beta_0 + \beta_1 x_i, \quad \text{for } i=1, \ldots, n
  \label{eq:logit}
\end{equation}

This is the model in `logit space', where the right-hand side of the equation 
%(called the \emph{linear predictor}) 
looks the same as for simple linear regression (equation~\ref{eq:linreg2}) except for the lack of an error term, while the left-hand side of the equation looks similar to simple linear regression except for the logit function. (These differences are explored further below: Box~\ref{box:log-lin-reg-diffs}.)
%(called the \emph{link function}).



The same model can be written in `probability space':
\begin{eqnarray*}
  P(y_i = 1) & = &  \text{logit}^{-1}(\beta_0 + \beta_1 x_i) \\
  & = & \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}, \quad \text{for } i=1, \ldots, n
\end{eqnarray*}


The model in equation \eqref{eq:logit} predicts the log-odds of $y=1$  for the $i$th observation to be:
\begin{equation*}
  \text{logit}(\hat{p}_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\end{equation*}


The fitted regression coefficients, $\hat{\beta}_0$ and $\hat{\beta}_1$, can be interpreted in terms of log-odds, odds, or probability (Table~\ref{tab:simple-log-reg-coefficients}).




\begin{table}
\begin{tabular}{lp{4cm}p{4cm}}
\toprule
& Intercept interpretation & Slope interpretation \\
\midrule
Log-odds & \(\hat{\beta}_0\): predicted log-odds of \(y = 1\) when \(x = 0\)  &  {predicted change in log-odds} of \(y = 1\) for a unit change in \(x\)

\\
Odds & \(e^{\hat{\beta}_0}\): predicted odds of \(y = 1\) when \(x = 0\) & 
\(e^{\hat{\beta}_1}\): predicted amount odds of \(y = 1\) is multiplied by for a unit change in \(x\)
\\
Probability & \(\text{logit}^{-1}(\hat{\beta}_0)\): probability of \(y = 1\) when \(x = 0\) &  
\(\hat{\beta}_1\) interpretation in probability depends on the value of \(x\)
\\
\bottomrule
\end{tabular}
\label{tab:simple-log-reg-coefficients}
\caption{Interpretation of coefficients of a simple logistic regression.}
\end{table}

Below we discuss estimation of these coefficients and the usual associated measures (standard errors, CIs, $p$-values), after working through some examples of fitted models for intuition.


% \hypertarget{log-reg-hyp-test}{%
% \subsection{Hypothesis testing}\label{log-reg-hyp-test}}


% \subsection{Coefficient interpretations}

% 
% For the intercept:
% \begin{itemize}
% \item
%   \(\beta_0\): \textbf{predicted log-odds} of \(y = 1\) when \(x = 0\)
% \item
%   \(e^{\beta_0}\): predicted odds of \(y = 1\) when \(x = 0\)
% \item
%   \(\text{logit}^{-1}(\beta_0)\): probability of \(y = 1\) when \(x = 0\)
% \end{itemize}
% 
% For the slope:
% 
% \begin{itemize}
% \item
%   \(\beta_1\): \textbf{predicted change in log-odds} of \(y = 1\) for a unit change in \(x\)
% \item
%   \(e^{\beta_1}\): predicted amount odds of \(y = 1\) is multiplied by for a unit change in \(x\)
% \item
%   The meaning of \(\beta_1\) in probability depends on the value of \(x\)
% \end{itemize}
% 
% 



\subsection{Examples}

Logistic regressions are fit  using the \texttt{glm()} function with the argument \texttt{family="binomial"}, because they are one kind of `generalized linear model' (Box~\ref{box:glm}).

Let us fit a simple logistic regression which predicts whether a verb is regular as a function of its frequency, for the \ttt{regularity} dataset.  In this case $y$ is \ttt{Regularity} (0/1 = \tsc{irregular}/\tsc{regular}) and $x$ is \ttt{WrittenFrequency}.
% 
% In the \texttt{givenness} data (described in detail \protect\hyperlink{givedata}{here}), whether stress shifted is captured by the continuous variable \texttt{acoustics}, which is a composite of prosodic measurements. We check using a simple logistic regression how the perceptual measure is related to the acoustic measure. That is:
% 
% \begin{itemize}
% \item
%   \(Y\) is \texttt{stressshift}
% 
%   \begin{itemize}
%   \tightlist
%   \item
%     \(=1\) if shifted, 0 if not
%   \end{itemize}
% \item
%   \(X\) is \texttt{acoustics}
% 
%   \begin{itemize}
%   \tightlist
%   \item
%     (centered + scaled)
%   \end{itemize}
% \end{itemize}

This code fits this model after  
%fit this model in R, first
rescaling \ttt{WrittenFrequency} (center and divide by 2 SDs) for interpretability:
%center the two continuous predictors and divide by 2 SD so their coefficients will be comparable with the others (

<<>>=
regularity <- mutate(regularity,
  written_frequency = rescale(WrittenFrequency))

logreg_mod_1 <- glm(Regularity ~ written_frequency,
  data = regularity, family = "binomial")
@

As for linear regression, we can see the model's results using either \ttt{summary()}, which is more standard but includes a lot of output besides the coefficient table of primary interest, or \ttt{tidy()} (from the broom package). 
%We'll use both as we go along.

<<>>=
tidy(logreg_mod_1)
@


%% FUTURE: clean up this box. just don't have time.
\begin{boxedtext}{Broader context: Logistic regression as a GLM}
\label{box:glm}

%Logistic regressions are fit in R using the \texttt{glm()} function with the option \texttt{family="binomial"}.
%Why? 




Logistic regression is one type of \emph{generalized linear model} (GLM): a family of models that look like linear regression, but with different choices for each part of equation \eqref{eq:logit} which make it look different from the equation for linear regression (equation~\ref{eq:linreg2}): the \textbf{link function} (here, logit) tying the right-hand side of the equation (the \emph{linear predictor}), to the probability \textbf{distribution} over $y$.
%(here, a bernoulli distribution). 
(In a linear regression, the link function is just the identity, and $y$ is normally distributed.)
%link function, linear predictor, and the probability distribution over $y$. 
GLMs let us model many different types of data just using (almost) the same tools as for linear regression; in R this is the \ttt{glm()} function.

Logistic regression assumes a binary response $y$, and uses the logit link function. Because the logit link is the most commonly used for binomially-distributed data (of which a binary response is a special case, with \(n=1\)), specifying \texttt{family\ =\ "binomial"} in \ttt{glm} gives a logit link by default (it's the same as writing \texttt{family\ =\ binomial(link\ =\ "logit")}).   Logistic regression can be characterized either as the probability of a binary outcome for each observation, or as modeling the number of `hits' for a Bernoulli distribution over \(n\) outcomes, for each set of observations that share the same predictor values. Both  are possible using
%(The latter describes a binomial distribution; hence the \ttt{binomial} flag.) The
\texttt{glm()} with \texttt{family\ =\ "binomial"}, but we only cover the binary outcome option.
%can handle data in either format (\(y_i\) binary or giving the number of 0's and 1's for a set of predictor values), and if the response variable in the data passed to \texttt{glm()} only has two values, R assumes that you're modeling a binary outcome. 
% 
% There is a further complication here: we have presented logistic regression as modeling data with a binary outcome, that is a \emph{Bernoulli} random variable \(y_i\) for the \(i^{\text{th}}\) observation, where we model \(P(y_i=1)\). Why is the option to \texttt{glm()} then \texttt{family\ =\ "binomial"}, given that a binomial distribution refers to \(n {\ge} 0\) trials with probability \(p\) of any outcome being 1? (The Bernoulli case is when \(n=1\).) It 

Logistic regression is the only type of GLM we will cover in this book, and is by far the GLM most commonly used for linguistic data.   But other types of GLM linear model are very useful and cover many other types of data. Some which are used in linguistics are:
\begin{itemize}
\item   \emph{Poisson regression} is used to model count data, where $y$ counts how many times an
%(rare) 
event occurs, such  as the number of times a word occurs in a corpus, or the number of times a participant gestures during a conversation \citep{winter2021poisson}.
%syllable types possible in each language in a typological sample
%\citep{winter2021poisson}.
%$y$ is assumed to follow a Poisson distribution, and the (most common) link function is log, so a Poisson regression models log-counts.
%The (most common) link function is log, so  in other words


\item \emph{Multinomial regression} is a generalization of logistic regression, where the response is one of $k$ discrete outcomes, 
%(for logistic regression $k=2$), 
such as different types of syntactic construction, or qualitatively different phonetic realizations, for the same meaning/sound (\citealp[][chap.~12]{levshina2015linguistics}; \citealp[][\S5.4.1]{gries2021statistics}).
%\citealp{davidson2016variability}). 
%Multinomial regression models are approximately equivalent to `maximum entropy models' which are widely used in phonology.
%

\item \emph{Ordinal (logistic) regression} is a special case of multinomial regression used to model a response which can take on several levels which have an intuitive `order', such as ratings on a discrete scale in psycholinguistic experiments (\citealp[][\S6.3]{baayen2008analyzing}; \citealp{liddell2018analyzing}).
%(\citealp[][\S6.3]{baayen2008analyzing}; \citealp{liddell2018analyzing}).
%\citep[e.g.,][]{scontras2017cross}; 
%(such as ratings on a 1--5 scale, or language proficiency = \tsc{basic}, \tsc{intermediate}, \tsc{advanced});
%this is is a special case of multinomial regression.  
%%The simplest and most common case is a \emph{proportional odds model}, which assumes that the predictors always affect the odds of being in level $i$ versus $i+1$ the same way (e.g., the experimental manipulation increases the relative odds of ratings 2/1 similarly to ratings 3/2).
%additional years of language training increases the relative odds of intermediate/basic similarily to advanced/intermediate).
\end{itemize}

Other kinds of GLM are 
%widely used in other fields and would be 
a good fit for different kinds of linguistic data, but 
%(to my knowledge)
are less widely 
used in linguistics, including gamma regression (for skewed continuous $y$) or log-normal regression (for right-skewed data where $y$ must be positive).
% \begin{itemize}
% \item \emph{Gamma regression}: for data where $y$ is continuous, but has a skewed distribution. is the most common kind of GLM used to model data where $y$ is continuous (as in linear regression) but has a skewed distribution (so a linear regression isn't appropriate).  Many phonetic and psycholinguistic measures (such as voice onset time, reaction times) follow this kind of distribution.
% %but gamma regressions are not currently widely used in linguistics.
% \end{itemize}

There is no comprehensive introduction to GLMs for linguistic data, but different sources cover the GLM framework generally or individual model types (references above, plus \citealp[][\S5.3--5.4]{gries2021statistics}; \citealp{coupe2018modeling}; \citealp[][chap.~13]{winter2019statistics}).
%\citealp[][chap.~13]{levshina2015linguistics}).  
\citet[][chap.~1--9]{faraway2016extending} is a general introduction to GLMs using R.  
% 
% \citealp[][chap.~5]{gries2013statistics}
% \citealp[][chap.~13]{winter2019statistics}
% \citealp{coupe2018modeling}, 
% 
% , poisson regression 
% (\citealp[][chap.~13]{winter2019statistics}, 
% \citealp[][chap.~5]{gries2013statistics}), multinomial regression (\citealp[][chap.~5]{gries2013statistics}, \citealp[][chap.~13]{levshina2015linguistics}), and ordinal regression \citep[][\S6.3]{baayen2008analyzing}.
%% FUTURE: breakpoint regression from Baayen, where?
% and \
% - Linguistic: Gries 2013 chap.~5 (10 pp: Poisson, multinomial); Levshina 2015 chap.~13 (10 pp on multinomial); Winter 2013 Ch 13 (15 pp on Poisson), Baayen 6.3 (15 pp on ordinal logistic regression and breakpoint regression)
\end{boxedtext}

<<echo=FALSE>>=
beta_0 <- coefficients(logreg_mod_1)[["(Intercept)"]]
p_0 <- invlogit(beta_0)
beta_1 <- coefficients(logreg_mod_1)[2]
odds_rat_1 <- exp(beta_1)
@

\subsection{Interpreting coefficients}
\label{sec:log-reg-ex-1}

We can interpret the model's coefficients in log-odds or probability.
The log-odds
%($\beta_0$)  
of regularity for a word with average frequency 
(\ttt{written\_frequency}=0) is \Sexpr{beta_0}, 
and increasing \ttt{written\_frequency} by 1 (i.e.,\ increasing frequency by two standard deviations) increases the log-odds of regularity by \Sexpr{beta_1}.

<<echo=FALSE>>=
b0 <- coefficients(logreg_mod_1)[["(Intercept)"]]
b1 <- coefficients(logreg_mod_1)[["written_frequency"]]
@


%and the corresponding probability, are:
%$\text{logit}^{-1}(\beta_0)$ is the probability:
% <<eval=FALSE>>=
% coefficients(logreg_mod_1)[['(Intercept)']]
% invlogit(coefficients(logreg_mod_1)[['(Intercept)']])
% @

% Increasing \ttt{written\_frequency} by 1 (i.e.,\ increasing frequency by two standard deviations) increases the log-odds of stress shifting by \Sexpr{beta_1}; equivalently, the odds of stress shifting are multiplied by \Sexpr{odds_rat_1}.
% 
% <<eval=FALSE>>=
% beta_1 <- coefficients(logreg_mod_1)[['written_frequency']]
% beta_1 # change in log-odds
% exp(beta_1) # odds ratio
% @

Turning to probability: the probability of regularity for the `average word' is  \Sexpr{p_0} (\ttt{invlogit(\Sexpr{beta_0})}).
Figure~\ref{fig:simple_logreg_ex} shows the predicted probability $P(\ttt{Regularity})$ as a function of \ttt{written\_frequency}, which shows the characteristic `s-shaped' logistic curve.  This plot illustrates why it is not straightforward to describe the effect of a predictor as a change in probability: the `slope' in probability space (literally, the slope of the s-shaped curve) changes depending on where on the curve you are. 

Nonetheless it is more intuitive to interpret logistic regression coefficients as changes in probability, so this is often done holding the predictor at  (1) its average value, or (2) the value which maximizes the slope (that is, where predicted probability = 0.5).

Let $p_0$ denote the predicted probability when all predictors are held at average values ($p_0 = \text{logit}^{-1}(\beta_0)$).  The slopes at points (1) and (2) are:
\begin{enumerate}
\item $\beta_1 \cdot p_0 (1-p_0)$
\item $\beta_1 / 4$
\end{enumerate}

It is also useful to know how to calculate a probability-space interpretation automatically.  One option is the `average marginal effect', using functionality from the {margins} package,  described in a later chapter (Section~\ref{sec:average-marginal-effects}):

<<>>=
margins(logreg_mod_1)
@

This is the slope of \ttt{written\_frequency} (in probability), averaging over the dataset.

% ---which we can calculate using 
% This is the   This is the `average marginal effect'.  The second gives the \textbf{maximum} value of the slope of \ttt{written\_frequency}.   Both are in probability space.  When the regression equation contains several predictors,  these options are two kinds of `marginalization' discussed in Box~\ref{box:marginal-effects}: the average marginal effect of a predictor, and the marginal effect at one representative value (MER)---the maximum effect of this predictor for this dataset (averaging over others).
% \end{boxedtext}


%The second which is $\beta_1/4$.  

%Even if you use Option (1), it is useful to remember the `divide-by-4 rule' to roughly interpret logistic regression slopes  \citet[][82]{gelman2007data}: the magnitude of the slope (in log-odds) is about 4x the maximum change in probability.

% plot_model(logreg_mod_1, type="pred", terms='written_frequency [all]') + geom_point(aes(x=written_frequency, y=as.numeric(Regularity)-1), regularity)

<<simple_logreg_ex, echo=FALSE,out.width='55%', fig.width=default_fig.width*.55/default_out.width, fig.cap='Predicted probability of verb regularity as a function of \\ttt{written\\_frequency} for model \\ttt{logreg\\_mod\\_1}, with 95\\% CIs (shading). Dotted lines are at the mean  of \\ttt{written\\_frequency} (left) and the maximum slope (right).'>>=
## set up dataframe with range of acoustics.std we want to predict over
newdata <- data.frame(written_frequency = seq(-2, 2, by = 0.01))
## get the model's predictions in log-odds space
newdata$predLogit <- predict(logreg_mod_1, newdata = newdata)
## transform those preditions to probability space
newdata <- newdata %>% mutate(predP = invlogit(predLogit))

## find where prob = 0.5
min_freq <- newdata[which.min(abs(newdata$predP - 0.5)), "written_frequency"]

plot_model(logreg_mod_1, type = "pred", terms = "written_frequency [all]", title = "") + geom_jitter(aes(x = written_frequency, y = as.numeric(Regularity) - 1), size = 0.5, height = 0.02, regularity) + geom_linerange(aes(x = 0, ymin = 0, ymax = 1), lty = 2) +
  geom_linerange(aes(x = min_freq, ymin = 0, ymax = 1), lty = 2) +
  xlab("Word frequency (standardized)") +
  ylab("P(regular)") + ylab("Predicted effect")

# ggplot(aes(x=written_frequency, y=predP), data=newdata) +
#   geom_line() +
#   geom_jitter(data=regularity, aes(y=(as.numeric(Regularity)-1)),
#               size=0.5, height=0.02) +
#   geom_linerange(aes(x=0, ymin=0, ymax=1), lty=2) +
#     geom_linerange(aes(x=min_freq, ymin=0, ymax=1), lty=2) +
#   xlab("Word frequency (standardized)") +
#   ylab("P(regular)") + ylab("Predicted effect")
@

For the current example, (1) and (2) correspond to the left and right dotted lines in Figure~\ref{fig:simple_logreg_ex}, and have values \Sexpr{invlogit(beta_0)*(1-invlogit(beta_0))*beta_1} and \Sexpr{beta_1/4} .


% The slope at (1) is the difference in probability between $x=0.5$ and $x=-0.5$, which can  be calculated using \ttt{predict} (with argument \ttt{type='response'} to predict probabilities rather than log-odds):
% <<>>=
% p1 <- predict(logreg_mod_1, newdata = list(written_frequency=0.5), type='response')
% p2 <- predict(logreg_mod_1, newdata = list(written_frequency=-0.5), type='response') 
% p1 - p2 
% @
% 
% The slope at (2) is $\beta_1/4$: \Sexpr{beta_1/4}.

The probability of verb regularity is thus predicted to decrease by \Sexpr{abs(invlogit(beta_0)*(1-invlogit(beta_0))*beta_1)}
%\Sexpr{abs(p1 - p2)} 
or \Sexpr{abs(beta_1/4)} as $x$ is changed by two standard deviations, which is a large effect.  Visually this corresponds to the s-shaped curve spanning most of probability space (0--1) for the observed range of $x$.


% 
% Let's work out in more detail what ``increase in log-odds'' means in probability space for this model:
% 
% \begin{itemize}
% \item
%   When \texttt{acoustics} = 0:
% 
%   \begin{itemize}
%   \item
%     log-odds = -0.68
%   \item
%     odds = 0.506
%   \item
%     \(P\)(stress shift) = 0.336 = \(\text{logit}^{-1}\)(-0.68)
%   \end{itemize}
% \item
%   When \texttt{acoustics} = 1:
% 
%   \begin{itemize}
%   \item
%     log-odds = 0.947
%   \item
%     odds = 2.58 = \(e^{0.947}\)
%   \item
%     \(P\)(y = 1) = 0.720 = \(\text{logit}^{-1}\)(0.947)
%   \end{itemize}
% \end{itemize}
% 
% Thus, the change in probability \(P\)(y = 1) when you increase \(X\) by 1 is 0.348 here (\(X: 0 \to 1\)) However, in general what change in probability results when \(X\) is increased by 1 (a ``unit change'') will depend on where you start from:
% 
% <<echo=FALSE, fig.align='center', fig.height=3, fig.width=3.5>>=
%     x <- seq(-2, 2, by=0.01)
% 
%     pred1 <- predict(mod1, newdata=data.frame(acoustics.std=x))
%     pred2 <- predict(mod1, newdata=data.frame(acoustics.std=x+1))
% 
%     df <- data.frame(x=x, probDiff = invlogit(pred2) - invlogit(pred1))
%     ggplot(aes(x=x, y=probDiff), data=df) + 
%       geom_line() + 
%       xlab("X") + 
%       ylab("Change in prob. for unit change in X")
% @
% 
% Note that the greatest change in probability occurs when \(P(Y=1|X) = 0.5\) (at \(X=0.69 \Rightarrow \beta_0 + \beta_1 X = 0\)). The change in probability here (\(\approx 0.4\)) is roughly 1/4 of the fitted slope value:
% \[
% \frac{\beta_1}{4} = \frac{1.637}{4} = 0.41
% \]
% It turns out that this relationship holds more generally, and the \emph{divide-by-4 rule} can be used to roughly interpret logistic regression slopes in terms of probability (\citet{gelman2007data}, p.~82): the slope (in log-odds) is about 4x the maximum change in probability.

As an example with categorical $x$, let's now fit a logistic regression
corresponding to the \ttt{diatones} example above (Section~\ref{sec:odds-ratios}): whether stress shift is more likely for words with a first-syllable coda. In this case $y$ is \ttt{stress\_shifted} and $x$ is \ttt{syll1\_coda} (0/1 = no coda/coda).
% 
% of whether a verb is regular as a function of whether the 
% \texttt{Auxiliary} is \tsc{hebben} or not, which we call \ttt{type}.\footnote{This division makes sense because the auxiliary ``hebben'' can be thought of as the default case. See \href{http://www.dutchgrammar.com/en/?n=verbs.au04}{here} (less technical) or ``Non-finite forms'' \href{https://en.wikipedia.org/wiki/Dutch_grammar}{here} (more technical) if interested.}
% 
% <<>>=
% regularity <- mutate(regularity, type=factor(ifelse(Auxiliary=='hebben',"hebben", "nonhebben")))
% @
% 
% \ttt{type} is a two-level factor, with values with levels \tsc{hebben} ($x=0$) and \tsc{nonhebben} ($x=1$).
% Here we have made \ttt{type} numeric and standardized (see ...), so its average value is 0 and \ttt{type}$>$0 corresponds to a non-\tsc{hebben} auxiliary. 
% 
% Let's now predict the probability of stress shifting (\texttt{stressshift}) for the \texttt{givenness} data, as a function of the NP type (\texttt{npType}). This predictor is ``dummy coded'', with values:
% 
% \begin{itemize}
% \item
%   \(X = 0\): Full NP
% \item
%   \(X = 1\): Pronoun
% \end{itemize}

The regression model is still equation \eqref{eq:logit}, where the intercept $\beta_0$  is the log-odds of stress shifting for a word without a coda and the slope  $\beta_1$ is the change in log-odds that stress shifting between words with and without a coda.

To fit this model and see its coefficients:

<<output.lines=10:13>>=
logreg_mod_2 <- glm(stress_shifted ~ syll1_coda, 
  data = diatones, family = "binomial")
summary(logreg_mod_2)
@


%is regular between \tsc{nonhebben} and \tsc{hebben} verbs. 

The predicted probabilities of stress shifting for \ttt{syll1\_coda} = 0 and 1 words are:

<<>>=
logreg_mod_2 %>% 
  predict(list(syll1_coda = c(0, 1)), type = "response")
@

% 
% \begin{itemize}
% \item
%   Full NPs: \(\text{logit}^{-1}(-0.832)=\) \textbf{0.303}
% \item
%   Pronouns: \(\text{logit}^{-1}(-0.832 + 0.435)=\) \textbf{0.402}
% \end{itemize}

We can compare to the observed proportion of stress-shifted words with and without a coda:
%verbs for each \ttt{type}:

<<>>=
diatones %>%
  group_by(syll1_coda) %>%
  summarise(prop_shifted = sum(stress_shifted) / n())
@

The model matches these proportions exactly, because
% This makes sense, as 
predicting these two numbers is all a model with one categorical predictor has to do.
%model has to do for simple logistic regression with one categorical predictor.



\begin{boxedtext}{Broader context: Logistic/linear regression differences, log-odds versus probability space}
\label{box:log-lin-reg-diffs}

There are two important differences between logistic regression and linear regression.
%for our purposes.

First, the response and what is being modeled are not the same thing as each other: in logistic regression, we observe \(y\), which takes on values 0 or 1, but we model the expected value of $y$:
%($E(y)$): 
the probability that \(y=1\).  In linear regression \(y\) is the response variable, which is also what the regression is modeling. 
%In logistic regression, these are not the same thing: we observe \(y\), which takes on values 0 or 1, but we model the expected value of $y$ ($E(y)$): the probability that \(y=1\).

The most important consequence is that ``logistic regressions are inherently more difficult than linear regressions to interpret'' \citep[][101]{gelman2007data}, because the model is only linear in a scale we're not interested in (log-odds), and is \textbf{nonlinear} in the actual scale of interest (probabilities).
%interpreting logistic regression models is trickier, 
%because we are interested in one `scale' (probabilities), but the model is fitted on another scale (log-odds). 
So when interpreting logistic regressions (e.g., plotting model predictions, interpreting model coefficients) we must always choose between (1) a more interpretable model (almost identical to linear regression) on an unintuitive scale and (2) an intuitive scale (probabilities) but a less-interpretable model. The same issue exists with any GLM.  The tradition in language sciences prefers (1):  just report logistic regression without much discussion of results in probability space, which assumes that results in log-odds are meaningful.  Interestingly, researchers in some other fields prefer (2) (e.g., political science, sociology, ecology: \citealp[][chap.\ 5]{gelman2007data}; \citealp[][chap.~4]{long2006regression}; \citealp[][\S9.2]{mcelreath2020statistical}): it is assumed that only results on the probability scale are of interest, so a nonlinear model's results must be interpreted, which in turn requires more subtle understanding of fitted models,
%(as `interactions among all variables': McElreath), 
and care in interpreting their results. For example, a significant interaction effect in log-odds is neither necessary nor sufficient for a meaningful interaction in \textbf{probability} \citep{berry2010testing}.  (2) seems like a reasonable default to consider for linguistic data as well,
%(we rarely actually care about log-odds),
but we focus on (1) given current practice.


The second difference is the presence of an error term. 
%The \textbf{presence of an error term}. 
In linear regression, there is an error term \(\epsilon_i\), capturing the difference between the fitted value (\(\hat{y}_i\)) and the actual value (\(y_i\)) for each observation, along with the linear predictor (\(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}\)). In logistic regression, there is no error term: the right-hand side of equation \eqref{eq:logit} predicts \(P(y_i=1)\) using just the linear predictor.

%These differences ultimately come from the fact that the thing being modeled (a Bernoulli random varia)
% FUTURE: so what is the takeaway? This paragraph seems didactic, why does this matter? (it does matter once you're dealing with any GLM predicting counts other than 0/1)
Why? As explained in Box~\ref{box:bernoulli-se}, 
for a Bernoulli random variable $y$, the expectation ($p$) and  variance ($p(1-p)$) only depend on one unknown parameter, $p$ (the probability of a `success'). %$p$
%\[
%E(Y) = p, \quad \text{Var}(Y) = p(1-p)
%\]
So unlike for linear regression, where the mean (\(\mu\)) and variance (\(\sigma\)) in our model of the response can be tweaked independently to better fit the data, in logistic regression these two things are not independent. By fitting the mean (probability that \(y=1\)), we have already determined the variance.

\end{boxedtext}

\section{Inference for logistic regression}
%: maximum likelihood}

% 
% \hypertarget{c4differences}{%
% \subsection{Differences from linear regression: Fitting and interpretation}\label{c4differences}}


%\hypertarget{fitting-a-logistic-regression-model}{%
%\subsection{Fitting a logistic regression model}\label{fitting-a-logistic-regression-model}}

\subsection{Parameter estimation: Maximum likelihood}

For linear regression, it was possible to estimate the regression coefficients by `least squares' (Section~\ref{sec:lr-parameter-estimation}): minimizing the difference between the model's prediction and the observed data (squared residuals),
%(\(\epsilon_i^2\)), 
across all points. For least-squares, 
%Least-squares has a solution which is simple to  `closed-form' solution:
the regression coefficients can be solved for by plugging the predictor and response values for all observations into an equation.

For logistic regression, there are no residuals (Box~\ref{box:log-lin-reg-diffs}), so we can't use a least-squares method. Instead, we use a method based on the \emph{likelihood}: the probability of the observed data as a function of values of the regression coefficients.
A \emph{maximum likelihood} (ML) method finds the values of regression coefficients that maximize this probability. 
% (e.g., confidence intervals) reflect how `sharp' the likelihod confidence intervals
% \emph{likelihood}-based method: determine the probability of the observed data, given values of the regression coefficients (the data \emph{likelihood}), then find the regression coefficient values that maximize this probability.

For simple logistic regression: for any \(\beta_0\) and \(\beta_1\), the likelihood of the data is:
\begin{equation}
  L(\beta_0, \beta_1) = \prod^n_{i = 1} p_i^{y_i} (1-p_i)^{1-y_i},
  %
      \label{eq:likelihood}
\end{equation}
where $p_i$ is the predicted probability for the $i$th observation:
\begin{equation*}
p_i =  \text{logit}^{-1}(\beta_0 + \beta_1 x_i), \quad i = 1, \ldots, n
\end{equation*}

We choose \(\hat{\beta}_0, \hat{\beta}_1\) that maximize the likelihood: the ML estimates. There is no equation to determine
%form solution for 
\(\hat{\beta}_0, \hat{\beta}_1\), which must instead be estimated numerically, using an estimation algorithm.  The \ttt{glm()} function 
uses the Fisher scoring algorithm by default.

\subsection{Uncertainty}
\label{sec:log-reg-uncertainty}

There are two approaches to quantifying uncertainty in the coefficients, and thus obtaining standard errors, confidence intervals, and $p$-values: Wald tests and likelihood methods.  %While this topic feels pretty technical, 
It is worth 
%at least 
knowing that both methods exist, because the former is used by default in R in many settings (e.g., mixed models), while the latter is typically recommended for use whenever possible. 
%If you are willing to treat how uncertainty (and hence $p$-values, etc.) is calculated for regression models in R as a black box, you could skip this section.

\subsubsection{Wald test approach}
\label{sec:log-reg-wald-test}

This approach assumes that coefficient estimates are normally distributed, which is true when estimating log-odds for large enough samples. Thus, a standard error can be obtained for each coefficient, and the test statistic of the form mean/SE  is (approximately) normally distributed \citep[][\S5.5.2]{agresti2003categorical}. For example for $\beta_1$, the test statistic is:
%Thus a linear regression is basically being beformed on log-odds, and standard errors can thus be obtained just as for 
% 
% 
% Standard erorrs for each coefficient can also be obtained by... ; this approximation works because estimates that each coefficient to be estimated is normally distributed. This is legitimate for large samplesSince we are estiamting this is the advantage of 
\begin{equation}
    z = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} 
    \label{eq:wald}
\end{equation}
%turns out to be approximately normally distributed (e.g.,~\citet[][\S5.5.2]{agresti2003categorical}.
%

As for any normally-distributed test statistic we can perform a \emph{Wald test} of the null hypothesis that $\hat{\beta}_1 = 0$, i.e.,\ that there is  no relationship between $P(y=1)$ and $x$.  From this test we obtain a $p$-value and confidence interval for the slope 
$\hat{\beta}_1$.
(The same is true for the intercept $\hat{\beta}_0$.) This is the method used in \ttt{glm()} to calculate the standard errors, $z$-values, and $p$-values shown in the model summary (e.g., \ttt{logreg\_mod\_2} above).  The corresponding confidence intervals (which are not the default) can be calculated using \ttt{confint.default()}:

<<>>=
confint.default(logreg_mod_2)
@


% %\[
% H_0~:~\hat{\beta}_1 = 0
% \]

\subsubsection{Likelihood approach}
\label{sec:likelihood-logistic-reg}

The likelihood approach calculates $p$-values by dropping terms from the model using a \emph{likelihood-ratio test}, analogously to how model comparison was performed for linear regression models, where the full model was compared to the intercept-only model, and an $F$-test was used to test whether the change in sum-of-squares was significant, given the added predictors in the full model (Section~\ref{sec:nested-model-comparison}). Logistic regression models don't have sums-of-squares because they don't have residuals. Instead, we compare the likelihood (equation \ref{eq:likelihood}) of the two models: Is the change in likelihood between \(M_0\) and \(M_1\) significant?

Likelihood methods typically use log-transformed likelihood, to avoid numerical issues when computing with very small numbers. It is also customary to multiply by -2 and work with the 
%Instead of working with straight log-likelihood, it is also customary 
%to define the 
\emph{deviance} of a model \(M\) \citep[][\S3.4.3]{agresti2007}:
%as:
\begin{equation*}
  D = -2 \log L(M)
\end{equation*}
Thus, a better model (higher likelihood) has lower deviance.  To get some intuition, we can substitute in for $L(M)$ from above (equation~\ref{eq:likelihood}):

\begin{align}
D & = -2 \sum_{i=1}^{n} \log \hat{p}_i^{y_i} (1-\hat{p}_i)^{1-y_i} \\
& =  \sum_{1=1}^{n} 2  [y_i \log(1/\hat{p}_i) + (1-y_i) \log(1/(1-\hat{p}_i))]
\label{eq:deviance-error}
\end{align}

So the deviance sums up an `error' from each observation (the part in brackets in equation~\ref{eq:deviance-error}). For $y=1$ observations, this error is  is zero when $p=1$ (perfect prediction) and larger the further $p$ is from 1. For $y=0$ observations, this error is zero when $p=0$ (perfect prediction) and larger the further $p$ is from 0.

%% the justification for deviance is in Faraway Extending book as well and makes sense


%% FUTURE: Michaela comment: This section (particularly this page) seems to bounce around
 %a bit; can it be streamlined? the terms 'likelihood-ratio test' and 'profile method' seem like they should be highlighted more/earlier.  (MS: I got in LR test earlier, but otherwise not changed due to time.)
Returning to model comparison: the difference in deviance (\(\Delta D\)) between models with (\(M_1\)) and without (\(M_0\)) differing in a single predictor $x$ is related to the log of their \emph{likelihood ratio} (LR):
\begin{equation}
  \Delta D = -2 \log \frac{L(M_0)}{L(M_1)}
  \label{eq:lr-definition}
\end{equation}

For large enough sample size, \(\Delta D\) follows a \(\chi^2(1)\) distribution if the slope of $x$  is 0. Thus, we can use \(\Delta D\) as a test statistic for the null hypothesis that $\beta_1 = 0$. This kind of test is called a {likelihood-ratio test}, and gives a $p$-value.  

The corresponding confidence interval uses a \emph{profile method}:
%, which is a bit trickier to think about: 
with all coefficients except $\beta_1$ held at the values which maximize likelihood, find the values of $\beta_1$ such that a likelihood-ratio test (LRT) dropping $x$ would reject the null hypothesis.  

For example, we can calculate a LRT-based $p$-value and 95\% CI for the \ttt{syll1\_coda} slope in model \ttt{logreg\_mod\_2}, using the useful function \ttt{drop1()} for excluding a single term from a model:

<<output.lines=5:7>>=
drop1(logreg_mod_2, "syll1_coda", test = "Chisq")
@

<<>>=
confint(logreg_mod_2)["syll1_coda", ]
@

Note that this method gives a slightly larger $p$-value/larger CI than the Wald-test-based method used above: 
<<echo=FALSE>>=
tibble(
  method = "Wald",
  p = summary(logreg_mod_2)$coefficients["syll1_coda", "Pr(>|z|)"],
  confint_width = sum(abs(confint.default(logreg_mod_2)["syll1_coda", ]))
) %>%
  add_row(
    method = "LR",
    p = drop1(logreg_mod_2, "syll1_coda", test = "Chisq")[["Pr(>Chi)"]][2],
    confint_width = sum(abs(confint(logreg_mod_2)["syll1_coda", ]))
  )
@

The likelihood ratio method is more conservative in general.


% 
% \begin{boxedtext}{Practical notes: calculating and reporting LR tests}
% 
% A report of a likelihood-ratio test should contain (following Sec ...) the test statistic, degrees of freedom, effect size, and $p$-value.  In practice, the effect size is often omitted unless the LR test is for a single coefficient of a regression (in which case the coefficient estimate is an effect size).  For example the LR test just above could be reported as:
% 
% \begin{quote}
% A first-syllable coda did not significantly the likelihood of stress shift, assessed by a likelihood-ratio test ($\Delta D = 3.3$, $df=1$, $p=0.07$)
% 
% \end{quote}
% 
% FUTURE (?): add this stuff when discussing mixed models, where it's more relevant... not clear to me how to do `effect size' in general corresponding to an LR test
% 
% - FUTURE (?): example of calculating LR $p$-values and CIs.
% 
% - Include note that `a likelihood-ratio test...' is often used as shorthand for `a chi-squared test with $df = p$ applied to the difference in deviance between two nested models which differ in $p$ terms'.  (and sometimes `model comparison' is used as shorthand for this or F-test depending on context.)
% 
% \end{boxedtext}

%The \(p\) value of this test gives a measure of goodness of fit, while \(\Delta D\) gives a sort of measure of variance accounted for (if you think of deviance as capturing ``variance'' from the perfect model).


\begin{boxedtext}{Practical note: Does it matter how we get $p$-values?}

% Does it matter which method is used to obtain $p$-values/confidence intervals?  This situation comes up often: 
% %This is our first case of a situation which will come up often: 
% for regression models beyond the simplest linear regressions, there are often different ways of calculating the `same thing' (especially for hypothesis testing), which tend to vary from `easier/faster but less accurate' to `harder/slower but very accurate'.

The likelihood and Wald test-based methods will give very similar results for large samples. The Wald test can be calculated instantaneously, but relies on a large-sample approximation which is poor even for medium-sized samples
%The Wald test assumes a 
%`large-sample approximation' and is what is assumed by \texttt{glm} in R in calculating logistic regression coefficient significances.  This approximation is poor for small enough samples 
(\(n < 100\) is sometimes given), or very large \(|\beta|\).  For these reasons \citet[][\S5.2.1]{agresti2003categorical} recommends always using a likelihood-ratio test 
%(described below: Section~\ref{sec:likelihood ratiot})
to assess coefficient significances.  But a likelihood-ratio test takes longer because it requires fitting two models. Similarly, a likelihood-based confidence interval is preferable to a Wald-based confidence interval, but takes longer to compute because profile methods are slow.  For the example here speed isn't an issue (small dataset / simple model), but in more realistic cases it quickly becomes an issue.

The R defaults for logistic regression reflect this tension. By default \ttt{summary()} shows Wald-based $p$-values, while  \ttt{confint()} shows likelihood-based confidence intervals.
%(using) are shown.
%presumably so the model summary can be displayed faster in all cases.

This kind of situation comes up often: for most regression models
%This is our first case of a situation which will come up often: 
%for regression models beyond the simplest linear regressions, 
there are  different ways of calculating the `same thing' (especially for hypothesis testing), which tend to vary from `easier/faster but less accurate' to `harder/slower but very accurate'.
\end{boxedtext}

% 
% \footnote{\texttt{glm()} uses the \emph{Fisher scoring algorithm} by default, hence the reference to ``Number of Fisher Scoring iterations'' in the \texttt{summary()} output of a model fitted with \texttt{glm()}. }

% 
% \begin{boxedtext}{Practical note: separation}
%   If you try to fit a logistic regression model with just \(y=1\) response values (no \(y=0\) values), you'll get the message \texttt{glm.fit:\ algorithm\ did\ not\ converge}:
% 
% 
% <<>>=
% badmodel <- update(logreg_mod_2,  data=filter(diatones, stress_shifted=1)
% @
% 
% This means that the numeric algorithm used to maximize likelihood couldn't find a good solution for the regression coefficients. This makes sense, because the model is trying to fit 100\% probabilities (so log-odds = \(\infty\)), and has no data on the basis of which it can estimate the slope (\(\beta_1\)).
% 
% \end{boxedtext}
% 
% 
% \hypertarget{interpretation}{%
% \subsection{Interpretation}\label{interpretation}}
% 
% You of course don't need to understand the math to fit \texttt{lm()} or \texttt{glm()} models---R will do this for you automatically. However, it can be helpful to know some basic aspects of how these models are fitted to interpret model output, and some otherwise cryptic error messages. For example:
% 
% \begin{itemize}
% \item
%   \texttt{(Dispersion\ parameter\ for\ binomial\ family\ taken\ to\ be\ 1)} in logistic regression output: related to the relationship between mean and variance---the amount of variability in the data is assumed to be similar to what you'd expect based on the estimated mean. If it's not, your data is ``over-dispersed'' or ``under-dispersed''.\footnote{Dispersion is not a concern for logistic regression models where each observation is just 0 or 1, the only kind of logistic regression covered in this chapter. (As opposed to more complex flavors, like observing proportions between 0 and 1 over multiple observations.)}
% \item
%   \texttt{Number\ of\ Fisher\ Scoring\ iterations:\ X} in logistic regression output: refers to the numeric algorithm used to fit logistic regression models (see footnote 3).
% \item
%   There is no \(F\) statistic or \(p\) value in logistic regression output: this is because there are no residuals, so the ``full'' and ``reduced'' models cannot be compared via their residual sums of squares (which are used to conduct an \(F\) test for linear regression).
% \item
% 
% A few notes about the model output, compared to the output of the linear regression models we've seen so far:
% 
% \begin{itemize}
% \item
%   There are no references to ``residuals'' or ``residual standard error''. This is because the logistic regression equation Eq. \eqref{eq:logit} has no error term, so there are no residuals.
% \item
%   There are no ``R-squared'' values. \(R^2\) is not a well-defined concept for logistic regression, for which different measures of goodness of fit are used (discussed \protect\hyperlink{logistic-regression-pseudo-r2}{below}).
% \item
%   The model table contains \texttt{z\ statistic} and \texttt{Pr(\textgreater{}\textbar{}z\textbar{})} columns rather than \texttt{t} and \texttt{Pr(\textgreater{}\textbar{}t\textbar{})} columns. This is because the test statistic \(\hat{\beta_i}/{SE(\hat{\beta}_i})\) for logistic regression (\(z\) in Eq. \eqref{eq:wald}) looks like a z-score and \protect\hyperlink{ux5cux23log-reg-hyp-test}{follows a normal distribution}, while the test statistic follows a \(t\) distribution for linear regression.
% \end{itemize}
% 
% % 
% % \begin{boxedtext}{Broader context: linear/logistic regression differences and model output}
% % \label{box:log-lin-reg-diffs}
% % 
% % There are two important differences between logistic regression and linear regression, for our purposes:
% % 
% % 
% % \begin{enumerate}
% % \def\labelenumi{\arabic{enumi}.}
% % \item
% %   The \textbf{relationship between the response and what's being modeled}. In linear regression, these are the same thing: \(Y\) is the response variable, which is also what the regression is modeling. In logistic regression, these are not the same thing: we observe \(Y\), which takes on values 0 or 1, but we model the \emph{expected value} of \(Y\) (\(E(Y)\)): the probability that \(Y=1\).
% % \item
% %   The \textbf{presence of an error term}. In linear regression, there is an error term \(\epsilon_i\), capturing the difference between the fitted value (\(\hat{y}_i\)) and the actual value (\(y_i\)) for each observation, along with the linear predictor (\(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}\)). In logistic regression, there is no error term: the right-hand side of Eq. \eqref{eq:logit} predicts \(P(Y=1)\) using just the linear predictor.
% % \end{enumerate}
% % 
% % Why? As explained in Box~\ref{box:bernoulli-se}, for a Bernoulli random variable $y$, the expectation ($p$) and  variance ($p(1-p)$) only depend on one unknown parameter, $p$ (the probability of a `success'). %$p$
% % %\[
% % %E(Y) = p, \quad \text{Var}(Y) = p(1-p)
% % %\]
% % So unlike for linear regression, where the mean (\(\mu\)) and variance (\(\sigma\)) in our model of the response can be tweaked independently to better fit the data, in logistic regression these two things are not independent. By fitting the mean (probability that \(Y=1\)), we have already determined the variance.
% % 
% % Both differences make interpreting and fitting logistic regression models less straightforward than linear regression models, 
% % 
% % 
% % \end{boxedtext}

%\hypertarget{evaluating-logistic-regression-models}{%
\section{Goodness of fit}
\label{sec:gof-logistic}
%\label{evaluating-logistic-regression-models}}

We would like a measure of goodness-of-fit for logistic regression models: How well does the model predict $y$?
%\(Y\), compared to a baseline model? 
For linear regression, we used \(R^2\) to quantify how similar the model's predictions (\(\hat{y}_i\)) were to the observations (\(y_i\)), relative to a baseline model where the model's prediction is always the grand mean---\(R^2\) was simply the correlation between observed and predicted values.

However, for logistic regression we cannot directly compare model predictions (\(p_i\): (log-odds of) probabilities) to observations (\(y_i\): 0 or 1), so we need a different measure of goodness-of-fit. We discuss a few options.

\subsection{Classification-based measures}
\label{sec:class-based-measures}

The simplest way to assess goodness-of-fit comes from thinking of the model as a classifier which predicts $y$, as follows:
\begin{equation*}
  \hat{y}_i =
  \begin{cases}
    1 & \text{if } \hat{p}_i > k \\
    0 & \text{if } \hat{p}_i \leq k,
  \end{cases}
\end{equation*}
where $k$ is some threshold, typically $k = 0.5$.

That is, we just predict \(y=1\) if the predicted log-odds are positive, and \(y=0\) if the predicted log-odds are negative. We can then define \emph{classification accuracy} as the percentage of observations where the predicted and observed values are the same (\(\hat{y}_i = y_i\)).  Classification accuracy needs to be compared to a baseline, usually ``how often would we classify correctly if we chose the most common case?''


%Unfortunately no one R package contains all widely-used measures; here we use a few common packages (\ttt{ModelMetrics},  \ttt{caret}, \ttt{performance}).
%but many such packages are available.)

Although quantities like this can be calculated ourselves with simple commands, it is good to start using packages for evaluating the quality of regression models which will generalize to more complex models.
%and which often do clever things by default that we wouldn't have thought of.
In this case we use \ttt{performance\_pcp()} from the {performance} package, which calculates the {expected} classification accuracy on unseen data (using threshold $k=0.5$), which is what we really care about, rather than just calculating the classifier's performance on this data (see \ttt{?performance\_pcp}; \citealp[][90--92]{herron1999postestimation}).\footnote{Expected classification accuracy is the sum of $\hat{p}_i$ for $y=1$ observations and $1-\hat{p}_i$ for $y=0$ observations, divided by $n$. Correct predictions count more the closer $\hat{p}_i$ is to 0 or 1.}  For the \ttt{regularity} model:

<<output.lines=1:5>>=
performance_pcp(logreg_mod_1)
@




%## put model predictions into format needed by confusionMatrix
%mod_1_preds <- factor(ifelse(logreg_mod_1$fitted.values>0.5, 'regular','irregular'))
%confusionMatrix(data=mod_1_preds, reference = regularity$Regularity)

The classification accuracy of the model with its 95\% confidence interval is shown first (`Full model'), followed by the baseline accuracy (`Null model', which consists of an intercept only).
%This function outputs a number of performance metrics for the 2x2 confusion matrix of predicted versus reference values.  The classification accuracy  
%(the `No Information Rate'). 
The model's improvement over baseline is a measure of effect size, and seems substantial ($\sim$6\%). However the 95\% confidence intervals overlap, suggesting the model does not give `significantly' better accuracy than the baseline.  
%though what a `large' increase in accuracy is depends on the setting.   More im
% Whehter the improvement is statistically significant can be quantified by a
% %In terms of significance, the 95\% CI overlaps with the baseline accuracy, meaning the model does not give `significantly' better accuracy than the baseline.  This intuition can also be quantified with a
% $p$-value using a hypothesis test of whether the accuracy is higher than the baseline value (a one-sided binomial test); here $p$=...

%
%
% % 
% % 
% % Although the classification accuracy seems high,
% % %(\Sexpr{round(class_act_df$accuracy,2)*100}\%), 
% % it is barely higher than the baseline.  Whether the difference in accuracy is `significant' can be formally tested by McNemar's test (\ttt{mcnemar.test})
% %   
% % which is very similar to a chi-squared test calcualting independence of the classifier used (non-baseline/baseline) and response accuracy (accuate/inaccurate) in a 2x2 contingency table (could add this back in, but I think not needed)
% 
% %(\Sexpr{round(class_act_df$baseline,2)*100}\%).  
% % 
% 
% % 
% % For our \protect\hyperlink{c4ex1}{Example 1} (\texttt{stresshift\ \textasciitilde{}\ acoustics.std}), the classification accuracy is
% % 
% % <<>>=
% % lrAcc(mod1, 'stressshift')
% % @
% % 
% % Compared to a baseline of 0.65 (using \texttt{baselineAcc(mod1,\ \textquotesingle{}stressshift\textquotesingle{})}). The model performs better than the baseline, but its performance is less impressive than it seems (70\%) given that the most common case (not shifting stress) accounts for 65\% of the data.
% 
% %%%%%
% 
% %However, classification accuracy alone is not a good metric for goodness of fit of a logistic regression---or the performance of a classifier, more generally.
% 
Although widely used for linguistic (and other) data in classification tasks, accuracy alone is not a good metric for classifier performance,
%of a classifier (including logistic regression),
because it
%logistic regression---or the performance of a classifier, more generally.
%
%accuracy alone is not a good description of classifier performanc%e.
%Accuracy
hides important information,
%about  well a classifier does,
especially when class sizes are very different.  
%First, assume the threshold $k$ is fixed, say at $0.5$.  
Accuracy doesn't distinguish between false positives and false negatives (i.e.\ Type I/II errors: Section~\ref{sec:type-i-ii-power})---accuracy can be very high just because one class is more frequent.  So when reporting performance of a classifier it is better to report two or more values which better summarize the trade-off between false positives and negatives, such as sensitivity and specificity (true positive and true negative rates, for a given threshold $k$), or \emph{area under the ROC curve} (AUC), which summarizes performance as $k$ is varied, trading off true positives for true negatives.\footnote{ROC stands for `receiving operator characteristic', but it is typically just used as an acronym (like `laser').}



%In computational linguistics it is common to report at least accuracy (because it's intuitive) and F1 or AUC (better measures) for binary classification tasks, both of which lie between 0 and 1 (perfect classifier).

%The Somers $D_{xy}$ measure mentioned in the text is closely related to AUC ($D_{xy} =  2 AUC - 1$), and so can be thought of as summarizing a logistic regression's performance when used as a classifier with different thresholds ($k$).

% For example, for the \ttt{logreg\_mod\_1} example, precision is the proportion of verbs labeled \tsc{irregular} which are in fact irregular ( 0.63), and recall is the proportion of irregular verbs which are labeled \tsc{irregular} ( 0.25); F1 is 0.35 ().
% In other words, if the classifier labels a verb as \tsc{irregular} it is often correct, but many irregular verbs are missed; the classifier is not very good overall (low F1).



% One issue is that accuracy evaluates the model as a binary classifier when the model actually predicts probabilities.  But even for evaluating a classifier, accuracy alone is not a good measure (Box~\ref{box:evaluating-classifiers}).
%
% \begin{boxedtext}{Broader context: Evaluating classifiers}
% \label{box:evaluating-classifiers}
%
% We do not cover classification in this book, but regression models are often used as classifiers for lingusitic data, and classification  metrics are often useful to evaluate regression models with discrete outputs (such as logisic regression). Many different metrics are available to deal with different cases (e.g., binary versus multiple outcomes);  you can read more e.g., ....   Just for a binary classification problem there are many possible metrics which quantify different aspects of performance, such as those in the output of \ttt{confusionMatrix} in the text.
%
% The most important takeaway is that \emph{accuracy alone is not a good description of classifier performance}.
% Although widely used for lingusitic (and other) data in classification tasks,
% %accuracy alone is not a good description of classifier performanc%e.
% accuracy hides important information about how well a classifier does, especially when class sizes are very different.  First, assume the threshold $k$ is fixed, say at $0.5$.  Accuracy doesn't distinguish between false positives and false negatives (REF to Table in chap.~2 in TI vs TII error?)---accuracy can be very high just because Class 1 is frequent.  So when reporting performance of a classifier it is better to report two or more values which better summarize the tradeoff between Type I and Type II error, such as sensitivity and specificity (true positive and true negative rates, for a given threshold $k$), \emph{area under the ROC curve} (AUC: summarizes performance as $k$ is varied, trading off true positives for true negatives), \emph{precision} and \emph{recall} (proportion of Class 1 predictions which are correct, proportion of Class 1 data which is correctly labeled),  or \emph{F1} (which combines them).  In computational linguistics it is common to report at least accuracy (because it's intuitive) and F1 or AUC (better measures) for binary classification tasks, both of which lie between 0 and 1 (perfect classifier).
% % 
% % The Somers $D_{xy}$ measure mentioned in the text is closely related to AUC ($D_{xy} =  2 AUC - 1$), and so can be thought of as summarizing a logistic regression's performance when used as a classifier with different thresholds ($k$).
% % 
% % For example, for the \ttt{logreg\_mod\_1} example, precision is the proportion of verbs labeled \tsc{irregular} which are in fact irregular (0.63), and recall is the proportion of irregular verbs which are labeled \tsc{irregular} ( 0.25); F1 is 0.35.
% % In other words, if the classifier labels a verb as \tsc{irregular} it is often correct, but many irregular verbs are missed; the classifier is not very good overall (low F1).
% % %and \tsc{irregular} verbs labeled correctly (0.26), and  specificity is the proportion of \tsc{regular} verbs labeled correctly (0.95).  In other words, the classifier correctly labels most regular verbs but mislabels many irregular  is the percentage of \tsc{regular} predictions which are correct (0.95); recall is the percentage of \tsc{regular} verbs which are correctly labeled (0.81).  
% % 
% % 
% % % 
% % % As the threshold $k$ is changed, the classifier trades off between false positives and false negatives, so we may want a measure that summarizes performance across all possible thresholds
% % % 
% % % - accuracy is one metric for how well a classifier does, but hides important variation. First, assume the threshold fixed, say $k=0.5$.  Can see this from table in Ch 2 on Type I vs Type I errors -- accuracy doesn't distinguish between false positive versus false negatives, or the similar tradeoffs of precision/recall or sensitivity/specificity.  Can have high accuracy just because one class very frequent, etc.  So it's common to report 2 or more values which better summarize, e.g., accuracy, precision, and recall -- common in comp ling.
% % % 
% % % - as threshold changes, tend to trade off between FP/FN or precision/recall (give intuition).  So we may want a measure that summarizes performance across all possible thresholds: this is *AUC* (area under RO curve).  Related to $D_{xy}$ as $AUC = D_{xy}/2 + 0.5$: ranges from 0.5 to 1.
% % 
% % % 
% % % - read more: many places
% % 
% % \end{boxedtext}
% 

We will report AUC
%(because it's intuitive) and AUC (a better measure) 
by default.  AUC lies between 0.5 and 1, where the baseline model has AUC=0.5.  AUC can be calculated using \ttt{auc()} in the {ModelMetrics} package, for example for the \ttt{regularity} model:

<<echo=1>>=
auc(logreg_mod_1)
auc_ex <- auc(logreg_mod_1)
@

This value of AUC suggests reasonably good fit even taking the skew of observed values (\tsc{regular} verbs more common) into account.

% FUTURE: michaela -- this paragraph is fairly dense and hard to follow
\begin{boxedtext}{Broader context: AUC intuition}
\label{box:auc-intuition}

In the logistic regression context, where probabilities are being predicted, it may be more intuitive to think of AUC (rather than a classification metric) as 
%another way to think of AUC instead of a classification metric is in terms of the 
%AUC is widely used as a classification metric, but its usual interpretation (area under the ROC curve) makes less sense in the logistic regression context, where probabilities are being predicted. It may be more intuitive to think of it as 
related to the (rank) {correlation between predicted probabilities  and observed responses}---this is called \emph{Somers' $D_{xy}$} \citep[][\S10.8]{harrell2015regression}, and is widely reported in linguistics (e.g.,~\citealp{baayen2008analyzing}).  As a correlation, $D_{xy}$ ranges between -1 and 1; AUC for a logistic regression model is just  $D_{xy}$ scaled to lie between 0 and 1 ($AUC = D_{xy}/2-0.5$).  Because we can always make a correlation lie between 0 and 1 by just switching the $y=0$ and $y=1$ labels, it is usually assumed that $D_{xy}$ lies between 0 and 1; hence AUC lies between 0.5 and 1. (AUC below 0.5 would mean predicting worse than randomly guessing, but for a regression model used as a classifier this shouldn't happen, since just setting all non-intercept terms to 0 already gives AUC=0.5.)

\end{boxedtext}


% $D_{xy} = \Sexpr{auc_ex}$, suggesting reasonably good fit even taking the skew of observed values (\tsc{regular} verbs more common) into account.
% %(Note that the `baseline' model would have $D_{xy} = 0$...)

\subsection{Pseudo-$R^2$ measures}
%measures}

%Another type of goodness-of-fit metric,
%Classification-based measures like accuracy and AUC lie between some lower bound (0, 0.5) and 1, which maintains one nice property of $R^2$ for linear regression---being able to read off what a `good' and `bad' model are from the measure.  Another type of accuracy measures for logistic regression are 
\emph{Pseudo-$R^2$ measures} of goodness-of-fit
%maintain a different property of $R^2$: 
quantify the degree of `variance' accounted for by the model, which was one interpretation of $R^2$ for linear regressions.  These `pseudo' measures replace variance  by likelihood to define $R^2$, since there is no variance for a logistic regression.

% 
% There are better measures of goodness of fit for a logistic regression which maintain at least one of the interpretations of $R^2$ for a linear regression:
% 
% \begin{enumerate}
% \item
%   Fraction reduction in sum-of-squares
% \item
%   Degree of `variance' accounted for 
% \item
%   Squared correlation between fitted and observed values 
%   %(different scales: probabilities versus 0/1)
% \end{enumerate}

% \emph{Somers' $D_{xy}$} is the rank correlation bewteen the  predicted probabilities ($p_i$) and observed values ($y_i$: 0/1). This is a non-parametric statisic
% %analagous to Kendall's $\tau$: REF-back), 
% in the spirit of property (3): it ranges from 0 (no association) to 1 (perfect association), and is zero for the baseline model containing only an intercept. 
% 
% Thus, $D_{xy}$ can be calculated using an \ttt{somers2} function in \ttt{Hmisc}.  For the \ttt{regularity} example: 
% 
% <<echo=1>>=
% somers2(logreg_mod_1$fitted.values, logreg_mod_1$y)
% somers_ex <- somers2(logreg_mod_1$fitted.values, logreg_mod_1$y)
% @
% 
% $D_{xy} = \Sexpr{somers_ex[['Dxy']]}$, suggesting reasonably good fit even taking the skew of observed values (\tsc{regular} verbs more common) into account.


The most popular pseudo-$R^2$ is
%Another popular measure is 
\emph{Nagelkerke's $R^2$} \citep[e.g.,][\S2.6]{faraway2016extending}.
%a `pseudo-$R^2$' measure in the spirit of property (2). 
%Since there is no `variance' for a logistic regression, only likelihood, 
The measure captures the relative likelihoods of the fitted model ($\hat{L}$) and a baseline model (including only the intercept: $\hat{L}_0$), scaled so that the measure  lies between 0 and 1:
\begin{equation*}
R_{n}^2 = \frac{1 - (\hat{L}_0/\hat{L})^{2/n}}{1 - \hat{L}_0^{2/n}}
\end{equation*}

$R_{n}^2$ can be calculated using \ttt{r2\_nagelkerke()} in the {performance} package (which also computes other pseudo-$R^2$ measures, such as Cox-Snell, McFadden, and Tjur). For the \ttt{regularity} example:

<<>>=
r2_nagelkerke(logreg_mod_1)
@


%(\citealp[][\S2.6]{faraway2016extending} gives  some intuition). 
% Various R packages compute $R_{n}^2$, but to avoid importing a package for just one function let's use the formula above to write a function to compute $R_{n}^2$, and apply it to the \ttt{regularity} example:

% <<>>=
% nagelkerke_r2 <- function(mod){
%   n <- nrow(mod$data)
%   L <- (-mod$dev/2) # likelihood
%   L0 <- (-mod$null/2) # likelihood of baseline
%   (1-(L0/L)^(2/n))/(1-(L0^2)^(1/n))
% }
% nagelkerke_r2(logreg_mod_1)
% @

% the measure is defined in terms of the

Because no  pseudo-$R^2$ measure has all the properties of $R^2$ for linear regression, interpreting their values is not intuitive, and they are not recommended as a goodness-of-fit measure except as a way of comparing different logistic regression models. Nonetheless $R_n^2$ 
%, Nagelkerke's $R^2$ 
is widely reported in linguistics. 
% 
% The low $R_{n}^2$ is typical---while technically Nagelkerke's $R^2$ can lie between 0 and 1, it tends to be low even for `good' models,
% % REF for that would be Faraway again
% making its interpretation different from $R^2$ for linear regressions (where a `good' model will have $R^2$ near 1). This is one reason Nagelkerke's $R^2$ (or other pseudo-$R^2$ measures) is not recommended as a goodness-of-fit measure except as a way of comparing different logistic regression models. Nonetheless it is widely reported in linguistics.





% 
% \begin{itemize}
% \item
%   \textbf{Cox-Snell} pseudo-\(R^2\): a value \(\geq 0\)
% \item
%   \textbf{Nagelkerke} pseudo-\(R^2\): a value between 0 and 1 (like \(R^2\) for linear regression)
% \end{itemize}
% 
% Both measures are related to the likelihood ratio of the full and reduced (intercept-only) model. Pseudo-\(R^2\) measures should not be taken too seriously, but can be useful, for example for comparing goodness of fit between logistic regression and linear regression models. We won't consider these methods further, but they are reported in some papers.
% 
% 
% 
% Nonetheless, we might want an \(R^2\)-like quantity that at least has similar properties, if we find such measures easier to interpret than classification accuracy or an LR test result. A number of \emph{pseudo-\(R^2\)} measures exist, of which the two most common are:
% 
% - Somers $D_{xy}$: rank correlation between predicted probabilities and observed responses (ranges from 0-1)
% 
% (or concordance index $C$, all related to AUC)
% 
% \subsection{Pseudo-$R^2$ measures}


%Formally, if \(n_1\) and \(n_2\) are the number of observations where \(y_i = 0\) and \(y_i = 1\) in the data, the baseline classification accuracy is:
%\begin{equation*}
%  \max \left( \frac{n_1}{n}, \frac{n_2}{n} \right)
%\end{equation*}



% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   Likelihood-ratio test
% \item
%   Classification accuracy
% \item
%   Pseudo-\(R^2\)
% \end{enumerate}
% 
% to compare a logistic regression model to a baseline (intercept-only model). We assume here that the logistic regression model has just one predictor \(X\), and denote this model by \(M_1\) and the baseline model by \(M_0\).
% 
% \hypertarget{c4lrt}{%
% \subsection{Likelihood-ratio test}\label{sec:lrt}}
% 
% This option is analogous to the \(F\) test for linear regression models, where the full model is compared to the intercept-only model, and we test whether the change in sum-of-squares is significant, given the added predictors in the full model.
% 
% Logistic regression models don't have sums-of-squares because they don't have residuals. Instead, we compare the \textbf{likelihood} (Eq. \eqref{eq:likelihood}) of the two models: is the change in likelihood between \(M_0\) and \(M_1\) significant?
% 
% Most methods involving likelihood use log-transformed likelihood, mostly to avoid numerical issues when computing with very small numbers. Instead of working with straight log-likelihood, it is also customary to define the \emph{deviance} of a model \(M\) (\citet{agresti2007} 3.4.3), as
% \begin{equation*}
%   D = -2 \log L(M)
% \end{equation*}\\
% Thus, a better model (higher likelihood) has lower deviance.
% 
% The difference in deviance (\(\Delta D\)) between models with (\(M_1\)) and without (\(M_0\)) the single predictor is related to the log of their \emph{likelihood ratio}:
% \begin{equation*}
%   \Delta D = -2 \log \frac{L(M_0)}{L(M_1)}
% \end{equation*}
% 
% For large enough sample size, \(\Delta D\) follows a \(\chi^2(1)\) distribution if the slope of \(X\) (\(\beta_1\)) is 0. Thus, we can use \(\Delta D\) as a test statistics for the null hypothesis that \(\beta_1 = 0\). This kind of test is called a \emph{likelihood-ratio test}. The \(p\) value of this test gives a measure of goodness of fit, while \(\Delta D\) gives a sort of measure of variance accounted for (if you think of deviance as capturing ``variance'' from the perfect model).
% 
% For our \protect\hyperlink{c4ex1}{Example 1}, an LR test is conducted in R as follows:
% 
% <<>>=
% mod1 <- glm(stressshift ~ acoustics.std, data=givenness, family="binomial")
% mod0 <- glm(stressshift ~ 1, data=givenness, family="binomial")
% ## LR test of the effect of acoustics.std in mod1
% anova(mod0, mod1, test='Chisq')
% @
% 
% Note that for simple linear regression, this is just another way to test the hypothesis that \(\beta_1 = 0\), in addition to the \(z\) test reported in the regression summary.
% 
% <<>>=
% summary(mod1)
% @
% 
% However, for multiple regression models with more than one predictor, the LR test gives a measure of overall model fit, and is not testing the same thing as the \(z\) tests reported by \texttt{summary(model)} for each predictor.

%\hypertarget{classification-accuracy}{%
%\subsection{Classification accuracy}\label{classification-accuracy}}

%Here are functions for computing the classification accuracy and %baseline accuracy of a logistic regression model:
% 
% <<>>=
% ## function for computing accuracy of a logistic regression model 
% ## (on the dataset used to fit the model)
% ## lrMod = fitted model
% ## responseVar = name of response variable for lrMod
% ##
% ## adapted from: https://www.r-bloggers.com/evaluating-logistic-regression-models/
% lrAcc <- function(lrMod, responseVar){
%   ## convert response variable into a factor if it's not one
%   if(!is.factor(model.frame(lrMod)[,responseVar])){
%     model.frame(lrMod)[,responseVar] <- as.factor(model.frame(lrMod)[,responseVar])
%   }
%     ## model predictions in log-odds
%     preds = predict(lrMod, newdata=model.frame(lrMod))
%     ## transform to 0/1 prediction
%     preds <- ((sign(preds)/2)+0.5)
%     
%     ## response variable values, transformed to 0/1
%     y <- (as.numeric(model.frame(lrMod)[,responseVar])-1)
%     
%     ## how often is prediction the same as the actual response
%     acc <- sum(preds==y)/length(preds)
%     
%     return(acc)
% }
% 
% ## baseline accuracy for a logisitic regression model lrMod
% ## with a given response variable
% baselineAcc <- function(lrMod, responseVar){
%     response <- model.frame(lrMod)[,responseVar]
%     tab <- table(response)
%     return(max(tab)/sum(tab))
% }
% 
% ## baseline accuracy
% @

% 
% \hypertarget{logistic-regression-pseudo-r2}{%
% \subsection{\texorpdfstring{Pseudo-\(R^2\)}{Pseudo-R\^{}2}}\label{logistic-regression-pseudo-r2}}
% 
% There is no equivalent to \(R^2\) for logistic regression, meaning a quantity with similar properties and multiple interpretations:
% 
% \begin{itemize}
% \item
%   Fraction reduction in sum-of-squares (there is no sum-of-squares)
% \item
%   Degree of ``variance'' accounted for (``variance'' isn't well-defined)
% \item
%   Squared correlation between fitted and observed values (different scales: probabilities versus 0/1)
% \end{itemize}
% 
% Nonetheless, we might want an \(R^2\)-like quantity that at least has similar properties, if we find such measures easier to interpret than classification accuracy or an LR test result. A number of \emph{pseudo-\(R^2\)} measures exist, of which the two most common are:
% 
% \begin{itemize}
% \item
%   \textbf{Cox-Snell} pseudo-\(R^2\): a value \(\geq 0\)
% \item
%   \textbf{Nagelkerke} pseudo-\(R^2\): a value between 0 and 1 (like \(R^2\) for linear regression)
% \end{itemize}
% 
% Both measures are related to the likelihood ratio of the full and reduced (intercept-only) model. Pseudo-\(R^2\) measures should not be taken too seriously, but can be useful, for example for comparing goodness of fit between logistic regression and linear regression models. We won't consider these methods further, but they are reported in some papers.
% 
% \hypertarget{example-15}{%
% \subsubsection*{Example}\label{example-15}}
% \addcontentsline{toc}{subsubsection}{Example}
% 
% How well does our \protect\hyperlink{c4ex2}{Example 2} model (\texttt{stressshift\ \textasciitilde{}\ npType}) do at predicting whether stress shifts?
% 
% <<>>=
% mod2 <- glm(stressshift ~ npType, data=givenness, family="binomial")
% @
% 
% By the likelihood-ratio test:
% 
% <<>>=
% ## LR test of the effect of npType in mod2
% anova(mod2,mod0, test='Chisq')
% @
% 
% there is a (barely) significant effect of \texttt{npType}, corresponding to a difference in deviance of \(\Delta D = 4.1\).
% 
% However, classification accuracy:
% 
% <<>>=
% ## accuracy of mod2
% lrAcc(mod2, 'stressshift')
% 
% ## it's the same as the baseline's accuracy
% baselineAcc(mod1, 'stressshift')
% @
% 
% does not differ between the baseline and full models. This example illustrates a couple of points:
% 
% \begin{itemize}
% \tightlist
% \item
%   Different methods for comparing two models won't necessarily give the same qualitative answers---as we \protect\hyperlink{non-nested-model-comparison}{already saw} with AIC versus BIC-based model selection.
% \end{itemize}
% 
% \begin{itemize}
% \tightlist
% \item
%   Classification accuracy is a blunter tool than an LR test---it does not reflect effects that are small compared to baseline accuracy, because in order to affect classification accuracy an effect has to be big enough to change the sign of the predicted log odds.
% \end{itemize}


\hypertarget{multiple-logistic-regression}{%
\section{Multiple logistic regression}\label{multiple-logistic-regression}}

Generalizing from one predictor to multiple predictors is similar for logistic regression as for linear regression (Section~\ref{sec:multiple-linear-regression}).  We assume there are $k$ predictors (\(x_1, ..., x_k\)) of a binary response \(y\), where each \(x_i\) can be continuous or categorical. The logistic regression model is now:
\begin{equation}
  \text{logit}(P(y = 1)) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_k
  \label{eq:mult-log-reg-1}
\end{equation}

Written in terms of individual observations, the log-odds of $y=1$ for the $i$th observation would be (using the notation $x_{ij}$ as in Section~\ref{sec:mlr-ex-1}):
\begin{equation*}
  \text{logit}(P(y_i = 1)) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ik}
\end{equation*}

We can rewrite the model in equation \eqref{eq:mult-log-reg-1} in terms of odds by exponentiating each side:

\begin{align}
\text{odds}(y=1) & =   \frac{P(y = 1)}{P(y = 0)}  \\
& = e^{\beta_0}e^{\beta_1 x_1}\cdots e^{\beta_p x_p}
\end{align}

If we redefine the coefficients as  $\alpha_i = e^{\beta_i}$, which we do to interpret them in terms of odds, this becomes:
\begin{equation*}
\text{odds}(y=1) = \alpha_0 \alpha_1^{x_1}\cdots \alpha_p^{x_p}
\end{equation*}

That is, each predictor's effect on the odds is multiplicative. A predictor that has no effect corresponds to multiplying by 1 (\(\alpha_i = 1\)), while predictors with `negative' effects decrease the odds (\(\alpha_i < 1\)) and predictors with `positive' effects increase the odds (\(\alpha_i > 1\)).  

For example, suppose the predictor $x_1$ is a two level factor which can take on values 0 and 1, and $\alpha_1 = 2$.  When $x_1 = 1$, the odds are multiplied by 2 ($\alpha_1^{x_1} = 2^1$) relative to when $x_0 = 0$ ($\alpha_1^{x_1} = 2^0$).  If $x_1$ were a continuous predictor, then every increase of 1 in $x_1$ would correspond to multiplying the odds by 2.


\subsection{Likelihood-ratio test: General case}
\label{sec:log-reg-lik-ratio}

The Wald test and likelihood approaches discussed above (Section~\ref{sec:log-reg-uncertainty}) generalize to the multiple regression case, for calculating standard errors, confidence intervals, and $p$-values for  coefficients of a multiple logistic regression.  The likelihood is now:

\begin{align}
  L(\beta_0, \beta_1, \ldots, \beta_k ) = \prod^n_{i = 1} p_i^{y_i} (1-p_i)^{1-y_i}
      \label{eq:likelihood-mult} 
\end{align}
where $p_i$ is defined in equation \eqref{eq:mult-log-reg-1}.


The likelihood-ratio test also generalizes---the difference in deviance $\Delta D$ between models $M_0$ (subset) and $M_1$ (superset)  is still defined as equation \eqref{eq:lr-definition}, but now the models have $k$ and $q$ predictors:
\begin{align}
M_0 & : \text{predictors } x_1, ..., x_k  \\
M_1 & : \text{predictors } x_1, ..., x_k,  x_{k + 1}, ..., x_q
\end{align}

\(\Delta D\) approximately follows a \(\chi^2(q - k)\) distribution, under the null hypothesis that the additional terms in $M_1$ have no effect ($\beta_{k + 1} = \beta_{k + 2} = \cdots = \beta_q = 0$: \citealp[][\S3.4.3]{agresti2007}).
%(Agresti 3.4.3). 
Thus, we can use a likelihood-ratio test to test whether adding these predictors significantly improves model likelihood.  Examples are shown below for two common applications: assessing whether a model improves at all on the baseline, and whether adding additional terms is justified.
% Thus, we can use \(\Delta D\) as a test statistic for the null hypothesis that all the added predictors have no effect:
% \[
% H_0: \beta_{p + 1} = \beta_{p + 2} = \cdots = \beta_q = 0
% \]

\paragraph{Effect size}
%\label{sec:effect-size-logistic-comparison}

An effect size should be reported with any hypothesis test, but it is unclear 
%to me 
if there is a standard effect size to report for this kind of likelihood-ratio test---which is very common in practice.  As for the linear regression case (Section~\ref{sec:cohens-f}), we will just use one measure which is intuitive rather than discussing different options.

Let $odds_0$ and $odds_1$ be the accuracy of the two models (defined as above, with threshold 0.5), transformed to odds. The \emph{odds ratio} (OR), $odds_1/odds_0$ can then be used as an effect size.  What counts as small/medium/large odds ratios is unclear, since this depends on the base rate \citep{chen2010big}, but a rough rule of thumb is that OR$<$1.5 is small and OR$>4$ is large.
% FUTURE: revisit, I think I just decided that because they work across base rates.

There is not an R function for this exact case, but we can write one using the {performance} functionality used above to compute model accuracy:

<<echo=1:3>>=
# m1: reduced model
# m2: full model
oddsratio_lr <- function(m1, m2) {
  p2 <- performance_pcp(m2)$pcp_model
  p1 <- performance_pcp(m1)$pcp_model
}
## FUTURE: revisit -- I made this up
@

\subsection{Example}

In this example we model whether stress shifts for the \ttt{diatones} data as a function of 
%of the predictors for the \ttt{diatones} data: 
word \ttt{frequency} and syllable structure (captured by 
\ttt{syll1\_coda}, \ttt{syll2\_coda}, and \ttt{syll2\_td}). We first fit a simple model with only main effect terms (no interactions),  as an example of multiple logistic regression, interpreting coefficients, and goodness of fit.  We then consider adding interaction terms, which leads to the first model from \citet{sonderegger2010tfs}.
%for the \ttt{diatones} data.
%\footnote{The model there is `penalized' to avoid overfitting; we report models without penalization.}

\subsubsection{Model 1}
\label{sec:mlogreg-mod-1}



We first standardize all predictors, for interpretability (Section~\ref{sec:centering-scaling}):

<<>>=
diatones <- mutate(diatones,
  syll1_coda = rescale(syll1_coda_orig),
  syll2_td = rescale(syll2_td_orig),
  syll2_coda = rescale(syll2_coda),
  frequency = rescale(frequency)
)
@

%For practice, let's go through what this means for the interpretation of the regression coefficients.
Since the first two predictors are two-level factors, they have been transformed to 0 and 1 then centered; the second two predictors are numeric, so they have been centered and divided by two standard deviations. The coefficients will mean:
\begin{itemize}
\item \ttt{syll1\_coda}: predicted change (in log-odds of stress shifting) between when a first-syllable coda is/isn't present (similar for \ttt{syll2\_td}).
\item \ttt{syll2\_coda}: predicted change when the number of second-syllable coda consonants is increased by 2 standard deviations (similar for \ttt{frequency}). In this case, an increase of 2 SD corresponds to an increase of  \Sexpr{2*sd(diatones_old$syll2_coda)} consonants, so the coefficient is a bit more than the predicted change in log-odds for each added consonant.
\end{itemize}



To fit the model and view its coefficient table:

<<>>=
mlogreg_mod_1 <- glm(stress_shifted ~ syll2_coda + syll2_td + 
    frequency + syll1_coda, data = diatones, family = "binomial")
mlogreg_mod_1 %>% tidy()
@

The effects capture the empirical trends shown in Figure~\ref{fig:diatones-empirical}.  The main patterns are that words with a  first syllable coda or \textbf{without} a second syllable coda seem more likely to shift stress. In addition higher frequency words may be slightly less likely to shift stress. 

Because the predictors are standardized, we can interpret their values as relative effect sizes:  the \texttt{syll2\_coda} and \texttt{syll1\_coda} effects are stronger than the others, and  \ttt{frequency} has a smaller effect than the other predictors (which index syllable structure).

Let's interpret a couple coefficient estimates from the model, in terms of (log-odds) and probability.
%(Coefficient significances are interpreted similarly to linear regression.)

The intercept is  $\hat{\beta}_0 = \Sexpr{mlogreg_mod_1$coefficients[[1]]}$, which corresponds to probability of
\Sexpr{invlogit(mlogreg_mod_1$coefficients[[1]])} (inverse-logit of  \Sexpr{mlogreg_mod_1$coefficients[[1]]}).  
Thus the predicted probability of stress shifting is
\Sexpr{invlogit(mlogreg_mod_1$coefficients[[1]])*100}\%,
for an `average word' (all predictors held at mean values).

<<echo=FALSE>>=
mlogreg_mod_1_c1 <- mlogreg_mod_1$coefficients[["syll1_coda"]]
mlogreg_mod_1_c2 <- mlogreg_mod_1$coefficients[["frequency"]]
@


The \ttt{syll1\_coda} coefficient is 
\Sexpr{mlogreg_mod_1_c1}
meaning the odds of stress shifting are predicted to be 
\Sexpr{exp(mlogreg_mod_1_c1)}
times higher for words where the first syllable has a coda. 

The \ttt{frequency} coefficient
%which is of theoretical interest,
is smaller: 
%the coefficient is
\Sexpr{mlogreg_mod_1_c2},  meaning the odds of stress shifting are predicted to be about 50\% lower for words with frequency 2 standard deviations higher ($e^{\Sexpr{mlogreg_mod_1_c2}} = \Sexpr{exp(mlogreg_mod_1_c2)}$).

Each term $\beta_i$ of a multiple logistic regression can also be interpreted in terms of probability,  similarly to simple logistic regression (Section~\ref{sec:log-reg-ex-1}): either as the slope with all predictors held at 0,
%($\beta_i \cdot p_0 (1- p_0)$), 
or at the maximum possible slope (when probability=0.5),
%: $\beta_i / 4$), 
or as the average marginal effect.
%(using \ttt{margins}).  
Exercise~\ref{ex:mlogreg-prob} asks you to do this for Model 1.
 





% 
% Interpretation: the odds of stress shifting are multiplied by \textbf{24.0} (\(e^{3.18}\)) in Williams condition (with other predictors at mean values). This is a huge effect!
%   \item
%     Corresponds to a maximum change of 0.80 in probability (divide-by-4-rule: 3.18/4=0.80).
% 
% \item 
% \end{itemize}
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   Intercept
% 
%   \begin{itemize}
%   \item
%     \(\beta_0 = -1.01\), corresponds to probability \(0.267\)
%   \item
%     Thus, 27\% stress shift is predicted overall (all predictors held at mean values)
%   \end{itemize}
% \item
%   Slope for \texttt{conditionLabel.williams}
% 
%   \begin{itemize}
%   \item
%     Coefficient = 3.18
%   \item
%     Interpretation: the odds of stress shifting are multiplied by \textbf{24.0} (\(e^{3.18}\)) in Williams condition (with other predictors at mean values). This is a huge effect!
%   \item
%     Corresponds to a maximum change of 0.80 in probability (divide-by-4-rule: 3.18/4=0.80).
%   \end{itemize}
% \end{enumerate}
% 


% 
% \hypertarget{log-reg-worked-example}{%
% \subsection{Worked example}\label{log-reg-worked-example}}
% 
% In this example, we model the probability of shifting stress (\texttt{stressshift}) for the \texttt{givenness} data, as a function of four predictors---all standardized (see code chunk at the beginning of this chapter):
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   Condition: \texttt{conditionLabel.williams}
% \item
%   NP Type: \texttt{npType.pronoun}
% \item
%   Voicing: \texttt{voice.passive}
% \item
%   Trial number: \texttt{order}
% \end{enumerate}
% 
% We will carry out a full analysis (except model criticism):
% 
% \begin{itemize}
% \item
%   Exploratory data analysis, motivating which terms to include in a model
% \item
%   Fit a first model
% \item
%   Variable selection: adding and dropping terms
% \item
%   Model evaluation
% \end{itemize}
% 
% \hypertarget{exploratory-data-analysis}{%
% \subsubsection*{Exploratory data analysis}\label{exploratory-data-analysis}}
% \addcontentsline{toc}{subsubsection}{Exploratory data analysis}
% 
% In this empirical plot of how \texttt{order} affects the probability of stress shifting, our actual observations appear as points at \(y=0\) or \(y=1\), while the prediction from a simple logistic regression (on just \texttt{order}) appears as the fitted blue line:
% 
% <<fig.align='center', fig.height=4, fig.width=6>>=
% ggplot(aes(x=order, y=stressshift.num), data=givenness) + 
%   geom_smooth(method="glm", method.args = list(family = "binomial"))  +
%   geom_jitter(position=position_jitter(width=0.2,height=0.03),size=1) + 
%   ylab("Proportion shifted") + 
%   ggtitle("Order vs. proportion with shifted prominence") +
%   theme(plot.title = element_text(hjust = 0.5))
% @
% 
% Proportion tables showing how the probability of stress shifting depends on each categorical predictor (plots would be better):
% 
% \begin{itemize}
% \tightlist
% \item
%   \texttt{conditionLabel}:
% \end{itemize}
% 
% <<>>=
% prop.table(xtabs(~conditionLabel+stressshift, data=givenness), margin=1)
% @
% 
% \begin{itemize}
% \tightlist
% \item
%   \texttt{npType}:
% \end{itemize}
% 
% <<>>=
% prop.table(xtabs(~npType+stressshift, data=givenness), margin=1)
% @
% 
% \begin{itemize}
% \tightlist
% \item
%   \texttt{voice}:
% \end{itemize}
% 
% <<>>=
% prop.table(xtabs(~voice+stressshift, data=givenness), margin=1)
% @
% 
% The expected directions of effects of the predictors on percent stress-shifted are:
% 
% \begin{itemize}
% \item
%   \texttt{order}: positive
% \item
%   \texttt{conditionLabel.williams}, \texttt{npType.pronoun}, \texttt{voice.passive}: positive
% \end{itemize}
% 
% \hypertarget{model}{%
% \subsubsection*{Model}\label{model}}
% \addcontentsline{toc}{subsubsection}{Model}
% 
% We fit a first multiple logistic regression model using just main effects of the four predictors:
% 
% <<>>=
% mod3 <- glm(stressshift ~ npType.pron + clabel.williams + voice.passive + order.std, 
%             family = "binomial", 
%             data = givenness)
% summary(mod3)
% @

% Note that because the predictors are standardized, we can interpret their values as relative effect sizes: the \texttt{conditionLabel} effect is much stronger than the others, then \texttt{voice} \textgreater{} \texttt{npType} \textgreater{} \texttt{order}. The direction of all effects is positive, as predicted.

% 
% \textbf{Exercise}:
% 
% What are the predicted log-odds of stress shifting when:
% 
% \begin{itemize}
% \item
%   \texttt{order} = mean value
% \item
%   \texttt{npType} = pronoun
% \item
%   \texttt{conditionLabel} = Williams
% \item
%   \texttt{voice} = active
% \end{itemize}
% 
% Hint: assume that the three categorical variables just take on values -0.5 and 0.5.
% 
% <<echo=FALSE>>=
% coeffs <- coefficients(mod3)
% 
% coeffs[['(Intercept)']] + 0.5*coeffs[['npType.pron']] + 0.5*coeffs[['clabel.williams']] -0.5*0.5*coeffs[['voice.passive']]
% @

We can evaluate the model using a likelihood-ratio test and AUC, 
%Somers' $D_{xy}$, 
discussed above.    To test how much likelihood improves from a null model:

<<output.lines=5:7>>=
mlogreg_mod_1_baseline <- update(mlogreg_mod_1, . ~ 1)
anova(mlogreg_mod_1, mlogreg_mod_1_baseline, test = "Chisq")
@

<<echo=FALSE>>=
mlogreg_mod_1_aov <- anova(mlogreg_mod_1, mlogreg_mod_1_baseline, 
  test = "Chisq")
@

Note that the degrees of freedom of this comparison is \(k-q = 4\), since there are four predictors in the full model. 
%The difference in deviance is \(\Delta D = \Sexpr{abs(mlogreg_mod_1_aov$Deviance[2])}\). This is fairly small, reflected  
The four terms significantly
contribute 
%a (barely) significant contribution
to overall model likelihood ($\alpha=0.05$), but the $p$-value is high.
%as reflected in a $p$-value of \Sexpr{mlogreg_mod_1_aov$`Pr(>Chi)`[2]}, near the $\alpha = 0.05$ cutoff.    
Intuitively the null model already predicts quite well (since $\sim$80\% of words don't shift stress), so there is a high bar for improving it.  Indeed, the effect size (as an odds ratio) of adding the four terms is small (OR$<$1.5):
%(odds ratio = 1):

<<>>=
oddsratio_lr(mlogreg_mod_1_baseline, mlogreg_mod_1)
@


The AUC value is \Sexpr{auc(mlogreg_mod_1)} (\ttt{auc(mlogreg\_mod\_1)}), 
suggesting the model  has some explanatory power to predict whether stress shifts (baseline would be AUC=0.5).
% 
% Somers $D_{xy}$ is \Sexpr{mlogreg_mod_1_dxy}, suggesting the model does have some explanatory power (baseline would be $D_{xy} = 0$).
% <<eval=FALSE>>=
% somers2(mlogreg_mod_1$fitted.values, mlogreg_mod_1$y)
% @

%when we use a metric which takes into account...


% 
% 
% <<>>=
% ## classification accuracy
% lrAcc(mod3, 'stressshift')
% @
% 
% Compared to baseline accuracy:
% 
% <<>>=
% ## baseline accuracy
% baselineAcc(mod3, 'stressshift')
% @
% 
% Thus, the four predictors improve classification accuracy by 15\%.
% % 
% % \hypertarget{variable-selection}{%
% \subsubsection*{Variable selection}\label{variable-selection}}
% \addcontentsline{toc}{subsubsection}{Variable selection}
% 
% In this exercise we will follow the variable selection guidelines from Gelman \& Hill discussed \protect\hyperlink{c2varselect}{in the previous chapter}.
% 
% First two steps:
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   Include main effects for predictors expected to affect the response (done)
% \item
%   Consider interactions between terms with large effects (we do this now)
% \end{enumerate}
% 
% These empirical plots, for each pair of variables with the largest effects, tell us which interactions are most suggestive:
% 
% <<fig.align='center', fig.height=4.5, fig.width=8>>=
% day14plt1 <- ggplot(aes(x=conditionLabel, y=stressshift.num), data = givenness) + 
%   stat_summary(fun.data="mean_cl_boot", geom="errorbar", width=0.25, aes(color=voice)) +
%   facet_wrap(~voice) + 
%   ylab("% shifted stress") + 
%   ylim(0,1)
% 
% day14plt2 <- ggplot(aes(x=voice, y=stressshift.num), data = givenness) + 
%   stat_summary(fun.data="mean_cl_boot", geom="errorbar", width=0.25, aes(color=npType)) +
%   facet_wrap(~npType) + 
%   ylab("% shifted stress") + 
%   ylim(0,1)
% 
% day14plt3 <- ggplot(aes(x=conditionLabel, y=stressshift.num), data = givenness) + 
%   stat_summary(fun.data="mean_cl_boot", geom="errorbar", width=0.25, aes(color=npType)) +
%   facet_wrap(~npType) + 
%   ylab("% shifted stress") + 
%   ylim(0,1)
% 
% grid.arrange(day14plt1, day14plt2, day14plt3, ncol = 2)
% @
% 
% Let's try adding the \texttt{voice:conditionLabel} interaction, which looks the largest:
% 
% <<>>=
% mod3 <- glm(stressshift ~ npType.pron + clabel.williams + voice.passive + order.std, 
%             family = "binomial", 
%             data = givenness)
% mod3.1 <- glm(stressshift ~ npType.pron + clabel.williams * voice.passive + order.std,
%               data = givenness, 
%               family = "binomial")
% anova(mod3, mod3.1, test = 'Chisq')
% @
% 
% This model comparison suggests we should include the \texttt{voice:conditionLabel} interaction in the model.
% 
% Why, pedantically:
% 
% \begin{itemize}
% \item
%   \(\Delta D = 10.08\)
% \item
%   Probability of \(\Delta D\) at least this larger under \(\chi^2(1)\) distribution is 0.0015
% \item
%   \(\Rightarrow\) \textbf{reject} \(H_0\) that \(\beta_{\texttt{voice:conditionLabel}} = 0\)
% \item
%   \(\Rightarrow\) \textbf{include} \texttt{voice:conditionLabel} interaction in the model.
% \end{itemize}
% 
% We can carry out similar tests for the other two possible two-way interactions, and get:
% 
% <<>>=
% mod3.2 <- glm(stressshift ~ npType.pron + clabel.williams + voice.passive + order.std + npType.pron * clabel.williams, 
%               data = givenness, 
%               family = "binomial")
% anova(mod3, mod3.2, test="Chisq")
% @
% 
% <<>>=
% mod3.3 <- glm(stressshift ~ npType.pron + clabel.williams+voice.passive + order.std + voice.passive * npType.pron, 
%               data = givenness, 
%               family = "binomial")
% anova(mod3, mod3.3, test="Chisq")    
% @
% 
% Thus, \(p>0.3\) for the \texttt{npType:conditionLabel} and \texttt{voice:npType} interactions, so we don't add these interactions to the model.
% 
% The new model is:
% 
% <<>>=
% summary(mod3.1)
% @
% 
% Examining the coefficient values, and comparing to our prior expectations---from exploratory plots and from the study design---there are no coefficients to consider removing by Gelman \& Hill's guidelines. (Make sure you understand the different reasons we don't drop the \texttt{order.std} or \texttt{voice.passive} main effects.)
% 
% The classification accuracy for this new model is:
% 
% <<>>=
% lrAcc(mod3.1, 'stressshift')
% @
% 
% Recall that without the \texttt{voice:conditionLabel} interaction, we had the classification accuracy:
% 
% <<>>=
% lrAcc(mod3, 'stressshift')
% @
% 
% So accuracy actually \textbf{decreases} slightly when this interaction is added. This can happen, because improving model likelihood and improving classification accuracy are not the same thing.\footnote{It's worth noting that classification accuracy is very similar for the two models---close enough to be effectively the same, taking finite sample size into account. For a proportion estimated at \(\hat{p} = 0.80\) (like classification accuracy, here) from \(n=382\) observations, standard error is \(\sqrt{p(1-p)/n} = 0.02\), so the 95\% CIs of classification accuracy of the two models overlap.}


\subsubsection{Model 2}
\label{sec:diatones-mod-2}

% FUTURE: is it actually clear from description of data at chapter beginning that these are of interest?
We now add all the frequency-structure interaction terms, as these are of interest for the research questions for the \ttt{diatones} study.  To fit and summarize the model:

<<>>=
mlogreg_mod_2 <- update(mlogreg_mod_1, . ~ . +
    (syll2_td + syll1_coda + syll2_coda):frequency)
tidy(mlogreg_mod_2)
@

We can verify using likelihood-ratio tests that the \ttt{syll2\_coda:frequency} term does not significantly contribute, while other terms do. For example (output not shown):

<<results=FALSE >>=
mlogreg_mod_3 <- update(mlogreg_mod_2, . ~ . - syll2_coda:frequency)
## Test the syll2_coda:frequency interaction
anova(mlogreg_mod_2, mlogreg_mod_3, test = "Chisq")
@

Following the Gelman \& Hill model selection guidelines (Section~\ref{sec:gh-model-selection}): since we have no expectations about the directions of the interaction effects, we will keep those which significantly contribute to model likelihood and drop those which do not---the updated model is \ttt{mlogreg\_mod\_3}.  The coefficients for this final model are:



% 
% Confirm that the two interaction terms significantly contribute to model likelihood via LR test:
% <<>>=
% anova(mlogreg_mod_1, mlogreg_mod_3, test='Chisq')
% @

%Thus, the coefficients for the final model are:
<<>>=
tidy(mlogreg_mod_3)
@

We call this Model 2.  The predicted effects of `structure' predictors remain the same as in Model 1: stress shift is predicted to be more likely for words with a first-syllable coda, or a second syllable ending in /t/ or /d/ (averaging over \ttt{frequency}), or without a second-syllable coda. 

Otherwise, interpreting the model's results just from the coefficient table is unintuitive.  To go further---in particular interpreting the frequency effect of theoretical interest---we must consider the interaction terms. This will serve as an example of how to visualize effects for logistic regression.
%more generally.

\subsection{Visualizing effects for logistic regression}
\label{sec:viz-effects-logistic}


%Interpretation of frequency effects -- example of plotting interaction terms and visualizing effects for logistic regression.

Model prediction plots are especially useful for interpreting the results of logistic regression models containing multiple predictors, because in \textbf{probability} space, the effect of each predictor depends on the value others are held at.  In addition, prediction plots are always helpful for understanding models containing interaction terms. We exemplify using the \ttt{mlogreg\_mod\_3} model.

It is useful when making prediction plots to refit with all two-level factors coded as factors, so that plotting functions treat these variables as categorical rather than continuous:
%---these variables can only take on two values.

<<>>=
mlogreg_mod_3_fact <- update(mlogreg_mod_3, . ~ syll2_coda +
  (syll2_td_orig + syll1_coda_orig) * frequency)
@


We have  previously used {sjPlot} to plot predictions of linear regression models (\secref{sec:model-predictions-intro}; Figure~\ref{fig:simple_logreg_ex}). Functions in this package like \ttt{plot\_model()} (1) compute model predictions using functions from the \ttt{ggeffects} package, then (2) plot them.  (1) is the  hard part, since various choices must be made, such as whether/how to `marginalize' over predictors not plotted.

For (2), you can either work with automatically generated plots, using 
%You can always see the automatically-generated plots using
\ttt{plot\_model()} or a similar function (e.g., \ttt{allEffects()} in the {effects} package), or work directly with the model predictions to make customized plots.


For the \ttt{mlogreg\_mod\_3} model, we first visualize the main effect of each predictor %the simple way---just 
using \ttt{plot\_model}.
%, and accept the choices it makes.  
For example, the code for the \ttt{syll1\_coda} partial-effect plot is:

<<mlogreg-eff-1,  echo=1, out.width='40%', fig.width=default_fig.width*.4/default_out.width, fig.cap='Partial effect of each predictor in model \\ttt{mlogreg\\_mod\\_3}, marginalizing over other predictors (at mean values).'>>=
plot_model(mlogreg_mod_3_fact, "eff", terms = "syll1_coda_orig", title = "") +
  ylab("Pred. probability") + xlab("Syll 1 coda")
plot_model(terms = "syll2_coda", type = "eff", title = "", mlogreg_mod_3_fact) + ylab("Pred. probability") + xlab("Syll 2 coda")
plot_model(terms = "syll2_td_orig", type = "eff", title = "", mlogreg_mod_3_fact) + ylab("Pred. probability") + xlab("Syll 2 = t/d")
plot_model(terms = "frequency", type = "eff", title = "", mlogreg_mod_3_fact) + xlab("Frequency") + ylab("Pred. probability")
@

Figure~\ref{fig:mlogreg-eff-1} shows the `marginal effect'  of each predictor: other predictors  are held at an `average' value, which is the mean for continuous predictors and the weighted average (using the proportion of observations at each level) for factors.  This is the `marginal effect at the mean' from Box~\ref{box:marginal-effects}.

\begin{boxedtext}{Broader context: Marginal effects}
\label{box:marginal-effects}

\emph{Marginal effects} are a very useful tool for interpreting regression results: they quantify the effect of varying one or more predictors, as others are held constant or averaged over in some way---which is often of substantive interest.  Marginal effects are widely used in other fields  (especially social sciences: \citealp[e.g.,][]{long2006regression}),
% FUTURE: more-refs
and deserve to be more widely used in linguistics. It is useful to define some terminology
%We will discuss marginal effects and R functionality for computing them (the \ttt{ggeffects}, \ttt{margins} packages) further for logistic regression (Section~\ref{sec:viz-effects-logistic}, 
%\ref{sec:average-marginal-effects}), where they are particularly useful, but it's useful
%to define some terminology
to connect with existing resources, especially 
%(very good) 
existing R packages such as emmeans, {ggeffects}, and {margins}.
%% FUTURE: fill in above -- more refs after Long, check if marginal effects actually used in linguistics..
%Marginal effects  deserve to be more widely used in linguistics; we will come back to them often for interpreting models. 

Suppose we are just referring to one predictor $x$, and an outcome $y$. Consider the `slope' of $x$, when other predictors are held constant at some values---how much $y$ changes per unit change in $x$ (formally, the partial derivative of $y$ with respect to $x$). There are three intuitive quantities we could calculate. The \emph{average marginal effect} (AME) of $x$ is the `average effect'---the slope of $x$ averaged over every observation in the dataset. The \emph{marginal effect at the mean}  is the slope of $x$ for an `average observation'---the slope at the mean values of all covariates. The coefficients of main effects when predictors are standardized
%, described in the text, 
can be thought of as either MEMs (``all other predictors held at 0'') or AMEs. Finally,  the \emph{marginal effect at representative values} are the slopes of $x$ at theoretically-interesting values of other predictors. 
%% FUTURE: for example...

%% FUTURE: incorporate Vincent package instead, after trying it out.  margins no longer maintained.
The vignettes for the {margins} package (especially \citealp[][\S1]{leeper2018interpreting}) are a good place to read more.

\end{boxedtext}







To visualize the \ttt{frequency}:\ttt{syll1\_coda} and \ttt{frequency}:\ttt{syll2\_td} interactions, we will use \ttt{ggeffects()} directly to see how it works.  

<<>>=
int1_eff <- ggeffect(mlogreg_mod_3_fact, 
  terms = c("frequency", "syll1_coda_orig")
)
int2_eff <- ggeffect(mlogreg_mod_3_fact, 
  terms = c("frequency", "syll2_td_orig")
)
@

This function computes dataframes consisting of predicted probabilities, with standard errors and 95\% confidence intervals, as the predictions to be plotted are varied, with others marginalized over.

For example, part of one dataframe is:

<<>>=
head(int2_eff)
@

Dataframes produced by \ttt{ggeffects()} are automatically plotted using \ttt{ggplot()} (e.g., \ttt{plot(int2\_eff)}), which allows us to change the appearance of the plots just like any other ggplot.  Code to make Figure~\ref{fig:mlogreg-eff-2}, which plots the two interactions, is in the code file.


<<mlogreg-eff-2, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Interaction plots for each interaction in model \\ttt{mlogreg\\_mod\\_3}, marginalizing over other predictors (at mean values).', echo=1>>=
int1_eff %>% 
  plot(use.theme=F, colors = 'bw') + 
  ylab("Predicted probability") +
  ggtitle("") +
  scale_linetype_discrete(name = "Syll 1 coda") + 
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  theme(legend.position=c(.65,.8))
int2_eff %>% 
  plot(use.theme=F, colors = 'bw') + 
  ylab("Predicted probability") +
  scale_linetype_discrete(name = "Syll 2 = t/d") + 
  ggtitle("") + 
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  theme(legend.position=c(.65,.8))
@

These plots help us interpret the interactions: for words without a first-syllable coda or where the second syllable does not end in t/d, stress shift is predicted to be more likely for lower-frequency words.  Otherwise, this frequency effect is weaker, and may even change direction (stress shift more likely for higher frequency words).   

Together with the main effects visualized in Figure~\ref{fig:mlogreg-eff-1}, the overall interpretation is: stress shift is more likely for words with certain syllable structures (first syllable coda, lack of second syllable coda, second-syllable t/d coda)---in line with the empirical trends (Figure~\ref{fig:diatones-empirical}). There is no `overall' effect of frequency. Instead, the frequency effect depends on the word's syllable structure. Essentially, there is only a clear (and negative) frequency effect for words whose syllable structure does \textbf{not} promote stress shift.

The modulation of the frequency effect by syllable structure, as well as the effect of each `structure' predictor, is statistically significant, but with large confidence intervals, as reflected in the large CIs in the prediction plots.

\begin{boxedtext}{Practical note: Probability versus log-odds for interpreting predictions}

Interpreting logistic regressions in probability versus log-odds have  pros and cons (Box~\ref{box:log-lin-reg-diffs}),  which carry over to plotting predictions. We can use the prediction dataframes constructed above to instead plot predictions in log-odds, by just logit-transforming the predictions and CIs  before plotting; Figure~\ref{fig:mlogreg-eff-3} shows some of these plots. 

Some observations about the model are clearer from the log-odds plots. %(Box~\ref{box:prob-logit-plots}). 
It is clearer from these  plots how to interpret the interaction terms: each interaction only lets us be confident that a particular syllable structure modulates the frequency effect, not what the frequency effect is for any particular syllable structure.  Examining the model prediction plots, it looks like the predicted frequency effect is negative for some syllable structures (the CIs for the frequency effect when \ttt{syll1\_coda}=\tsc{no} or \ttt{syll2\_td}=\tsc{no} can't contain a horizontal line), but it may not be positive for some syllable structures.  We will see in Section~\ref{sec:post-hoc-trends} how to make this observation more precise. 

% FUTURE: JPK notes this example isn't great for shoing that log-odds plots are better, because they look pretty similar to the probability plots.
More generally, both  predicted probability and predicted log-odds plots are useful for understanding logistic regression results.
%even if you only have room to report one in your writeup. 
It is tempting to always just examine probabilities, as they are more intuitive, but often the model's interpretation is clearer from log-odds plots.
%couple observations about this model are clearer from the log-odds plots, and point to larger points.

\end{boxedtext}



<<mlogreg-eff-3, echo=FALSE, fig.asp = 1, fig.cap='Partial-effect and interaction plots  for model \\ttt{mlogreg\\_mod\\_3}, showing predictions in log-odds marginalizing over other predictors, corresponding to Figure~\\ref{fig:mlogreg-eff-1}  (top-right) and Figure~\\ref{fig:mlogreg-eff-2}.'>>=
p1<- ggeffect(mlogreg_mod_3_fact, terms = c("syll2_coda")) %>%
  mutate(
    predicted = logit(predicted),
    conf.low = logit(conf.low),
    conf.high = logit(conf.high)
  ) %>%
  ggplot(aes(x = x, y = predicted)) +
  geom_line(size=0.75) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.1) +
  xlab("Syll 2 coda") +
  ylab("Predicted log-odds")

p2<- int1_eff %>%
  mutate(
    predicted = logit(predicted),
    conf.low = logit(conf.low),
    conf.high = logit(conf.high),
    syll1_coda = group
  ) %>%
  ggplot(aes(x = x, y = predicted, lty = syll1_coda)) +
  geom_line(size=0.75) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.1) +
  xlab("Frequency") +
  ylab("Predicted log-odds") +
  scale_linetype_discrete(name = "Syll 1 coda") +
  theme(legend.position="bottom") 

p3<-int2_eff %>%
  mutate(
    predicted = logit(predicted),
    conf.low = logit(conf.low),
    conf.high = logit(conf.high),
    syll2_td = group
  ) %>%
  ggplot(aes(x = x, y = predicted, lty = syll2_td)) +
  geom_line(size=0.75) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.1) +
  xlab("Frequency") +
  ylab("Predicted log-odds") +
  scale_linetype_discrete(name = "Syll 2 = t/d") +
  theme(legend.position="bottom") 
p1 / (p2 + p3) + plot_layout(heights=c(2,3))
@
% 
% \begin{boxedtext}{Practical note: interpreting logistic regression in probability versus log-odds}
% \label{box:prob-logit-plots}
% 
% Both predicted probability and predicted log-odds plots are useful for understanding logistic regression results.
% %even if you only have room to report one in your writeup. 
% It is tempting to always just examine probabilites, as they are more intuitive, but a couple observations about this model are clearer from the log-odds plots, and point to larger points.
% 
% First, the \ttt{syll2\_coda} effect is most certain (narrowest CI) around 0---which corresponds to a C coda---and less certain for higher values, which are less common in the data. This is not clear in probability space because log-odds are so low for higher values. So we should not conclude anything about the effect of a more complex coda (CC or CCC, versus C); a safer interpretation is `having a coda makes stress shift less likely'. The more general point  is that plots of predicted probabilities are more intuitive but can be misleading about what the model actually predicts.
% 
% Second, it is clearer from the log-odds plots how to interpret the interaction terms: each interaction only lets us be confident that a particular syllable structure modulates the frequency effect, not what the frequency effect is for any particular syllable structure.  Examining the model prediction plots, it looks like the predicted frequency effect is negative for some syllable structures (the CIs for the frequency effect when \ttt{syll1\_coda}=\tsc{no} or \ttt{syll2\_td}=\tsc{no} can't contain a horizontal line), but not that it is positive for some syllable structures.  We will see in the next chapter how to make this observation more precise. 
% 
% The broader point is that the presence of a significant interaction with $x_1$ which `flips' an effect of interest $x_2$ does not actually imply that the predicted effect of $x_2$ changes sign.  This point applies for any regression model, but for logistic regression is harder to appreciate in probability space. 
% It sounds 
% %This point sounds 
% obscure, but comes up frequently in interpreting regression results and is often incorrectly reported in publications, affecting the qualitative conclusions.   An incorrect reporting of the current result would be ``when there is a first syllable coda or the second syllable ends with t/d, stress shift is more likely of lower-frequency words; otherwise, stress shift \textbf{is more likely for higher-frequency words}.''  Given that the direction of any frequency effect is of primary interest, the existence of a \textbf{positive} frequency effect would be a qualitatively different conclusion from the actual result.
% 
% DONE: moved these points to next chapter 
% \end{boxedtext}

\section{Model validation}
\label{sec:model-validation-logreg}

% augment(mlogreg_mod_3) %>% bind_cols(diatones) %>% filter(.resid>2 & .fitted<(-2)) %>% data.frame()

The previous chapter discussed `model validation' for linear regression in detail: the iterative process of diagnosing and addressing problems in the data and model to arrive at a model we have confidence in.   Because model validation addresses broad `problems' which are common to most regression models, 
%it is just as important for logistic regression as for linear regression, and
most model assumptions and diagnostic tools  are not qualitatively different for logistic regression.  For example, the `linearity' assumption is now that the predictors are linearly related to the log-odds that $y=1$, rather than to $y$ itself (as for linear regression).  There are some aspects of model validation that differ from linear regression, which we highlight below.
%or do not apply.
%crucially different from linear regression, and some do not apply.  

We discuss aspects of model validation for logistic regression using Model 2, in the same order as for linear regression in Section~\ref{sec:lr-problems-with-errors}--\ref{sec:overfitting}.
%(problems with errors, etc.) 
Because of the overlap with linear regression our discussion is briefer for logistic regression (see `Other readings': Section~\ref{sec:other-reading-ch6}).

% 
% \begin{itemize}
% \item
%   Linearity: The predictors are assumed to be linearly related to \textbf{log-odds of \(Y=1\)} (rather than to \(Y\) itself, for linear regression).
% \item
%   Collinearity: The predictors are assumed to not be linearly dependent, and the presence of collinearity causes similar issues for regression coefficients as for linear regression.
%   \end{itemize}

% - Many aspects are very similar to linear regression, such as (non-)linearity (the predictors are now assumed to be linearly related to the \textbf{log-odds of \(Y=1\)}),  multicollinearity (similar issues with coefficient estimation result when predictors are correlated), and motivations for transforming predictors.    

As for linear regressions, \ttt{augment()} in broom is a quick way to add additional information useful for model validation to the dataframe,
%for each observation, 
including predicted values (in log-odds: \ttt{.fitted}), residuals (\ttt{.resid}), and Cook's distance (\ttt{.cooksd}), discussed below.

<<out.width='45%', fig.width=3, fig.height=3>>=
mlogreg_mod_3_df <- augment(mlogreg_mod_3, data = diatones)
@


% Because of the degree of overlap with model validation for linear regression, our discussion is relatively brief.  If you regularly use logistic regression models to analyze data I recommend reading a full treatment so you're aware of the issues (e.g.,~\citet{chatterjee2012regression}, \citet{baayen2008analyzing}, \citet{gelman2007data}).


% \begin{boxedtext}{Broader context: assumptions of logistic regression}
% 
% - Model validation is sometimes presented as checking `assumptions' of linear regression; another way to think about it is dealing with possible problems to arrive at a model we can trust.  Logistic regression illustrates why the second view makes more sense.
% 
% - As Harrell (10.1) points out, logistic regression actually makes many fewer `assumptions' than linear regression.  Because logistic regression just models $P(y=1)$ directly, technically the only assumptions are independence and a correct model (linear function of predictors).  Because there are no residuals, there are none of the assumptions of linear regression related to them: normally-distributed errors, constant variance, and so on.  So technically speaking, there are fewer things to check for logistic regression---the 
% 
% 
% 
% 
% \end{boxedtext}

\subsection{Problems with the errors}

% FUTURE: JPK notes -- this is a bit confusing, given that earlier we've said logistic regressions don't have residuals  because they don't have errors.

\subsubsection{Residuals and residual plots}
\label{sec:log-reg-residuals}

Similarly to linear regression, residual plots are useful for diagnosing issues with the data or model, by looking for patterns in the errors the model makes.  For logistic regression there is no error term in the model,
so we must define residuals which preserve some intuitive properties of linear regression residuals.  There are several options which preserve different properties \citep[e.g.,][\S3.4]{agresti2007};
%but it usually doesn't make a difference which you use so 
we just discuss the most commonly-used option.\footnote{\citet[][\S5.6]{gelman2007data} illustrates using raw residuals for diagnostic plots.}
%most intuitive option would be the difference between observed (0/1) and predicted values (probabilities) (\emph{raw residuals}), possibly standardized so different observations have similar variances (\emph{Pearson residuals}) (see e.g., REF Faraway-Extending 2.4, 3.3) These are less commonly used so we don't discuss, but standardsized Pearson residuals (but they are default of some R functions, so always check what residuals you're using).



% 
% \begin{boxedtext}
% The simplest option is  \emph{Pearson residuals}: the difference between observed (0/1) and predicted values (probabilities),  scaled so the residuals have roughly equal variance for different predicted values:
% 
% $$
% e_i = \frac{y_i - \hat{p}_i}{\sqrt{\hat{p}_i(1-\hat{p}_i)}}, \quad i = 1, \ldots, n
% $$
% 
% This scaling is important 
% 
% \end{boxedtext}

\emph{Deviance residuals}
%More commonly used are \emph{deviance residuals}, 
intuitively capture each observation's contribution to the model's error on the data, analogously to the residual sum-of-squares for linear regression. Recall from equation \eqref{eq:deviance-error} that deviance can be thought of as a sum of errors from each observation.  We can define these `errors' in a way which makes deviance look  like the residual sum-of-squares ($RSS = \sum_{i=1}^{n} e_i^2$):

\begin{align}
D & = \sum_{i=1}^{n} 2 \cdot (y_i \log(1/\hat{p}_i) + (1-y_i) \log(1/(1-\hat{p}_i))) \\
& = \sum_{i=1}^{n} r_i^2
\label{eq:deviance-resid}
\end{align}

Written in this way, it is natural to define the \emph{deviance residuals} as:
$$
d_i = \text{sign}(y_i - \hat{p}_i) r_i
$$

These residuals are positive for observations where $y=1$ (since the predicted probability is always less than 1), and negative for $y=0$ observations.  These residuals, which are generated by \ttt{augment()} (or \ttt{residuals()})by default, can be used to make diagnostic plots, analogously to linear regression: fitted-residual plots,  plots of each predictor versus residuals, and so on.    


<<logreg-resid-1, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Fitted-residual (left) and fitted-binned-residual plots (right: with 10 bins, size scaled by number of observations) for Model \\ttt{mlogreg\\_mod\\_3}.'>>=
## fitted vs. deviance residuals
mlogreg_mod_3_df %>% ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  xlab("linear predictor") +
  ylab("Deviance residuals")

## fitted vs. binned residuals
## number of bins (10) is arbitrary
mlogreg_mod_3_df %>%
  group_by(bins = cut(.fitted, breaks = 10)) %>%
  summarise(fitted = mean(.fitted), resid = mean(.resid), n = n()) %>%
  ggplot(aes(x = fitted, y = resid)) +
  geom_point(aes(size = sqrt(n))) +
  ylab("Binned deviance resid.")
@

Figure~\ref{fig:logreg-resid-1} shows two such plots for Model 2.  The left panel is a fitted-residual plot:  two lines  corresponding to 0 and 1 observations.   This plot is not very useful,
%for assessing whether there is constant `mean' and `variance' in some sense throughout the data, 
but could show gross outliers (there are none here).
%In general, deviance residuals by themselves are not so useful. 

It is more helpful to use \emph{binned residuals}:  the average residual over observations within a certain range of expected values. That is, you chop the plot in the left panel into vertical bins, and average the ``Residuals'' value within each bin to get a single \(y\)-axis number for each slice.  The right panel  shows the fitted-residual plot for Model 2, using 10 bins.  

This plot now looks more like a fitted-residual plot for a linear regression.  An important difference from linear regression is that  logistic regression residuals are \textbf{not} assumed to follow a normal (or any) distribution, so it is not strictly necessary to check whether the residuals show constant variance, are normally-distributed, or have mean of zero. Nonetheless, residual plots remain useful for diagnosing qualitative problems with the data or model (e.g., nonlinearity, outliers), similarly to linear regression.

For example, in the fitted-binned-residual plot, we expect the residuals to have roughly constant mean and variance (regardless of whether they are normally-distributed)---qualitatively like the ideal fitted/residual plot for a linear regression. A trend in the mean could indicate a nonlinear predictor effect, for example.  This plot  doesn't suggest any major issue.
%(like a ``funnel'' shape would). 

%We expect these residuals to be roughly normally distributed with constant mean and  variance across different expected values---that is, we expect an expected-value/binned-residual plot to look like the ideal fitted/residual plot for a linear regression.\footnote{With the exception that the mean value is not necessarily zero, as for linear regression.}  This plot doesn't suggest any major issue (like a ``funnel'' shape would).


% 
% There are several alternative residuals that can be defined for logistic regression, including:
% 
% \begin{itemize}
% \item
%   Deviance residuals (use \texttt{glm.diag()} in the \texttt{boot} package)
% \item
%   Binned residuals (use \texttt{binned.resids()} in the \texttt{arm} package)
% \end{itemize}
% 
% Using either kind of residuals, you can evaluate a logistic regression model using similar diagnostic plots as for linear regression: Q-Q plots, fitted-residual plots, and plots of each predictor versus residuals.




% 
% - In logistic regression there are no residuals in the model, so there are no assumptions related to residuals to satisfy per se---such as constancy of variance or normality of residuals, for a linear regression.

\subsubsection{Independence}
\label{sec:diatones-mod-glm-indep}

Logistic regression assumes that observations are independent:  knowing the  predicted log-odds for one observation should not give you any information about the predicted log-odds of another observation, after accounting for their predictor values.
%(they are `conditionally independent').
%(Technically:  $Prob(y_i)$ conditionally independent of $Prob(y_j)$, given predictors.)  
As for linear regression,  violations of independence are the most important issue to address for logistic regression, and the hardest to diagnose (same as Section~\ref{sec:lin-reg-indep})---they require thinking about the structure of your data for possible sources of non-independence.  

For the \ttt{diatones} data, there is a natural possibility to check: that words sharing the same prefix (e.g., ``survey'', ``surprise'') are not independent.  We can check this by examining the distribution of residuals for different values of \ttt{prefix} (Figure~\ref{fig:prefix-resid}).


<<prefix-resid, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Average deviance residuals for Model \\ttt{mlogreg\\_mod\\_3} by word \\ttt{prefix} (errorbars are 95\\% CIs).'>>=
mlogreg_mod_3_df %>% ggplot(aes(x = prefix, y = .resid)) +
  stat_summary(fun.data = "mean_cl_boot") +
  ylab("Deviance residuals") + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.5))
@

It looks like different prefixes mostly have similar residual distributions, but a few have much higher residuals than others---especially the prefixes \tsc{iN} and \tsc{pro}.  We will return to this issue later (Section~\ref{sec:diatones-melr}), where we check whether fitting a model which accounts for this source of non-independence changes the qualitative conclusions we draw from the model.


\subsection{Problems with the model and predictors}


\subsubsection{Linearity, transformations, collinearity}

Possible problems with the model, and how these can be addressed by nonlinear effects or transformations (Section~\ref{sec:lr-model-problems}--\ref{sec:transformations-problems}), are broadly similar to linear regression.   Most importantly, \emph{linearity} remains a crucial assumption for logistic regression models (no missing nonlinear effects, interactions, etc.), which can be checked using similar methods to linear regression such as plotting (binned) residuals as a function of a continuous predictor, or 2+ predictors.     It is no longer possible to transform the response for a logistic regression, because its values must remain 0 and 1.  But all motivations for transforming  predictors---especially centering and scaling, to improve interpretability of regression coefficients and to make their magnitudes comparable---are just as important as for linear regression. 

As for linear regression (Section~\ref{sec:lr-predictor-problems}), the predictors in logistic regression are assumed to not be linearly dependent, and the presence of collinearity has similar effects on regression coefficients.  

Exercise~\ref{ex:model2-validation} asks you to assess possible nonlinearity in the effect  of \ttt{frequency} and assess the degree of collinearity for Model 2.

\subsubsection{Separation and quasi-separation}
\label{sec:sep-quasi-sep}

A new potential problem with predictors for logistic regression is \emph{separation}: when a subset of predictor values is associated only with $y=1$, or only with $y=0$.

As an example, first note that for some parts of the \ttt{diatones} data  no words have shifted stress, such as words with
%There are no words with 
second syllable coda=\tsc{ccc}:
%which have shifted stress:

<<>>=
xtabs(~syll2_coda_orig + stress_shifted, diatones)
@

% 
% <<>>=
% diatones %>%
%   count(syll2_coda_orig, stress_shifted) %>%
%   pivot_wider(
%     names_from = stress_shifted, values_from = n,
%     names_prefix = "stress_shifted",
%     values_fill = list(n = 0)
%   )
% @
%(Here we have used tidyverse functions for making contingency tables (\ttt{count}) and for `pivoting' the data to a readable format.)


% <<>>=
% xtabs(~stress_shifted + syll1_coda_orig+ syll2_coda_orig, data=diatones)
% @
% 
% In particular, suppose we didn't have the one case where syll1coda=yes and stress shifts, so that in this data all words with syll2coda=CC and first syllable coda have $y=0$:
% <<>>=
% diatones_sub <- diatones %>% filter(!(syll1_coda_orig=='yes' & syll2_coda_orig=='CC' & stress_shifted==1))
% @


Fitting a simple model to this data,  predicting stress shift just as a function of \ttt{syll1\_coda\_orig} and \ttt{syll2\_coda\_orig}, gives an odd result:

<<>>=
glm(stress_shifted ~ syll1_coda_orig + syll2_coda_orig,
  data = diatones, family = "binomial"
) %>% tidy()
@

The \ttt{syll2\_coda\_origCCC} coefficent has a very high estimate and standard error---which means that some observations (those with second syllable coda = \tsc{ccc}) will have a predicted probability very near 0.  
%The warning tells us that either $y=1$ or $y=0$ is predicted to be certain for some observations---
This is \emph{complete separation}.  To fit this data, the model has chosen coefficients which are very far from 0 so that the predicted probability is effectively 0 or 1 for some observations.  The problem with this is similar to linear dependence: the model is unstable in the sense that there is no longer a unique set of `best' coefficients.  Choosing a \ttt{syll2\_coda\_origCCC} estimate which is half or twice as large will make effectively the same model predictions, hence the standard errors are huge.

Like linear dependence, \textbf{complete} separation is fairly easy to spot, and indicates a problem with your data or model that should be fixed.\footnote{For more extreme cases, R throws a warning: \ttt{glm.fit: fitted probabilities numerically 0 or 1 occurred}.} Trickier is \emph{quasi-complete separation}, which is analogous to very high collinearity: if the predicted probability for some set of predictor values is very close to 0 or 1, some coefficients will have high values/standard errors because a range of coefficient values give similar model predictions \citep[][\S17.4.3]{baguley2012serious}.  The resulting issues are similar to high multicollinearity, and it is similarly debatable whether or not quasi-complete separation is a `problem' or not.  
%Quasi-complete separation often occurs as more predictors are added to a logistic regression model (because it is likely there are very few observations for some combination of predictor values)
%Unlike collinearity, to my knowledge there are not standard methods for quantifying the degree of quasi-separation in a dataset.

% 
% Like linear dependence, it is easy to get complete separation without realizing it---especially since R doesn't always throw an error. For example, fitting the same model without first excluding the one point:
% <<>>=
% sep_mod_2 <- glm(stress_shifted ~ syll1_coda_orig + syll2_coda_orig, data=diatones, family='binomial')
% summary(sep_mod_2)
% @
% 
% There is still complete separation, because words with syll2coda=CCC never shift stress---hence the large estimate/standard error for X. But now a warning was not given (possibly because the predicted probability is not numerically close enough to zero).
%Like linear dependence, *complete* separation is fairly easy to spot, and indicates a problem with your data or model that should be fixed.  

Complete/quasi-complete separation is essentially a case of overfitting---it is unlikely that the true probabilities are 0 or 1, but the more predictors are added to a model, the more likely some combinations of predictors will have only $y=0$ or only $y=1$ observations. Thus, possible solutions are those applicable to overfitting: collecting more data, using a simpler model (e.g., combining factor levels, in this case \tsc{cc} and \tsc{ccc}), or using a variant of logistic regression which penalizes large coefficients.
%% FUTURE: reinclude?

%(Box~\ref{box:pmle}).
% 
% Trickier is \emph{quasi-complete separation}, which is analagous to very high collinearity: if the predicted probability for some set of predictor values is very close to 0 or 1, some coefficients will have high values/standard errors because a range of ceofficient values give similar model predictions.  The resulting issues are similar to high multicollinearity, and it is similarly debatable whether or not quasi-complete separation is a `problem' or not.  Unlike collinearity, to my knowledge there are not standard methods for quantifying the degree of quasi-separation in a dataset.
%
% later: example, or just refer elsewhere or to exercise.  (QCS = similar issues to multicollinearity.  affects)

See  \citet[][\S5.4.2]{agresti2015foundations}; \citet[][\S17.4.3]{baguley2012serious}; \citet[][273]{levshina2015linguistics} for further discusion of (quasi-)separation and possible solutions. 
%(but not multicollinearity),
% discusses for linguistic data.
%maybe cite: "On the Relationship between Multicollinearity and Separation in Logistic Regression"


% 
% ---
% 
% For linear regression, residual plots were used to diagnose problems with data and the model, such as XX. These are still potential problems for logistic regression models, and it is useful to *define* residuals to make similar plots.
% 
% * Still useful to examine residuals plots to diagnose issues with data and model; examples below.
% * Two kinds of residuals: deviance and binned.
% * Fitted-residual plots: not so helpful for deviance resids, just get two lines, 
% 
% How should we define these residuals? The simplest option







% 
% - As for LMs, \ttt{augment} in broom is a quick way to add all needed info, incl predicted values (= linear predictor), Cook's distance, residuals (deviance by default):
% 
% Residual plots:
% 
% <<out.width='45%', fig.width=3, fig.height=3>>=
% ## get dataframe augmented.  the data argument is useful to just end up with the original dataframe plus more columns
% mlogreg_mod_3_df <- augment(mlogreg_mod_3, data=diatones)
% ## fitted vs. deivance residuals
% mlogreg_mod_3_df %>% ggplot(aes(x=.fitted, y=.resid)) + geom_point() + xlab("linear predictor") + ylab("Deviance residuals")
% 
% ## fitted vs. binned residuals
% ## n idea from faraway. number of bins a bit arbitrary
% mlogreg_mod_3_df %>% group_by(bins=cut(.fitted, breaks=10)) %>%
%   summarise(fitted = mean(.fitted), resid=mean(.resid), n=n()) %>%
%   ggplot(aes(x=fitted, y=resid)) + geom_point(aes(size=sqrt(n)))
% @

% - make sure to specify these are deviance residuals


\subsection{Problems with observations}
\label{sec:log-reg-prob-obs}

For the same reasons as for linear regression, we are interested in identifying observations which have particularly extreme predictor values, for which the model predicts particularly badly, or which have high influence on coefficient values (Section~\ref{sec:lr-problems-with-observations}). These are quantified by defining measures of {outlier}-ness (standardized residual), {leverage} (hat values), and {Cook's distance} (CD),  similarly to linear regression; above we added these columns to \ttt{mlogreg\_mod\_3\_df} using \ttt{augment()}.
%these are the \ttt{.std.resid}, \ttt{.hat}, and \ttt{.cooksd} columns added by \ttt{augment}.
% 
% 
% \protect\hyperlink{lin-reg-measuring-influence}{to linear regression}. 
% In R, the \texttt{glm.diag()} function in the \texttt{boot} package computes CD. Similarly to linear regression, the distribution of CD values can be plotted and used to identify observations with extreme values.

Again, visual inspection or quantitative diagnostics can be used to decide which points are `influential', etc.  For example, \ttt{outlierTest()} in the {car} package suggests no clear outliers.
%\footnote{This function just transforms residuals to (hopefully) follow a normal distribution (`studentized': Box~\ref{box:standardized-residuals}), then performs a $t$-test on every residual value, corrected for multiple comparisons.}

<<>>=
outlierTest(mlogreg_mod_3)
@

This can also be assessed by a leverage-residual plot 
%one of the diagnostic plots R makes by default for a logistic regression model
%(\ttt{plot(mlogreg\_mod\_3, which=5)}: 
(Figure~\ref{fig:log-reg-lev-resid} left).  This plot suggests a few possible outliers.  

%for  unusual observations for model \\ttt{mlogreg\\_mod\\_3}: leverage-residual plot (left), histograms of leverage and Cook's distance (center, right).'

<<log-reg-lev-resid, echo=FALSE, out.width='30%', fig.asp=.75, fig.width=default_fig.width*.3/default_out.width,  fig.cap='Diagnostic plots for  unusual observations for model \\ttt{mlogreg\\_mod\\_3}: leverage-residual plot (left), histograms of leverage and Cook\'s distance (center, right).'>>=

#library(grid)
#library(gridGraphics)

mlogreg_mod_3_df %>% ggplot(aes(x=.hat, y=.std.resid)) + 
  geom_point(color='darkgrey') +
  geom_smooth(color=default_line_color, se=F, size=0.75) +
  xlab("Leverage (hat)") + ylab("Std. residuals")

#p2<-
  mlogreg_mod_3_df %>% ggplot(aes(x = .hat)) +
  geom_histogram() +
  xlab("Leverage (hat)")
#p3<-
  mlogreg_mod_3_df %>% ggplot(aes(x = .cooksd)) +
  geom_histogram() +
  xlab("Cook's distance")

#old_par <- par(mar = c(0, 0, .5, 1), bg = NA)
#wrap_elements(
#  panel=~plot(mlogreg_mod_3, which = 5, sub.caption = ''), clip = FALSE
#) + p2 + p3
#par(old_par)
@


Histograms of hat values and and Cook's distance (Figure~\ref{fig:log-reg-lev-resid} center, right) don't suggest any points which have unusual leverage or influence.  The most influential points are a couple words with prefix \tsc{re}:

<<output.lines=1:3>>=
filter(mlogreg_mod_3_df, .cooksd > 0.1)
@


These words are also the potential outliers in the leverage-residual plot---they are influential because they have shifted stress while the model predicts a low probability of stress shift.

We might flag these points to see what effect they have on the model. It turns out that the model has higher predictive power (AUC) and all qualitative results are strengthened when these points are excluded (Exercise~\ref{ex:model2-validation}). So we can be confident that the model's results aren't due to a few points.  This should be reported in a write-up of the model.
%, even in one sentence.)

% For these words, same/similar for most predictors, model strongly predicts no shift. taking them out *strengthens* all qualitative results (and Dxy up a lot). so we can be confident results aren't just from a couple influential points. good to report if you have space.

% (already checked: the point about influential points for williams effect doesn't hold for this data, just take out.)

%(already checked collinearity -- quite low. the issue here would be *overfitting*.  could put in the optimism-corrected Dxy to illustrate, and also `penalized' version of likelihood ratio model -- suggests that we are overfitting some, but qualitative conclusions are robust.)


% \section{Model criticism for logistic regression}
% \label{sec:logistic-regression-validation}



% Regression assumptions and model diagnostics are just as necessary to assess a fitted model (``model criticism'') as for linear regression, where \protect\hyperlink{linear-regression-assumptions}{we covered these topics} in depth. The assumptions and diagnostics differ somewhat for logistic regression, but not at a qualitative level. For example:
% 
% \begin{itemize}
% \item
%   Linearity: The predictors are assumed to be linearly related to \textbf{log-odds of \(Y=1\)} (rather than to \(Y\) itself, for linear regression).
% \item
%   Collinearity: The predictors are assumed to not be linearly dependent, and the presence of collinearity causes similar issues for regression coefficients as for linear regression.
% \item
%   Residual plots and measures of influence: conceptually very similar for logistic regression, except ``residuals'' needs to be defined differently because there is no error term (\(e_i\)) for logistic regression.
% \end{itemize}
% 


% \hypertarget{residual-plots}{%
% \subsection{Residual plots}\label{residual-plots}}
% 
% 
% \hypertarget{example-binned-residuals-versus-expected-values}{%
% \subsubsection*{Example: Binned residuals versus expected values}\label{example-binned-residuals-versus-expected-values}}
% \addcontentsline{toc}{subsubsection}{Example: Binned residuals versus expected values}
% 
% \emph{Binned residuals} are the average (raw) residual over observations within a certain range of expected values (log-odds). That is, you chop the plot immediately above into vertical slices, and average the ``Residuals'' value within each slice to get a single \(y\)-axis number for each slice.
% 
% We expect these residuals to be roughly normally distributed with mean 0 and constant variance across different expected values---that is, we expect an expected-value/binned-residual plot to look like the ideal fitted/residual plot for a linear regression.
% 
% Consider the final model we ended up with in \protect\hyperlink{log-reg-worked-example}{the example above}:
% 
% <<>>=
% ex1.mod <- glm(stressshift ~ npType.pron + conditionLabel.williams + voice.passive + order.std + conditionLabel.williams:voice.passive, 
%                family="binomial", 
%                data=givenness)
% @
% 
% To make a plot of expected values (= fitted values) versus binned residuals:
% 
% <<fig.align='center', fig.height=4, fig.width=6>>=
% ## binned residual plot
% binnedplot(predict(ex1.mod), resid(ex1.mod))
% @
% 
% The diagnostic plot doesn't suggest any major issue (like a ``funnel'' shape would), but the observations around expected value = -2.5 merit further investigation.
% 
% \hypertarget{logistic-regression-cooks-distance}{%
% \subsection{Cook's distance}\label{logistic-regression-cooks-distance}}
% 
% Cook's distance (CD) can be defined for each observation in a logistic regression model, similarly \protect\hyperlink{lin-reg-measuring-influence}{to linear regression}. In R, the \texttt{glm.diag()} function in the \texttt{boot} package computes CD. Similarly to linear regression, the distribution of CD values can be plotted and used to identify observations with extreme values.

% Here is an example using the multiple logistic regression model considered above:
% 
% <<message=FALSE, fig.align='center', fig.height=4, fig.width=6>>=
% ## add a column showing Cook's distance values to dataframe
% givenness <- mutate(givenness, cooksDistance=glm.diag(ex1.mod)$cook)
% 
% ggplot(aes(x=cooksDistance), data=givenness) + 
%   geom_histogram()
% 
% @
% 
% It turns out that the points with highest Cook's Distance are passive voice observations in contrast condition where stress was shifted:
% 
% <<fig.align='center', fig.height=4, fig.width=6, message=FALSE>>=
% ggplot(aes(x=cooksDistance), data=givenness) + 
%   geom_histogram(aes(fill=conditionLabel)) + 
%   facet_wrap(~stressshift+voice)
% @

% In fact, all shifted observations in the \emph{contrast} condition are influential, as well as many non-shifted observations in the \emph{williams} condition (top-right panel). This is related to the fact that the Williams effect in this data is very strong: basically, any observation that is unexpected---does not show the Williams effect in the \emph{williams} condition, or does show the effect in the control (\emph{contrast}) condition---is influential, because it goes against the overwhelming majority of the data.

% This example illustrates why it may be preferable to use visual inspection of the distribution of CD values to flag ``highly influential'' points, rather than a rigid cutoff (\citet{chatterjee2012regression}, 4.9.1). One common suggested cutoff is 4/\(n\) (\(n\) = number of observations), which would be 0.01 for this dataset. This cutoff would flag as influential \emph{most} data where stress doesn't shift as expected by the Williams effect. This is correct, in some sense (those points are more influential than others), but not helpful in terms of deciding which observations are ``highly influential''---these are the exact observations which let us estimate the size of the Williams effect.
% 

\subsection{Example: Assessing overfitting}
\label{sec:log-reg-overfitting}

In Section~\ref{sec:overfitting-underfitting} we discussed different `internal validation' methods for assessing overfitting (for linear regression), without a practical example.
Because the \ttt{diatones} dataset is fairly small relative to the number of predictors, this is a good case to show one way to assess whether a model overfits, and whether overfitting affects the conclusion of the analysis.  

The discussion there carries over to logistic regression, with the rough rule of thumb that overfitting may be a concern for a model with more than $n/15$ predictors--where $n$ is now the number of observations with the \textbf{less common} outcome \citep[][\S4.4]{harrell2015regression}.  For the \ttt{diatones} data, $n=21$ (words with shifted stress), so $n/15 = 1.4$, and our best model (Model 2) has more predictors than this (6), so overfitting is a possibility.




At a high level, we will be:
\begin{enumerate}
\item Assessing the degree of overfitting (with a bootstrapping method)

\item Refitting the model using a method robust to overfitting
%(with penalized maximum likelihood)

\item Comparing the results of the original and new model 
\end{enumerate}
%(with visual inspection)

We use functionality from the {rms} package for (1)--(3), described in more detail by  \citet[][\S6.2.4]{baayen2008analyzing}. There are different ways to do (1), (2), and (3), but the general idea is the same.  






We first refit the model using the logistic regression function from {rms}:

<<>>=
rms_mod_1 <- lrm(formula(mlogreg_mod_3), data = diatones, 
  x = T, y = T)
@

This model has an identical coefficient table to Model 2 fitted with \ttt{glm()} above (not shown).  Its goodness of fit, as an AUC value, can be extracted from an \ttt{lrm} model as follows:
%from the fitted model as follows:\footnote{We calculate AUC from $D_{xy}$ (Box~\ref{box:auc-intuition}) because the \ttt{lrm()} functions we're using only give $D_{xy}$.}

<<>>=
## The C-index is the same as AUC for a logistic regression
rms_mod_1$stats[["C"]]
@

We assess overfitting with the \ttt{validate()} function, which applies a bootstrapping method \citep[][\S5.3.5]{harrell2015regression}: refit the model on many re-sampled datasets, each time re-calculating goodness of fit using the new model on the full dataset and the resampled dataset---the difference between these is the `optimism', or how much the goodness-of-fit measure is inflated due to overfitting.

<<output.lines=1:2>>=
val <- rms::validate(rms_mod_1, B = 100)
val
@

In this case, the AUC measure has optimism (estimated over 100 runs) of \Sexpr{val['Dxy', 'optimism']/2} (the $D_{xy}$ value divided by 2), so the corrected AUC is \Sexpr{0.5 + val['Dxy', 'index.corrected']/2} (0.5 + \Sexpr{val['Dxy', 'index.corrected']}/2). That's a big drop (recall that 0.5 is baseline)---so there is some overfitting, as expected.

To assess whether it matters that the model is overfitted with respect to our research questions, we refit the model correcting for overfitting, 
%using a method which corrects for overfitting,
and compare to our original results. Specifying the \ttt{penalty} argument to \ttt{lrm()} fits the model using \emph{penalized maximum likelihood}, where a term is added to the likelihood which penalizes coefficients further from zero.
%thus shrinking coefficeints towards zero.
%% FUTURE: re-add if box re-added
% (Box~\ref{box:pmle}).
This has the effect of shrinking coefficients towards zero.
%An `optimal' value of \ttt{penalty} is used (from \ttt{pentrace()}) which which The \ttt{pentrace} function tries a range of \ttt{penalty} values and finds the `optimal' one, defined as maximizing a modified AIC.  

%% FUTURE: reinclude?
% \begin{boxedtext}{Broader context: penalizing model complexity}
% \label{box:pmle}
% It is worth going into more detail on refitting the model, 
% %two steps used in refitting the model,
% to understand the general idea: fitting a model which somehow penalizes `more complex' models (a.k.a.\ \emph{regularization})
% % 
% % 
% % ---which come up all over the place (especially for mixed models), but without explanation.  
% % 
% % There are two steps: (1) fitting a model which somehow penalizes `more complex' models parametrized by $\lambda$; (2) choosing an optimal $\lambda$.
% 
% Penalized maximum likelihood estimation is one way to do this. The penalized log-likelihood is:
% 
% \begin{equation}
% \log(L'(\beta_0, \beta_1, \ldots, \beta_p, \lambda)) = 
% \log(L(\beta_0, \beta_1, \ldots, \beta_p)) -
% \frac{1}{2} \lambda \sum_{i=1}^{p} \beta_i^2
% \lambda
% \end{equation}
% 
% where $L$ is the unpenalized likelihood (Eq.~\ref{eq:likelihood-mult}), and $\lambda>0$. (This formulation assumes that every predictor has been standardized, which we assume for simplicity.)  For a given $\lambda$, PMLE chooses the coefficients maximizing $L'$, which is a trade-off between fitting the data well ($L$) and keeping coefficients closer to 0 (the penalty). $\lambda$ controls the degree of \emph{shrinkage} of coefficients towards zero: too-small $\lambda$ will overfit, too-large $\lambda$ will underfit (Section~\ref{sec:overfitting-underfitting}).
% %A small $\lambda$ gives a model (the \ttt{glm} default) which fits the data well at the risk of overfitting; increasing $\lambda$ gives a model which generalizes better to unseen data, at the risk of underfitting.  
% There are many variants on this theme---for example `LASSO regression' replaces $\beta_i^2$ with $|\beta_i|$, while Bayesian logistic regression  uses a penalty term describing how much less likely coefficient estimates further from zero are---but the general idea is the same. 
% 
% There are many applications of this general idea. For example, when we get to mixed-effects models, the \ttt{lme4} package uses a penalized least-squares algorithm to choose the values of random effects, which are all `shrunk' towards zero relative to the values that would be estimated by just taking averages of empirical data.  Another example is fitting models in the presence of complete or quasi-complete separation (Section~\ref{sec:sep-quasi-sep}), which can be thought of as a case of overfitting (the \textbf{true} probabilities are probably not exactly 0 or 1). The \ttt{logistf} package provides an easy-to-use Bayesian logistic regression specialized for this case \citep{logistf}.
% % 
% % We do (2) by fitting models with a range of $\lambda$ values (the `optimization algorithm'), then choosing the one which maximizes an AIC-like metric quantifying fit to the data versus model complexity (the `objective function').  This optimization algorithm, `grid search', is the brute force option; it is more common to use a more complex option that doesn't require evaluating all possible $\lambda$. It is also more common to use cross-validation error rather than a metric like AIC.
% % 
% % (maybe we don't need this at all, just )
% \end{boxedtext}
% % 
% This all sounds complicated but it is just two conceptually simple steps: 
%  refitting using a method robust to overfitting parametrized by $\lambda$; choosing the best $\lambda$. It's easy to get lost in the weeds here, esp since `overfitting' more commonly addressed in machine learning. Important to know that there are different ways to do these steps: different methods `regularize' in different ways (like `ridge regressin', `lasso'), parametrized by some $\lambda$ which somehow shrinks towards 0. Choose $\lambda$ in some way---maximize a criterion like AUC or cross-validation error.  But the general idea is simple.
% 
% - assess overfitting (here: bootstrapping method)
% 
% - refit the model using a method robust to overfitting (here: penalized maximum likelihood)
% 
% - compare the results of the original and new model (here: visual inspection)
% 

<<output.lines=1:10>>=
# Find 'optimal' penalty: maximizes modified AIC
pt_result <- pentrace(rms_mod_1, seq(0, 2.0, by = 0.02))
## Refit model with this penalty
rms_mod_2 <- update(rms_mod_1, penalty = pt_result$penality)
@


<<output.lines=14:22>>=
rms_mod_2
@

Compare to the unpenalized model:

<<output.lines=13:22>>=
rms_mod_1
@

Every (non-intercept) coefficient in the penalized model are closer to zero, and has a $z$-value closer to zero (= a higher $p$-value).  However, the \textbf{qualitative} results are almost the same: the coefficients have the same signs, the same relative effect sizes, and relatively similar $p$-values (e.g., on the same side of $\alpha = 0.05$ or 0.01).
%which side of 0.05 they are on).
%on the same side of $p=0.05$. 
%(The one exception is the \ttt{syll2\_td}:\ttt{frequency} interaction, which was previously just below the significance threshold and is now just above it.) 
We don't expect any of our qualitative conclusions about the model to change, as can be verified by making the same prediction plots as in Section~\ref{sec:viz-effects-logistic}---although all confidence intervals are wider and all effects a bit `flatter', the qualitative patterns are identical.   Thus,  (1) our original model was overfitted, but (2) overfitting wasn't severe enough to affect the conclusions we draw from the analysis.  At this point we could either report the penalized model (as in \citealp{sonderegger2010tfs}) or report the original model, noting (1) and (2).  This choice is an instance of the bias-variance trade-off (Section~\ref{sec:omitted-variable-bias})---coefficients from the penalized model are more biased than the original model (by design), but have lower variance.

Two important take-aways: it is important to evaluate overfitting when sample size is small relative to the model's complexity,
%(assessed e.g., by the $n/15$ rule).
but an overfitted model is not necessarily a problem if the conclusions drawn in your analysis are robust.


\section{Reporting and summarizing}
\label{sec:log-reg-reporting}


\subsection{Reporting a logistic regression model}

Reporting a logistic regression model in a write-up is generally similar to reporting a linear regression model: the guidelines and rationale in Section~\ref{sec:reporting-mlr} for reporting individual coefficients and the whole model hold, with some adjustments.

\paragraph{Coefficients}

For each regression coefficient you report at a minimum the  coefficient estimate, its standard error, the test statistic value (whether $z$ for a Wald test or $\chi^2$ and $df$ for a likelihood-based test:  Section~\ref{sec:log-reg-uncertainty}), and corresponding $p$-value .  It is best to report all model coefficients in a table (Example below), in addition to possible in-text reporting.
% 
% - For multiple regression strongly recommended to report all coeffs in a table.  Again, minimum and most common:  beta, standard error, test stat value and corresponding $p$-value. Example below for Mod 2
% 
% - Most common to use $z$ and corresponding $p$. (If you use a different method, change test stat/p/df.)

As for linear regression it is useful to also give visualizations, confidence intervals, and basic descriptive statistics, but what is appropriate will depend on context and space.  Model prediction plots are especially important for interpreting logistic regressions,  as discussed above (Section~\ref{sec:viz-effects-logistic}). 
%are more important than for linear regression for visualizing effects -- especially when interactions involved -- because of probability scale.  

\paragraph{Model summaries}

It is common to report some quantitative measures of the model under the regression table: its goodness of fit (e.g., AUC, $D_{xy}$, Nagelkerke $R^2$, accuracy); or measures of likelihood/information (deviance of the full and baseline model with associated $df$, AIC or BIC, log-likelihood); or the number of parameters and observations.  These and other measures are reported by \ttt{summary()}, \ttt{glance()}, or packages discussed in Section~\ref{sec:gof-logistic}.  

There is no standard set of measures to report; the minimum seems to be one or more measures of goodness of fit (such as AUC or $D_{xy}$, not accuracy), number of observations, and one or more measures of model likelihood versus the baseline (e.g., deviance of full and null models, difference in deviance), all of which give important information about the model and from which other information of interest can be determined.
%(see Example below).  

%It is not uncommon in linguistics (including my own work) to report no summaries, but this is not a good option.

%Model summaries (like 4.6.2): common to report some quant summarise of the model under regression talbe: goodness of fit (see above), measues of likelihood/info (deviance of the full and null model and associated $df$, AIC/BIC, log-lik), number of params and observations.  all reported by summary or glimpse

% Like lin reg, no standard set (including frequent no summary) in linguistics. Reasonable minimum: measure(s) of goodness of fit (such as AUC or DXY), $n$ (which could alternatively be in text), measure of model likelihood vs.\ baseline (e.g., deviance on full and null models, or just diff in deviance).  Example below.

\paragraph{Example}

This is one possible regression table for Model 2 (\ttt{mlr\_mod\_3}), following \citet{faraway2016extending} (and adding AUC):


\begin{tabular}{lcrrr}
\toprule 
Coefficient & $\hat{\beta}$ & $SE(\hat{\beta})$ & $z$ & $p$ \\
<<echo = FALSE, warning=FALSE, message=FALSE, results = 'asis'>>=
tidy(mlogreg_mod_3) %>%
  select(one_of("estimate", "std.error", "statistic", "p.value")) %>%
  add_column(coeff = c("Intercept", "Syll.\ 2 coda", "Syll.\ 2 t/d", "Frequency", "Syll.\ 1 coda", "S2 t/d:Frequency", "S1 coda:Frequency"), .before = 1) %>%
  printWrapper(pCol = "p.value")
@
\end{tabular}
{\footnotesize 
$n = \Sexpr{length(mlogreg_mod_3$y)}$, 
AUC = $\Sexpr{auc(mlogreg_mod_3)}$, 
$p = \Sexpr{mlogreg_mod_3$rank}$, 
deviance = \Sexpr{deviance(mlogreg_mod_3)},
null deviance = \Sexpr{glance(mlogreg_mod_3)$null.deviance}
(difference = \Sexpr{glance(mlogreg_mod_3)$null.deviance - deviance(mlogreg_mod_3)})
}

% accuracy and another measure (like AUC or Dxy) for goodness of fit, n, 
% 
% - log-lik: gimpse, CH
% - G : CH
% - p-val of LR total: CH
% - num coeffs: Far, CH
% - n: Far
% - Deviance: summ, Far, glipmse
% - Null Dev: summ, Far, glimpse
% - Dev diff: Far
% - AIC: summ, performance, glimpse
% - BIC: performance, glimpse
% - PCP (sort of accuracy)

% 
% \begin{boxedtext}{Practical note: reporting a logistic regression}
% 

% 
% - same principles apply as for a linear regression as for mult lin reg (?)
% 
% - do include 'a figure such as (s-shaped curve effect plot already done) can be helpful, more important than for LR because of difficulty of interpretation of log reg'
% 
% %(and maybe just say that to report a single logistic regression same principles apply but now need a table, as for MLR)
% 
% \end{boxedtext}


% FUTURE: clarify this section
% Michaela: It's not totally clear to me how this section fits in with the rest of the chapter. It seems like this is no longer really about logistic regression
% And then finds it hard to follow... "can transitions be smoothed"?
%
% Maybe at minimum: at the beginning, make clearer why we are discussing this now -- because we want to be able to deal with models with interactions, or where several terms correspond to one RQ.
%
% (but )

\subsection{Variable importance in more complex models}
\label{sec:variable-importance-complex}

It is often of interest to compare the relative importance of different predictors for a model.  We have discussed how to do this in two ways, so far only for relatively simple linear and logistic regression models---containing only continuous predictors or two-level factors (no interactions, factors with 3+ levels, non-linear effects, etc.).  The \textbf{effect size} of different predictors can be read off from the model coefficients (when predictors are standardized). The \textbf{significance} of different predictors can be compared via their $p$-values.

These methods don't generalize to more complex models, where we want to ask `what is the importance of predictor $x$'? For example, given the motivation of the \ttt{diatones} study, it is of interest for Model 1 to know how important each predictor is overall---\ttt{frequency}, and each `structure' predictor---including both the main effect and interactions including this predictor.  We discuss a couple ways to do this, defining `importance' differently.

\subsubsection{Model comparison}
\label{sec:logreg-model-comparison}

The contribution of each predictor can be assessed by comparing the full model with a model where the predictor and all its interaction terms are dropped.

The `significance' of each comparison can be assessed by an appropriate hypothesis test.  For example, a likelihood-ratio test for \ttt{syll1\_coda} would exclude the \ttt{syll1\_coda} and \ttt{syll1\_coda:frequency} terms:

<<output.lines=6:8>>=
mlogreg_mod_3_nosyll1_coda <- update(mlogreg_mod_3, . ~ . -
  frequency:syll1_coda - syll1_coda)
anova(mlogreg_mod_3, mlogreg_mod_3_nosyll1_coda, test = "Chisq")
@

Similar test would be conducted for \ttt{syll2\_coda}, \ttt{syll2\_td}, and \ttt{frequency}. As a shortcut, we can  apply \ttt{anova()} to the model refitted using \ttt{lrm}.  (This applies Wald tests rather than likelihood-ratio tests, but the idea is the same.)
<<>>=
anova(rms_mod_1, india=FALSE)
@

(The `india` option excludes some rows we don't need.)  Here, the \ttt{syll1\_coda} row reports the hypothesis test when the two terms above are excluded, and so on.\footnote{This  method also works for linear regression models, 
%using $F$ tests---either by hand, with \ttt{update}/\ttt{anova} calls, or 
by refitting the model using the \ttt{ols()} function from {rms}.}  
The $p$-values for the first four rows are metrics of the 
%Analogous hypothesis tests can be carried out for \ttt{syll2\_coda}, \ttt{syll2\_td}, and \ttt{frequency}.  The $p$-values of the four tests are metrics of the 
\textbf{significance} of each variable's contribution.  This gives the following order of predictor importance  (Exercise~\ref{ex:var-imp-diatones}):
%$\chi^2 - df$ can be used as an \textbf{effect size} measure (I think)---this is what is shown as a measure of variable importance by default if you plot the object above (\ttt{plot(anova(rms\_mod\_1))}). 
\begin{center}
\ttt{syll1\_coda} $>$ \ttt{frequency} $>$ \ttt{syll2\_td} $>$ \ttt{syll2\_coda}
\end{center}

That is, the $p$-value is lowest for \ttt{syll1\_coda}, and so on.

Alternatively, an appropriate \textbf{effect size} can be calculated for each model comparison (Cohen's $f$ for linear regression, the odds ratio for logistic regression: equation~\ref{eq:cohens-f}, section~\ref{sec:log-reg-lik-ratio}) and used as a metric of each predictor's importance. For example, for \ttt{syll1\_coda} the odds ratio would be:

<<>>=
oddsratio_lr(mlogreg_mod_3_nosyll1_coda, mlogreg_mod_3)
@

The odds ratios turn out to give the same order of predictor importance as above (Exercise~\ref{ex:var-imp-diatones}), but in general the conclusions from effect size and significance could be different.

% - Here, X2 and X2-df  serve as undstandardized and standardized effect sizes -- I think.  The latter is shown by default when you plot the anova.  So the order of importance is (comments):
% syll1\_coda $>$ freq $>$ syll2\_td $>=$ syll2\_coda


% - We have discussed ways to compare the contributions of different predictors for relatively simple linear and logistic regression models: just main effects, no interactions or anything more complex (like nonlinear terms)
% 
% - Can compare effect size using (standardized) model coefficients; or contibution to model likelihood using t/z values.
% 
% - These methods don't apply straightfowardly to more complex models where we want to know `what is the effect of X' (whether effect size or `significance').  This kind of `variable importance' number is often of interest
% 
% - For example, for Model 2 above: given research questions, it's of interest how important each predictor is *overall* -- across whole dataset: one number each for syll1\_coda, frequency, etc.
% 
% - One option: LR tests excluding each predictor plus higher-order interactions its involved in. (see comments)
% anova(rms_mod_2)

This order makes intuitive sense, looking at the partial-effect plots for this model (Figure~\ref{fig:mlogreg-eff-3}). The first syllable coda clearly has a large effect. Frequency also has a large effect, if we are thinking of its `absolute' effect---adding together its positive and negative effects for different values of `structure' variables.
% 
% - This makes intuitive sense, looking at our plots: biggest effect is syll1, and frequency also has a lot of effect adding together its contribution for different values of structural variables.

%(syll1\_coda $>$ freq $>$   syll2\_td $>$ syll2\_coda).  

% - The p-value can be used to compare the *signficance* of contributions of each variable. 
% 
% but in general these could be different.  (Also the p-value has a hypothesis test interpretation)


% - (These methods also work for linear regression models, but have to do lr tests yourself.)

% 
% - The above method is commonly used in language sciences, usually using p-value as a measure of effect size, which is technically incorrect but probably OK.

\subsubsection{Average marginal effects}
\label{sec:average-marginal-effects}

Another approach is based on how much a change in the predictor actually affects the response, with other predictors held constant---the `marginal effect' (Box~\ref{box:marginal-effects}).   The \emph{average marginal effect} (AME) is the mean of the marginal effect calculated for each observation in the dataset, and gives a sense of the average effect of the predictor.\footnote{In \citet{sonderegger2010tfs} quantifies the effect of each predictor for models of the \ttt{diatones} data using the very similar  `average predictive difference' method of
\citet[][\S21.4]{gelman2007data}.}  

% - A very common and useful approach is used in other fields (economics, other social sciences): the `average marginal effect'; similar to what Gelman and Hill call the `average predictive difference per unit'.  Based on measuring how much this variable actually affects the predicted response.

% - The idea: calculate the marginal effect of $X$, as defined above -- how much $Y$ changes for a very small change in $X$, divided by $X$, given the values of other predictors.  Formally this is the partial derivative.   Do for every observation, average -- done.


% The important \ttt{margins} package ports this implementation to R.  

The \ttt{margins} package \citep{margins,leeper2018interpreting} computes AMEs, including uncertainties, for a wide range of models.
%% Seems like TMI?
%\footnote{Correct calculation of marginal effects, especially uncertainties (confidence intervals, etc.), is very tricky, and marginal effects are more widely used in fields like political science in part because Stata is widely used for statistical analysis in these fields, and contains a good implementation. The \ttt{margins} package ports the Stata implementation to R.}  
When applied to our \ttt{mlogreg\_mod\_3}, the \ttt{margins} function from this package gives the AME for each predictor, as well as measures of uncertainty:

<<echo=1:2>>=
ame <- margins(mlogreg_mod_3)
summary(ame)
ame_summ <- summary(ame)
@

Each AME is in probability space. So for example, a unit change in frequency leads to a decrease of \Sexpr{ame_summ[1,'AME']} in probability, with 95\% confidence interval (\Sexpr{ame_summ[1,'lower']}, \Sexpr{ame_summ[1,'upper']}).
%In other words, on average there is effectively no effect of \ttt{frequency} on the likelihood of stress shifting.

We can use the absolute values of the AMEs as a measure of variable importance:
\begin{center}
\ttt{syll2\_coda} $>$ \ttt{syll1\_coda} $>$ \ttt{syll2\_td} $>$ \ttt{frequency}
\end{center}

This is very different from the ordering above: \ttt{frequency} now has the smallest effect, while the three `structure' predictors have comparable importance (taking into account their overlapping CIs).  Frequency now has the smallest effect because \textbf{on average} it has a near-zero effect---averaging across the cases where its effect is positive and negative (Figure~\ref{fig:mlogreg-eff-3}, center and right panels). This ordering is what we'd expect based on the empirical plots for each predictor in Figure~\ref{fig:diatones-empirical}---other predictors are effectively averaged over.

Which option makes more sense for assessing variable importance (hypothesis testing or average marginal effects) will depend on context. For the \ttt{diatones} data, one research question was how important a word's frequency is in determining stress shift. Rephrasing this question as ``how much does frequency help predict stress shift?'', the hypothesis testing method makes more sense. 
% 
% Note that this is pretty different from the answer above. In particular frequency has the smallest effect, because *on average* it has a near-zero effect -- averaging across one case where its effect is positive and one where its effect is negative.
% 
% Which option makes more sense will depend on the research questions. In our case, probably the first option?

\begin{boxedtext}{Broader context: AMEs as standardized regression coefficients}


The order of predictor importance we got using AMEs (syll2\_coda $>$ syll1\_coda $>$ syll2\_td $>$ frequency) is the same as what we would get from comparing the absolute value of the main effect coefficients for model \ttt{mlogreg\_mod\_3}---which is another way to assess predictor importance in models where predictors are standardized by `rescaling' (Section~\ref{sec:centering-scaling}).

In fact, calculating the AMEs in log-odds space,  we see that the AMEs are the same as the main effect coefficients for this model:
<<>>=
margins(mlogreg_mod_3, type = "link")
coefficients(mlogreg_mod_3)[2:5]
@

This is because we have standardized all variables---one motivation for standardizing was so that each main effect coefficient could be interpreted as an `average effect', averaging across other predictors.  

Calculating AMEs is still useful for a model where the predictors aren't standardized, or for a more complex model (e.g., with nonlinear effects) where standardizing isn't possible.  It is also good to know that AMEs are a way to calculate the `average effect' of predictors for \textbf{any} model---regardless of how you've coded the predictors. 
\end{boxedtext}

%What is important here is the general idea, not the particular methods used for (1), (2), and (3)---many are available.
%For (2) in 
%

%For the \ttt{diatones} example



\section{Other readings}
\label{sec:other-reading-ch6}

Many  sources
%(see e.g., Section~\ref{sec:other-reading-ch2})
cover
%and many of those written for linguists, cover
%categorical data analysis 
topics in this chapter, including detailed treatments of logistic regression. 
%(give detailed treatments of) logistic  regression (contingency tables, chi-squared tests,  Fisher exact tests, odds/odds ratios); many cover logistic regression in detail.
I have relied primarily on \citet[][chap.~1--5]{agresti2007}, \citet[][chap.\ 12]{chatterjee2012regression}, \citet[][chap.\ 1, 6]{faraway2016extending}, \citet[][chap.\ 4--5]{gelman2007data}.    \citet{chatterjee2012regression} and \citet{faraway2016extending} are particularly good on model validation. \citet{agresti2007} is an
%particularly good and 
accessible resource for categorical data analysis generally, and \citet{agresti2003categorical} is a more technical/comprehensive version.  %\citet{chatterjee2012regression}, \citet[][chap.\ 1, 6]{faraway2016extending
% 
% (e.g., those listed in Section~\ref{sec:other-reading-ch2}). Some discussion of odds and odds ratios is given by \citet{agresti2007} chap.~2, \citet{gelman2007data} 5.2, \citet{crawley2015statistics} chap.~14.
% %\citep[e.g.,][]{dalgaard2008introductory,field2012discovering,crawley2015statistics,NavarroOnline}. 
% \citet{agresti2007} is  a particularly good and accessible resource for categorical data analysis generally (chap.\ 1--2 for these topics). \citet{agresti2003categorical} is a more technical/comprehensive version.  
% 
% Some good sources for general audiences covering the same logistic regression topics in more detail:
% 
% \begin{itemize}
% \item
%   \citet{gelman2007data} chap.~4.6, 5.1-5.5 (The rest of chap.~5, on additional topics, is excellent.)
% \item
%   \citet{chatterjee2012regression} 12.1-12.5
% \item
%   \citet{agresti2007} chap.~4-5
% \end{itemize}

For language scientists specifically, there are resources focusing on specific subfields %especially focusing
%Most focus 
%on subfields 
where logistic regression has been used the longest---variationist
%the longest use: 
variationist sociolinguistics  and corpus linguistics (\citealp{tagliamonte2006analysing}; \citealp[][chap.~5]{johnson2008quantitative}; \citealp{speelman2014logistic}; \citealp[][\S5.3]{gries2021statistics})---as well as more general treatments (\citealp[][chap.\ 12]{winter2019statistics}; \citealp[][\S6.3.1]{baayen2008analyzing}; \citealp[][chap.~13]{levshina2015linguistics}).
%are more general and have useful discussion of model validation; and 
\citet{jaeger08} introduces logistic regression in comparison to ANOVAs.
%useful discussion of why logistic regression is preferable to ANOVA and  has a . There is a long tradition of logistic regression in variationist sociolinguistics \citep[e.g.,][]{tagliamonte2006analysing}; \citet[][\S5.7]{johnson2008quantitative} is an interesting discussion.

% 
% Very detailed tutorial for corpus linguistics: 
% Winter chap.~12 on logistic regression
% Gries 5.3 (log reg: 15 pages)
% 
% and psychologists specifically, we are not familiar with in-depth treatments. Some shorter ones:
% 
% \begin{itemize}
% \item
%   \citet{johnson2008quantitative} 5.4-5.7 (5.7 is interesting if you are familiar with GoldVarb from sociolinguistics)
% \item
%   \citet{jaeger08}
% \item
%   \citet{baayen2008analyzing} 6.3.1
% \end{itemize}
% 
% adjust / remove, just say exactly where diagnostics are --- include faraway We will only briefly discuss Log reg diagnostics here, but you should read more (e.g.,~\citet{chatterjee2012regression}, \citet{baayen2008analyzing}, \citet{gelman2007data}) if you are using logistic regression to analyze data.


% 
% \hypertarget{c4solns}{%
% \section{Solutions}\label{c4solns}}
% 
% \textbf{Q}: What are the interpretations of the regression coefficients?
% * Intercept (\(\beta_0\))
% * Slope (\(\beta_1\))
% 
% \textbf{A}: \(\beta_0\) is the probability of a stress shift when the NP type is `Full NP'. \(\beta_1\) is the change in probability between when the NP type is `Full NP' or `Pronoun'.
% 
% \hypertarget{c4appendix2}{%
% \section{Appendix: Other Generalized Linear Models}\label{c4appendix2}}


\section{Exercises}

\exer{\label{ex:cramer}
Calculate Cramer's $V$ for the \ttt{diatones} and \ttt{regularity} examples in Section~\ref{sec:cda-effect-size}.  Hint: you can use \ttt{chisq.test()} with argument \ttt{correct = FALSE} to compute $\chi^2$.
}

\exer{\label{ex:mlogreg-prob} For the model \ttt{mlogreg\_mod\_1} in Section~\ref{sec:mlogreg-mod-1}, we interpreted the \ttt{syll1\_coda} and \ttt{frequency} coefficients in terms of odds.  What is the interpretation of each coefficient in terms of {probability}, calculated in one of the ways discussed in the text (e.g., average marginal effects)?}
% 
% This corresponds to a  maximum change of about 30\% in probability (divide-by-4 rule: \Sexpr{mlogreg_mod_1_c1}/4 = \Sexpr{mlogreg_mod_1_c1/4}), or a change of 19\% in probability at the intercept.
% %predict(logreg_mod_3, data.frame(syll1_coda=c(-0.22, 0.77), syll2_coda=0, syll2_td=0, frequency=0))
% Frequency term:
%  This corresponds to a maximum change in probability of about 15\%, or a change of 7\% in probability at the intercept. 
% % predict(logreg_mod_3, data.frame(frequency=c(-0.5,0.5), syll2_coda=0, syll2_td=0, syll1_coda=0)) %>% invlogit()


\exer{The  model \ttt{mlogreg\_mod\_1} will predict the log-odds of stress shifting by the year 2000 for any English diatone.  Suppose that `disfug' were an English diatone with average \ttt{frequency}, and \ttt{syll1\_coda} = \ttt{yes}, \ttt{syll2\_coda} = \ttt{C}, \ttt{syll2\_td} = \ttt{no}.  What is the predicted probability of stress shifting by 2000 for this word? (Hint: to use \ttt{predict}, you'll need to determine what each predictor's value is for the standardized versions used to fit the model.)}

\exer{\label{ex:model2-validation} Additional model validation for  \ttt{mlogreg\_mod\_2}}

\subexer{Assess whether the assumption of a linear effect of \ttt{frequency} is appropriate, using one or more diagnostic plots.}

\subexer{Assess the degree of collinearity in the data this model is fitted to, using diagnostics from Section~\ref{sec:collinearity-diagnostics}.}

\subexer{In Section~\ref{sec:log-reg-prob-obs} we saw there are some potentially influential points for this model. Check what happens to AUC and to the qualitative conclusions we can make from the model when these points are excluded. Is the model robust to these observations?}

\exer{Carry out the full set of model comparisons and effect size computations referred to in Section~\ref{sec:variable-importance-complex}, resulting in four significances and effect sizes, and verify that they give the same order of importance for the four predictors.
\label{ex:var-imp-diatones} 
}




%% FUTURE: possible harder exercise -- there is in fact inherent collinearity here, from the way the predictors are defined. where? what effect might this have on the results?  (A: when syll2_coda is minimum syll2_td can only be 0.  not sure about possible effect..)
