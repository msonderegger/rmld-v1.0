% !Rnw root = master.Rnw


<<cache=FALSE, echo=FALSE>>=
## trying this out to use num signif digits (which is what
## tidy print does by default) instead of decimal
options(knitr.digits.signif = TRUE)
options(digits = 2)
@

\chapter{Linear mixed-effects models} 
\label{chap:lmm-1}

This chapter introduces regression models for clustered data,  focusing on conceptual understanding, fitting, and interpretation of basic linear mixed-effects models.  The next chapter extends this discussion to mixed-effects logistic regression. These chapters mostly assume the most common use case, where population-level effects (`fixed effects') are of primary interest. We assume that the set of terms to include in the models are known in advance and that there are no model fitting problems.
%there are no problems with fitting a valid model (`convergence').
Chapter~\ref{chap:mem-3} considers model selection, model convergence, and other `advanced' topics which come up frequently in practice.

% Similarly to our discussion of linear regression in Chapter~\ref{chap:linear-regression-1}, Chapters \ref{chap:lmm-1}--\ref{chap:mem-2} introduce mixed-effects regression models assuming that all assumptions of these models are met, and the set of terms to include in the model are known in advance. In Chapter~\ref{chap:mem-3} (analogous to Chapter~\ref{chap:linear-regression-2} for linear regression) we consider model validation, model selection, and other `advanced' topics which come up frequently in practice (e.g.,\ model convergence). 

\section{Preliminaries}
\label{sec:lmm-prelim}

\subsection{Packages}

This chapter assumes that you have loaded several packages from previous chapters, as well as the {lme4} package for fitting mixed-effects models \citep{bates2015fitting} and various packages for working with lme4 models ({broom.mixed}, {MuMIn}, {pbkrtest}, {RLRsim}, {lmerTest}: \citealp{broom.mixed,MuMIn,pbkrtest,scheipl2008size,lmerTest}):

<<cache=FALSE>>=
library(tidyverse)
library(broom)
library(arm)
library(lme4)
library(car)
library(broom.mixed)
library(MuMIn)
library(pbkrtest)
library(RLRsim)
library(lmerTest)
## makes `lmer` be lme4, rather than lmerTest version
lmer <- lme4::lmer
@

The most important package here is {lme4}, for fitting mixed-effects models.  We are using version \Sexpr{packageVersion("lme4")}, but any version after 1.0, when major changes were made, should be fine.

\subsection{Data}

We also assume you have set the default contrasts for factors to Helmert contrasts (Section~\ref{sec:factors-discussion}), and loaded the \ttt{neutralization} and \ttt{vot}  datasets (Section~\ref{sec:neutralization-data}, \ref{sec:vot-dataset}), followed by some processing done in previous chapters:

<<cache=FALSE>>=
options(contrasts = c("contr.helmert", "contr.poly"))

neutralization <- read.csv("data/neutralization_rmld.csv", 
  stringsAsFactors = TRUE) %>%
  ## Factor version of voicing, with voiceless < voiced
  mutate(voicing_fact = fct_relevel(voicing, "voiceless")) %>%
  filter(!is.na(prosodic_boundary))

vot <- read.csv("data/vot_rmld.csv", stringsAsFactors = TRUE) %>%
  # Relevel place to be labial < alveolar < velar
  mutate(place = fct_relevel(place, "labial"))
@

% 
% We also drop a row from \ttt{neutralization} with missing data for a control predictor we will use (\ttt{prosodic\_boundary}):
% 
% <<>>=
% neutralization <- neutralization %>% filter(!is.na(prosodic_boundary))
% @

Finally, for both datasets we standardize predictors we will use in models (by `rescaling': Section~\ref{sec:centering-scaling}):

<<>>=
## Two-level factors: change to 0/1 then center
## Continuous predictors: center and divide by 2 SD
neutralization <-
  mutate(neutralization,
    prosodic_boundary = rescale(prosodic_boundary),
    ## voiced > voiceless
    voicing = rescale(voicing_fact)
  )

vot <- vot %>% mutate(
  log_corpus_freq = rescale(log_corpus_freq),
  speaking_rate_dev = rescale(speaking_rate_dev),
  foll_high_vowel = rescale(foll_high_vowel),
  cons_cluster = rescale(cons_cluster),
  gender = rescale(gender)
)
@


\section{Motivation: Grouped data}
\label{sec:motivation-grouped}

% FUTURE: can this whole section be shortened, given previous discussion in several places:
% Section~\ref{sec:lin-reg-indep}, \ref{sec:t-test-indep-assumptions}, Box~\ref{box:pseudoreplication})?  Compressed some (1/21), but more seems possible.

Previously, in the context of $t$-tests and linear regression, (Section~\ref{sec:t-test-indep-assumptions},~\ref{sec:lin-reg-indep}), we argued that independence assumptions are the most important assumption underlying analysis of linguistic data.
%generally.
%(see also \citealp[][\S14.2]{winter2019statistics}).
This is because most linguistic data has grouping structure---by participants, items, words, and so on---which immediately implies non-independence of observations.  Grouping by one or more \emph{grouping factors} is the norm in linguistic data---whether
%analyzed in many fields, including language sciences---whether 
from a laboratory experiment, a speech or text corpus, or the output of a computational experiment. There are some exceptions (e.g.,\ lexical data, as in the \ttt{regularity} and \ttt{diatones} datasets), but they prove the rule.
%and in statistical analysis
%To understand why, it is important that in most statistical analysis of linguistic data, the 
%let's take a step back and
%consider our general goal in statistical analysis of linguistic data---to make generalizations about units in a population (e.g.,\ words, people), based on finite data.  Typically 
%the 
%units over which we want to generalize are not individual observations, but 
%we are trying to generalize about some higher-level grouping, such as participants, items, or words---representing `all speakers of a language', etc.  There are typically multiple observations per unit level, such as all a participant's observations from one experiment, or all observations of the same word in one corpus. 
%Making
%Even when we want to make a generalization about observations (e.g.,\ the effect of speaking rate), doing so
%requires taking into account how the data is grouped.   
% 
%
Analyzing such data requires taking grouping structure into account, because
%The independence assumption is so important because grouping structure is ubiquitous in linguistic data, and
observations from the same unit are typically not independent. For example, in a study of lexical decision reaction time as a function of participant age and word frequency, certain participants will be characteristically fast, compared to other participants of the same age (they may have slept more, etc.),
 and certain
% may be more motivated, etc.)
%to finish the experiment
%they just had coffee, are highly 
%motivated to finish the experiment, or some other reason. 
%Similarly, 
%errors won't be independent from multiple observations of the same word. 
certain words will take longer to recognize, beyond the effect of word frequency (e.g., %such as %(orthographically) longer wordsor
orthographically longer words).

One way to analyze such data
%To analyze such data, one option 
is to average over observations to remove grouping structure (as in the \ttt{english} dataset); another is to analyze a subset where independence holds (as we did for the \ttt{vot\_michael} dataset). Neither is a good option, at best resulting in a loss of power.  

Analyzing grouped data without taking grouping into account, or `pseudoreplication' (Box~\ref{box:pseudoreplication}), can easily result in Type I or II errors,
as illustrated in 
%(spurious effects), or Type II errors (failing to detect a real effect).  
Section~\ref{sec:t-test-indep-assumptions} 
for the simplest case,  $t$-tests (does $y$ differ between two groups)? 
%the showed two examples of how this can happen, for the simplest possible case (does $y$ differ between two groups?) using  the \ttt{neutralization} data, which is  grouped by \ttt{subject} and by \ttt{item\_pair}. 
%Two-sample $t$-tests found that the first and second 50\% of participants in the experiment behaved differently (Type I error), and that the primary effect of interest (whether \ttt{voicing} affects \ttt{vowel\_dur}) was not significant for one subject (Type II error).  
These errors resulted from the mismatch between the data's structure and assumptions of the analysis method, and
analogous errors are a danger for \textbf{any analysis of grouped data} (linear regression, logistic regression, ANOVAs, etc.) not taking grouping structure into account.
Pseudoreplication can also result in errors in the magnitude or sign of an effect (Type M, S errors: Section~\ref{sec:type-m-s-error}), such as \emph{Simpson's paradox}, where an effect (e.g.,\ positive) observed across groups is reversed (e.g.,\ negative) or disappears once grouping structure is taken into account.\footnote{ \citet{jaeger2011mixed} (Figure\ 2) gives an elegant example from language typology.}
% %using simulated data, 
% where opposing answers the research question are reached
% %`how is geographic distance between languages related to their phonological diversity?' are reached,
% depending on whether grouping by language family is taken into account.}
%The key plot, which we assume you can see, is reproduced here: \url{https://hlplab.wordpress.com/2011/05/31/mixed-models-and-simpsons-paradox/}.}
% the relationship between a language's geographic distance from a fixed point (in Africa: $x$) and a measure of phonological diversity ($y$), across many languages, which are grouped by language family Across languages, $x$ looks negatively correlated with $y$,  but once grouping structure is taken into account, it is clear that $x$ we can see that the $x$ and $y$ are actually positively correlated, within language families. (In addition, there is a negative correlation  between a family's mean $x$ and mean $y.) Thus, we would make the opposite conclusion without taking grouping structure into account. 


One method for analyzing grouped data  is \emph{repeated measures ANOVAs}, which were the norm in many subfields (e.g.,\ psycholinguistics) 
%for analyzing data
until the early 2010s, and are still widely used.  It is useful to know about them (Box~\ref{box:repeated-measures}),
%% FUTURE: see old 7.1, is there anything left from there to incorporate?
and they are a fine analysis method for data from relatively simple experiments with balanced designs, but they have important limitations that do not make them a good default method. 
% (Similarly, paired $t$-tests can account for grouped data in very simple cases.)

\emph{Mixed-effects} regression models, or \emph{mixed models} (a.k.a.\ multilevel models), are another way of analyzing grouped data, which have become the standard for analyzing grouped data in much of the language sciences, as well as other fields.
%\citep[e.g.,][400]{mcelreath2020statistical}.


Mixed models have several advantages over other analysis methods for grouped data, which mirror the structure of linguistic data, making them a good default:
\begin{itemize}
\item \textbf{Multiple grouping factors} (e.g.,\ participant, item) can be included in the same model.
\item \textbf{Unbalanced data} is assumed (unequal number of observations per level).
\item \textbf{Explicit modeling of variability} among levels of a grouping factor, such as by-participant differences, allowing the analyst to explore and test questions about the variability itself.
\end{itemize}

\paragraph{Practical notes}

%% FUTURE: JPK - note that most import functions, whether tidyverse or read.csv/read.table, now default to stringsAsFactors = FALSE, so actually nothing will be imported as a factor.
Before fitting any mixed model, it is important to check that the variables to be used as grouping factors are indeed coded as factors.  For example, participant and item IDs are often numeric in data from laboratory experiments such as \ttt{neutralization}:

<<output.lines=3>>=
select(neutralization, subject, item_pair) %>% head()
@
<<>>=
# Recode subject and item_pair as factors
neutralization <- mutate(neutralization,
  item_pair = as.factor(item_pair),
  subject = as.factor(subject)
)
@

Coding these variables as factors ensures that functions for fitting and interpreting mixed-effects models will work properly.
%\footnote{While the actual {lme4} functions for fitting mixed models will code grouping factors as factors for you, functionality from many packages won't work correctly, e.g.,\  computing partial effects for individual participants.} 
We also do this for the relevant variables (\ttt{speaker}, \ttt{word}) for the \ttt{vot} data for good measure. Depending on how the data is loaded or subsequently processed, these may be coded as characters rather than factors:\footnote{For example, all tidyverse functions default to characters rather than factors.}

<<>>=
vot <- mutate(vot,
  word = as.factor(word),
  speaker = as.factor(speaker)
)
@

We also explicitly set the contrasts for factors which will be predictors in our models:
<<>>=
contrasts(neutralization$voicing_fact) <- contr.helmert(2)
contrasts(neutralization$place) <- contr.helmert(3)
contrasts(neutralization$vowel) <- contr.helmert(5)

contrasts(vot$place) <- contr.helmert(3)
@

This is necessary  in order for model predictions to work correctly in all cases, even though we have set unordered factors to default to Helmert contrasts (above).
Rather than go into the underlying reason, which is very technical,
%(and perhaps even a bug, to be fixed by the time you read this),
it is easiest to just remember the rule, \textbf{always explicitly specify contrasts for predictors in your mixed-effects models} to be safe.

\section{Linear mixed models: Introduction}
\label{sec:lmm-intro}

We will introduce 
%each part of
mixed models through a series of examples.  We start with the simplest case, where there is a single grouping factor and variability between groups just in the value of the intercept, to introduce fitting and interpreting a mixed model (Section~\ref{sec:fitting-interpreting-model}--\ref{sec:pred-rand-eff}), then turn to multiple grouping factors (Section~\ref{sec:multiply-grouped-data}) and variability between groups in the effects of predictors (random slopes: Section~\ref{sec:random-slopes}).

Consider the \ttt{neutralization} data for a single participant:

<<>>=
neutralization_2 <- filter(neutralization, subject == 2)
@

<<echo=FALSE>>=
n_neut_item <- nrow(distinct(neutralization, item_pair))
n_neut_sub <- nrow(distinct(neutralization, subject))
n_neut <- nrow(neutralization)
@


This data is grouped by item (grouping factor = \ttt{item\_pair}): there are \Sexpr{n_neut_item}  pairs of words, each differing only in the final consonant's voicing (see Section~\ref{sec:neutralization-data}).  We consider just this predictor ($x$ = \ttt{voicing}, values -0.5 and 0.5, corresponding to \tsc{voiceless}/\tsc{voiced} consonants) for simplicity; of interest is its effect on \ttt{vowel\_dur} ($y$).




\subsection{Simple linear regression}

A simple linear regression model for this case, {without} accounting for by-item variability, would be (equation~\ref{eq:linreg2}):
\begin{align}
  y_i & = \beta_0 + \beta_1 x_i + \epsilon_i \nonumber \\
  \epsilon_i & \sim N(0, \sigma), \quad \text{for } i = 1, \ldots, n
  \label{eq:lmm-linreg2}
\end{align}

This model assumes the same intercept ($\beta_0$) and slope ($\beta_1$) for all groups (i.e.,\ all items), as schematized in Figure~\ref{fig:neut-2-preds} (left), which shows the predictions of the model fitted in R:

<<>>=
neut_2_mod_0 <- lm(vowel_dur ~ voicing, data = neutralization_2)
@


<<neut-2-preds, echo=FALSE,  out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Predicted effect of \\ttt{voicing} for two models of the \\ttt{neutralization\\_2} data: a simple linear regression (left), and different items in a linear mixed model containing a by-item random intercept (right).'>>=
ggplot(aes(x = voicing_fact, y = vowel_dur), data = neutralization_2) +
  geom_jitter(width = 0.2, size = 0.5, alpha = 0.5) +
  stat_summary(fun = "mean", geom = "line", aes(group = 1), size=0.75) +
  xlab("Voicing") +
  ylab("Vowel duration") +
  ggtitle("Fixed effect")

n_sub_mod_1 <- lmer(vowel_dur ~ voicing_fact + (1 | item_pair), neutralization_2)

pred_df <- tidyr::expand(neutralization_2, voicing_fact, item_pair)
pred_df$pred <- predict(n_sub_mod_1, newdata = pred_df)

pred_df %>% ggplot(aes(x = voicing_fact, y = pred, group = item_pair)) +
  geom_line() +
  geom_jitter(aes(x = voicing_fact, y = vowel_dur, group = 1), width = 0.2, size = 0.5, alpha = 0.2, data = neutralization_2) +
  xlab("Voicing") +
  ylab("Vowel duration") +
  ggtitle("Fixed effect and\nrandom intercept")
@

The same \ttt{vowel\_dur} value is predicted for all observations with the same \ttt{voicing} value, regardless of which item an observation comes from.
Since the data is grouped by item, this model violates the independent error assumption of linear regression.   Items might differ in:
\begin{enumerate}
\item their mean \ttt{vowel\_dur}, or 
\item the effect of \ttt{voicing}. 
\end{enumerate}

Our goal in fitting the model is to assess whether there is an effect of \ttt{voicing}, beyond by-item variability.   

Specifically, based on this model we would conclude that the voiced/voiceless difference does not significantly differ from 0:

<<>>=
tidy(neut_2_mod_0)
@

So failing to account for (1)--(2) could have resulted in a Type II error (if there is in reality a voiced/voiceless difference).

Mixed models deal with non-independence of errors by using two types of coefficients: \emph{random effects}  are coefficients which vary between groups, while \emph{fixed effects} are coefficients which do not vary between groups.\footnote{We use `fixed effect' and `random effect' because this is standard terminology across language sciences.       \citet[][\S11.4]{gelman2007data} discuss problems with these terms (inconsistent and ambiguous definitions across fields),   and suggest more precise `constant'/`varying' terminology (e.g.,\ `varying intercepts', `varying slopes'), which is non-standard but  used by some R packages. See Box~\ref{box:varying-coeffs}.}

Fixed effects are also called \emph{population-level} or `average' effects, since they can be  thought of as the effect averaging across groups; these are often of primary interest.  The \emph{group-level} variability captured by random effects can be thought of either ``as error variance that must be controlled [to correctly interpret fixed effects], or as phenomena of scientific interest in their own right'' \citep{meteyard2020best}. `Random' refers to the  assumption that levels of the grouping factor are randomly sampled from the population we want to generalize across. For example, in modeling the \ttt{neutralization\_2} data below, we are assuming that the items (= word pairs differing only in final consonant voicing) seen by participant 2 are randomly sampled from all such possible word pairs in German. 



% () tefixed and random effects used inconsistently across fields and conflate the definition of each kind of coefficient with the motivation for using it (e.g.,\ fixed effects are `of interest' while random effects are not). Their `constant' and `varying' terminology (e.g., `varying intercept') is better, and is increasingly used in practice (e.g., REF McElreath), but the fixed/random terminology is standard
% 
% \begin{enumerate}
% \item
%   \emph{Random effects}: Coefficients which vary between groups
% \item
%   \emph{Fixed effects}: Coefficients which don't vary between groups
% \end{enumerate}

Fixed effects are essentially what we have referred to as `regression coefficients' in the linear and logistic regression models covered in previous chapters.  Random effects can model by-group variability in the \textbf{intercept} or predictor \textbf{slopes} (as in (1), (2)); these two kinds are introduced in the next models we consider. 

\subsection{One grouping factor, random intercept}

Let $J$ be the number of groups indexed by the grouping factor, and $j[i]$ be the group of the $i$th observation. For example, for the \ttt{neutralization\_2} data $J=\Sexpr{n_neut_item}$ (\Sexpr{n_neut_item} item pairs) and $j[1] = \Sexpr{as.character(neutralization_2[1, 'item_pair'])}$ (observation 1 is from item \Sexpr{as.character(neutralization_2[1, 'item_pair'])}).

The regression model for a simple linear regression where the intercept varies by group is:
\begin{align}
  y_i & = \beta_0 + \alpha_{j[i]} + \beta_1 x_i + \epsilon_i \nonumber \\
  \epsilon_i & \sim N(0, \sigma_E^2), \quad \text{for } i = 1, \ldots, n \nonumber \\
  \alpha_j & \sim N(0, \sigma_I^2) , \quad \text{for } j = 1, \ldots, J
  \label{eq:lmm1}
\end{align}

The first line looks like simple linear regression (equation~\ref{eq:lmm-linreg2}), except for the \emph{random intercept}
\(\alpha_{j[i]}\), which captures the offset of group \(j[i]\) from the population-level intercept (\(\beta_0\)).   The random intercept values for different items are assumed to be normally distributed, where \(\sigma_I\) captures the amount of between-item variation in the intercept value.

The errors \(\epsilon_i\) are still assumed to be normally distributed, as in the simple linear regression. We now write the error variance as $\sigma_E$
(instead of \(\sigma\)), to distinguish it from \(\sigma_I\).

This model makes predictions shown in Figure~\ref{fig:neut-2-preds} (right): one line per item, differing in intercept but not in slope.  (Section~\ref{sec:pred-rand-eff} shows how these predictions are calculated.)

In other words, the regression describes the relationship between $y$ and $x$ at several `levels'.   A different relationship is predicted for \textbf{each item}, for example for item 10:
$$
y = \beta_0 + \alpha_{10} + \beta_1 x,
$$
where \((\beta_0 + \alpha_{10})\) is the value of the intercept for item 10.  

The model for an \textbf{average item} is:
$$
y = \beta_0 + \beta_1 x 
$$
Here, the intercept is $\beta_0$, the population-level effect (averaging across items). The model for an average item captures the relationship in Figure~\ref{fig:neut-2-preds} (left), identically to the simple linear regression without random effects (equation~\ref{eq:lmm-linreg2}).

So the mixed-effects model can be thought of as a generalization of simple linear regression,  modeling both the `average' relationship and how it varies across groups.

\subsection{Fitting and interpreting the model}
\label{sec:fitting-interpreting-model}

Fitting a mixed-effects model requires estimating the fixed-effect coefficients ($\beta_0$, $\beta_1$), and the \emph{variance components} describing the degree of different kinds of variability ($\sigma_I^2$, $\sigma_E^2$).  (The random-effect coefficients $\alpha_j$ are not directly estimated by the model, a point we return to below.)

We use the {lme4} package  for fitting (\textbf{l}inear) \textbf{m}ixed \textbf{e}ffects models, which is the most widely-used R package for such models.  Functions in this package fit mixed models by one of two methods: \emph{maximum  likelihood} (ML), which tends to underestimate the extent of variability, giving biased variance component estimates, or \emph{restricted maximum likelihood} (REML: \citealp[][\S2.2.5]{pinheiro2000mixed}), which gives unbiased variance component estimates, and is typically the default.
The difference between ML and REML only matters in practice
% for small sample sizes and 
when doing model comparison (Section~\ref{sec:lmm-ht}).  

In either case, {lme4} uses an optimization algorithm to find a model that maximizes the likelihood.  Model-fitting problems---where the algorithm does not converge, or gives a model with unreasonable parameter estimates (`singular': Section~\ref{sec:singular-models})---are fairly common in practice.   We discuss these in more depth in Section~\ref{sec:lmm-convergence}, and mostly abstract away from them in this and the next chapter by fitting convergent models and ignoring singular model warnings. 

% We 
% 
% is seeking the model $\hat{M}$ which maximizes a likelihood function: the standard log-likelihood (ML), or an adjusted version (REML) (see e.g., \citealp[][\S3.4]{bates2015fitting}; \citealp{bolker2015linear}).  This likelihood function is non-trivial, and finding $\hat{M}$ requires using a numerical optimizer, which iteratively adjusts the model to be closer to $\hat{M}$ until (1) it \textbf{converges}---intuitively, when the difference between models in successive iterations is very small---and (2) the final model has reasonable parameter estimates 

% . Model-fitting issues related to (1) and (2) are fairly common when using {lme4} in practice. We discuss these in more depth in Section~\ref{sec:lmm-convergence}, and mostly abstract away from them in this and the next chapter by fitting convergent models and ignoring singular model warnings. 

% There are other important practical aspects of {lme4} to know about if you use it frequently (Box~\ref{box:lme4-details}).

% \begin{boxedtext}{Broader context: The lme4 package}
% \label{box:lme4-details}
% 
% The {lme4} package is so widely used in language sciences that `fitting a mixed model' is practically synonymous with `using {lme4}',  and some details of this particular implementation of mixed-effects models are worth knowing.  
% 
% {lme4} is frequently updated, and versions before versus after v.\ 1.0 are particularly different. Thus, it is important to cite the exact version of {lme4} used when reporting a mixed-effects model in a paper.
% 
% \ttt{lmer()} (and \ttt{glmer()}) models fit quickly for small datasets and simple model structures (seconds), but much longer for more complex data/models (hours). Verbose output (setting \ttt{verbose=1}) is useful to get a sense of how far along your model is.
% 
% Further readings, including on mixed models more generally, are in Section~\ref{sec:other-reading-ch8}. 
% 
% \end{boxedtext}

We fit the mixed model described above using the \ttt{lmer()} function:
%(from the {lme4} package):

<<>>=
neut_2_mod_1 <- lmer(vowel_dur ~ voicing + (1 | item_pair), 
  neutralization_2)
@

Here, \ttt{(1|item\_pair)} is {lme4} notation for a by-item random intercept (\texttt{1} means `intercept', \ttt{|item\_pair} means  `grouped by item').

% \textbf{Note}: You do \textbf{not} need to write \texttt{lme4::lmer} in general, just \texttt{lmer}. The notation \texttt{lme4::lmer} is used here to make sure we use the \texttt{lmer} function from the {lme4} package, rather than the redefined version of \texttt{lmer} from the {lmerTest} package discussed in Section \ref{c6sattapprox}.

This model's output is:

<<>>=
summary(neut_2_mod_1)
@


<<echo=FALSE>>=
neut_2_sig_i <- extractRanefEst(neut_2_mod_1, 'item_pair', 'sd__(Intercept)')^2
neut_2_sig_e <- extractRanefEst(neut_2_mod_1, 'Residual', 'sd__Observation')^2
@

The model has estimated the fixed-effect coefficients (under \ttt{Fixed effects}) and the variance components (under \ttt{Random effects}).   The estimated intercept is $\hat{\beta}_0 = \Sexpr{fixef(neut_2_mod_1)[[1]]}$, the slope of \ttt{voicing} is $\hat{\beta}_1 = \Sexpr{fixef(neut_2_mod_1)[[2]]}$, the estimated degree of by-participant variability in the intercept is $\hat{\sigma}^2_i = \Sexpr{neut_2_sig_i}$, and the estimated residual error is $\hat{\sigma}^2_e = \Sexpr{neut_2_sig_e}$.

Typically the information outside of these two blocks is not of interest, and we suppress it in the output. \ttt{Correlation of Fixed Effects} are the estimated correlations between fixed-effect estimates---analogous to the correlations between coefficient estimates for a linear regression (Section~\ref{sec:credit-assignment-ex}).

The fixed effects coefficients mean that the model predicts the relationship for an `average item' to be:
$$
\texttt{vowel\_dur} = \Sexpr{fixef(neut_2_mod_1)[[1]]} + \Sexpr{fixef(neut_2_mod_1)[[2]]} \cdot \ttt{voicing}
$$

There are no $p$-values, for reasons discussed below, but $|t|>2$ can be used as a rough rule of thumb for which fixed effects are `significant at the $\alpha=0.05$ level (Section~\ref{sec:wald-z-test}). Alternatively we can compute 95\% confidence intervals for the model parameters:

<<>>=
## 'parm' argument gives CIs only for fixed effects
confint(neut_2_mod_1, parm = "beta_")
@

By either method there is a `real' \ttt{voicing} effect in this model but not in the simple linear regression, suggesting that we made a Type II error using the latter.

Note that the estimated intercept and slope (of \ttt{voicing}) are almost exactly the same in the simple linear regression:

<<>>=
tidy(neut_2_mod_0) %>% select(estimate, std.error)
@

This makes sense, as both models are predicting the `average' effect. However the standard errors differ.   The slope estimate is {more} certain in the mixed-effects model because the effect is clearer once by-item variability in (average) \ttt{vowel\_dur} has been taken into account. This is very similar to what happens when a paired two-sample $t$-test is used instead of an unpaired test, as in Section~\ref{sec:type-ii-error} for the same data.

%% FUTURE: turn this into exercise
% The intercept estimate is less certain in the mixed-effects model because it knows that the mean being estimated is not from 48 independent observations, but something closer to 24 pieces of information (the number of items). That is, the degrees of freedom for this estimate ($n$ in $SE = \sigma/\sqrt{n}$) is somewhere between 24 and 48.   


% In general in a mixed model, taking varonce variability between groups in $x   This is a general characteristic of mixed-effects models: population estimates (the fixed effects) lie conceptually between two models \textbf{without} random effects: one where every group's effect is estimated independently (e.g.,\ as a 23-level factor) and one where a single mean is estimated (as in model \ttt{neut\_2\_mod\_0}). (REFS)

Note that mixed models 
%\begin{boxedtext}{Practical note: Tidy functions for mixed models}
can also be summarized and processed using the same tidy functions as used previously (now from the {broom.mixed} package): \ttt{tidy()} for cleaner model output, \ttt{glance()}, \ttt{augment()}, etc.:
%to compute model summaries, and \ttt{augment} to add information from the fitted model to the original dataframe, e.g.,

<<>>=
tidy(neut_2_mod_1)
@

%\paragraph{Interpretation of random effects}


<<echo=FALSE>>=

## extractRanefEst defined in _common.R
beta0_est <- fixef(neut_2_mod_1)[["(Intercept)"]]
sig_i_est <- extractRanefEst(neut_2_mod_1, "item_pair", "sd__(Intercept)")

sig_e_est <- extractRanefEst(neut_2_mod_1, "Residual", "sd__Observation")
@

To get a sense of what $\hat{\sigma}_I=\Sexpr{sig_i_est}$ means, note that because the random intercepts (the $\alpha_j$) are normally distributed, we expect roughly 95\% of items to have intercepts within $2 \sigma_I$ of the population-level intercept, $\beta_0$.   Thus, the model predicts that 95\% of items in the \textbf{population} have intercepts between \Sexpr{beta0_est-2*sig_i_est}  and \Sexpr{beta0_est+2*sig_i_est} msec:
\begin{align*}
\text{lower bound:} \quad & \hat{\beta}_0 - 2 \cdot \hat{\sigma}_I
= \Sexpr{beta0_est} - 2 \cdot \Sexpr{sig_i_est} =
\Sexpr{beta0_est-2*sig_i_est}
\\
\text{upper bound:} \quad & \hat{\beta}_0 + 2 \cdot \hat{\sigma}_I =
\Sexpr{beta0_est + 2*sig_i_est}
\end{align*}


% (In addition, \(\approx\) 95\% of observations are predicted to have an error between -1.5 and 1.5, \(=\pm 2 \cdot \hat{\sigma}_e\).)



\subsection{Predicted random effects}
\label{sec:pred-rand-eff}

It is often of interest to extract the \emph{predicted random effects}, for example to plot model predictions for individual items.  In the current example this would be  $\hat{\alpha}_1, \ldots, \hat{\alpha}_{24}$, the predicted offset of each item's intercept from the population value. These values, often called `BLUPs' (Box~\ref{box:blups}), can be extracted using   the \ttt{ranef} function:

<<output.lines=1:4>>=
ranef(neut_2_mod_1)$item_pair
@

Thus $\hat{\alpha}_1 = \Sexpr{ranef(neut_2_mod_1)$item_pair[1,1]}$, and so on.  It can be useful to instead use the predicted coefficient values for each level (which here would be $\hat{\beta_0} + \hat{\alpha}_1$ for item 1, and so on) using e.g.,\ the \ttt{coefficients} function:

<<output.lines=1:4>>=
coefficients(neut_2_mod_1)$item_pair
@

Here we see explicitly that the intercept varies by item (e.g.,\ item 1's predicted intercept is \Sexpr{coefficients(neut_2_mod_1)$item_pair[1,1]} msec), while the \ttt{voicing} slope does not.  


\begin{boxedtext}{Broader context: Predicted random effects}
\label{box:blups}

It is not immediately clear what  `predicted random effects' means, because the random effects (e.g., $\alpha_j$) are not parameters estimated by the model (as opposed to the {variances} of the random effects, which are). 

Because these are so often of interest for model interpretation, it is standard to {define} the `predicted random effects' to be the most likely values of the random effects when the fixed-effect coefficients are held at their estimated values (here, $\hat{\beta}_0$ and $\hat{\beta}_1$); these are called the BLUPs (`best linear unbiased predictors') or `conditional modes' \citep[e.g.,][\S5.1]{bates2015fitting}.  These are part of fitted {lme4} models, along with approximate standard errors, which  can be extracted with \ttt{ranef()}, or \ttt{tidy()} (from  {broom.mixed}):


<<output.lines=1:4>>=
tidy(neut_2_mod_1, effects = "ran_vals")
@



\end{boxedtext}


The predicted random effects are used to make model predictions (e.g.,\ using equation~\ref{eq:lmm2} for our example) by other functions. For example, \ttt{fortify.merMod()} (from {lme4}) adds the model's prediction to the original dataframe (as column \ttt{.fitted}):

<<eval=FALSE>>=
fortify.merMod(neut_2_mod_1)
@

And here \ttt{predict()} is used to get predicted values of \ttt{vowel\_dur} for each item for both \ttt{voicing} values, shown in Figure~\ref{fig:neut-2-preds} (right):

<<eval=FALSE>>=
## Define the prediction dataframe
pred_df <- neutralization %>% expand(voicing, item_pair)
## tidyr::expand: make dataframe with all combinations of variables

# Get predicted values
pred_df$pred <- predict(neut_2_mod_1, newdata = pred_df)
@

%% FUTURE: maybe just say see model predictions section?
<<emp-pred-comp, echo=FALSE, out.width='50%', fig.width=default_fig.width*.5/default_out.width, fig.cap='Example of shrinkage: empirical means of \\ttt{vowel\\_dur} versus model predictions (from model \\ttt{neut\\_2\\_mod\\_1}), for each item in the \\ttt{neutralization\\_2} data.'>>=
plot_df <- neutralization_2 %>%
  group_by(item_pair) %>%
  summarise(mean_dur = mean(vowel_dur)) %>% # empirical
  mutate(
    pred_dur =
      coefficients(neut_2_mod_1)$item_pair[, 1]
  ) # predicted

plot_df %>% ggplot(aes(x = mean_dur, y = pred_dur)) +
  geom_point() +
  geom_abline(lty = 2) +
  xlab("Item's empirical mean") +
  ylab("Item's predicted\nintercept")
@


\begin{boxedtext}{Broader context: Shrinkage and partial pooling}
\label{box:shrinkage-i}

To get a sense of what the random-intercept model is doing, we can compare the predicted intercept for each item ($\hat{\beta}_0$ + $\hat{\alpha}_j$ for item $j$) with the empirical mean for each item (by-item average \ttt{vowel\_dur}), shown in Figure~\ref{fig:emp-pred-comp}.

Each item's predicted intercept is closer to the overall mean (of \ttt{vowel\_dur}) than the empirical values, a phenomenon called \emph{shrinkage}, because the (absolute values of the) random intercepts are `shrunk' from the empirical means towards the population-level estimate (the fixed effect). 

%This is a very general property of all random effects, which improves the model's estimates.
%(Box~\ref{box:shrinkage-i}).

Shrinkage is an important property of all random effects, which improves estimates of the degree of variability (here, $\hat{\sigma}_I$) and thus generalization of the model to new data (here, items).  Surprisingly, shrinkage also makes the BLUPs a better estimate of each item's (true) mean value---better than the empirical means, on average \citep[][\S4.8]{snijders2011multilevel}. 
%(This is contrary to what is implied in an earlier version of this book: \citealp[][\S7.2]{qmld}.)
%(To be precise, the mean-squared error is lower for a randomly drawn item. 


The intuition behind shrinkage is that random effects are estimated by `partial pooling', which trades off between two extremes of how by-item variability could be accounted for in a vanilla regression model: `no pooling', where the model estimates the mean for each item separately (e.g.,\ a 24-level factor for \ttt{item\_pair}), and `full pooling', where grouping by item is ignored and only one `average' mean is estimated.  

Each random effect can be thought of as a weighted average of the empirical effect for one group level and the population-level effect, with more weight given to the empirical effect for levels with more observations. (For example, in the \ttt{vot} data the by-word intercept for the word ``and'' will effectively be its empirical average.)  This relates to two important features of mixed-effects models:  imbalanced data is not a problem, because there is no assumption that levels have the same number of observations, and the model can predict for unseen levels (e.g.,\ a new item), which is simply the extreme case of zero observations.

These concepts are discussed further in \citet[][chap. 12]{gelman2007data}.

\end{boxedtext}

\subsection{Multiply-grouped data}
\label{sec:multiply-grouped-data}

The mixed model above considered one kind of variability, by-item variability in the average of \ttt{vowel\_dur}.  
%There was a substantial degree of variability, and accounting for it avoided a Type II error in detecting the effect of \ttt{voicing} for data from a single subject.
Typically in linguistic data there is more than one grouping factor---some common cases are laboratory data where both participants and items are sampled from a larger population, and corpus data where both authors/speakers and the linguistic unit of interest (e.g.,\ words) are sampled from larger populations (e.g.,\ the \ttt{vot} data).  

Consider the full \ttt{neutralization} dataset, a typical laboratory dataset grouped by both participant and item, where the \ttt{voicing} effect is of interest.  Figure~\ref{fig:neut-var-emp} shows the empirical estimates of the intercept (mean \ttt{vowel\_dur}) and the slope (\tsc{voiced}/\tsc{voiceless} difference in \ttt{vowel\_dur}), for each item and each participant. For example, the lowest-left point in the left panel corresponds to a participant with average vowel duration of $\sim$120 msec and with a \tsc{voiced}/\tsc{voiceless} difference of $\sim$-10 msec (in the opposite direction to other participants).  Participants and items show substantial variability in intercept and slope: the \ttt{voicing} effect of interest varies from $\sim$0 to 30 msec, which turns out to be a large effect for this case (Section~\ref{sec:in-context}). 


<<neut-var-emp, echo=FALSE, out.width='45%', fig.asp=0.8, fig.width=default_fig.width*.45/default_out.width, fig.cap='Points: empirical estimates of the intercept (mean \\ttt{vowel\\_dur}) and the slope (mean \\tsc{voiced}/\\tsc{voiceless} difference in \\ttt{vowel\\_dur}), for each item and particiant in the \\ttt{neutralization} data, calculated from voiced/voieless pairs for each item and participant.  To help assess the degree of variability, dotted lines and shading show the estimated intercept and \\ttt{voicing} slope, with 95\\% confidence intervals, from a simple linear regression (i.e.,\ not taking item/participant variability into account).'>>=
by_sub_df <- neutralization %>%
  group_by(subject, item_pair, voicing_fact) %>%
  select(subject, item_pair, voicing_fact, vowel_dur) %>%
  pivot_wider(names_from = voicing_fact, values_from = vowel_dur) %>%
  mutate(diff = voiced - voiceless) %>%
  ungroup() %>%
  group_by(subject) %>%
  dplyr::summarize(mean_diff = mean(diff, na.rm = TRUE)) %>%
  full_join(neutralization) %>%
  group_by(subject, mean_diff) %>%
  summarise(mean_dur = mean(vowel_dur))

by_item_df <- neutralization %>%
  group_by(subject, item_pair, voicing_fact) %>%
  select(subject, item_pair, voicing_fact, vowel_dur) %>%
  pivot_wider(names_from = voicing_fact, values_from = vowel_dur) %>%
  mutate(diff = voiced - voiceless) %>%
  ungroup() %>%
  group_by(item_pair) %>%
  dplyr::summarize(mean_diff = mean(diff, na.rm = TRUE)) %>%
  full_join(neutralization) %>%
  group_by(item_pair, mean_diff) %>%
  summarise(mean_dur = mean(vowel_dur))

diff1 <- mean(filter(neutralization, voicing_fact == "voiced")$vowel_dur) - mean(filter(neutralization, voicing_fact == "voiceless")$vowel_dur)

neut_mod_0 <- lm(vowel_dur ~ voicing, data = neutralization)
int_low <- confint(neut_mod_0)[1, 1]
int_high <- confint(neut_mod_0)[1, 2]
beta_low <- confint(neut_mod_0)[2, 1]
beta_high <- confint(neut_mod_0)[2, 2]

by_sub_df %>% ggplot(aes(x = mean_dur, y = mean_diff)) +
  geom_point() +
  geom_rect(aes(xmin = int_low, xmax = int_high, ymin = -Inf, ymax = Inf), alpha = 0.01) +
  geom_rect(aes(ymin = beta_low, ymax = beta_high, xmin = -Inf, xmax = Inf), alpha = 0.01) +
  geom_hline(aes(yintercept = diff1), lty = 2) +
  geom_vline(aes(xintercept = mean(neutralization$vowel_dur)), lty = 2) +
  xlab("Mean vowel duration") +
  ylab("Voiced-voiceless\nduration difference") +
  ggtitle("By-participant") +
  ylim(-6, 30) +
  xlim(120, 215)

by_item_df %>% ggplot(aes(x = mean_dur, y = mean_diff)) +
  geom_point() +
  geom_rect(aes(xmin = int_low, xmax = int_high, ymin = -Inf, ymax = Inf), alpha = 0.01) +
  geom_rect(aes(ymin = beta_low, ymax = beta_high, xmin = -Inf, xmax = Inf), alpha = 0.01) +
  geom_hline(aes(yintercept = diff1), lty = 2) +
  geom_vline(aes(xintercept = mean(neutralization$vowel_dur)), lty = 2) +
  xlab("Mean vowel duration") +
  ylab("Voiced-voiceless\nduration difference") +
  ggtitle("By-item") +
  ylim(-6, 30) +
  xlim(120, 215)
@


Most linguistic data shows such clear variability, by 2+ grouping factors. In mixed-effects models it is possible to account for both kinds of variability simultaneously, by including by-participant and by-item random effects.  This is a case of \emph{crossed} random effects: multiple grouping factors which vary independently.  (The alternative is \emph{nested} random effects, discussed in
%such as by-dialect variability and each speaker varies within dialect:
Section~\ref{sec:ranef-further-uses}.)    Crossed random-effect structure is necessary to model even simple linguistic experiments, so the facility of fitting crossed random effects in {lme4} makes it well-suited for modeling linguistic data  (Box~\ref{box:repeated-measures}).

\begin{boxedtext}{Broader context: Repeated measures data in language sciences and beyond}
\label{box:repeated-measures}

The importance of accounting for both kinds of variability goes back to at least \citet{clark1973language}, who famously termed not accounting for by-item variability  the ``language-as-fixed-effect fallacy''.  Following Clark, the standard in some subfields (e.g.,\ psycholinguistics, phonetics)  has been repeated measures ANOVA analyses where separate `by-participant' and `by-item' models are fit, then interpreted together (e.g.,\ \citealp[][pp.\ 3--7]{meteyard2020best}; \citealp[][\S7.2]{baayen2008analyzing}; \citealp[][chap. 4]{johnson2008quantitative}; \citealp[][chap. 5]{gries2013statistics}).
%% NB: Gries (2021) removes this section, so leave this ref in!
%(Other subfields, such as sociolinguistics, used analysis methods which effectively continued to commit this fallacy.)
While this method is not wrong, it has issues including inflexibility to more complex designs and lowered statistical power.

Mixed-effects models were introduced for this case for psycholinguistics using {lme4} in \citet{baayen2008mixed}, which was an extremely influential paper across language sciences. (Earlier papers, such as \citealp{quene2004multi,brysbaert2007language} had introduced mixed-effects models without {lme4}.)  Mixed models had been long used before this in other fields with grouped data, such as education and ecology, and as \citet[][400]{mcelreath2020statistical} notes, ``... multilevel regression deserves to be the default approach [across fields]. There are certainly contexts in which it would be better to use an old-fashioned single-level model. But the contexts in which multilevel models are superior are much more numerous.''

%% FUTURE: consider re-adding
% Why did mixed-effects models only become popular in language sciences after \citet{baayen2008mixed}?
% I think there were two reasons: the availability of appropriate software, and interdisciplinary collaboration.  In many other fields where mixed models are used, crossed random effects only come up in particularly complex designs, while nested designs are common. Thus,  many mixed model software packages assume only one grouping factor, or nested grouping factors, such as \ttt{lme} (the precursor to {lme4}).  The ease of fitting crossed random effects in {lme4}  makes it particularly well-suited for modeling linguistic data---in part as a result of collaboration between its primary architect, the statistician Doug Bates, and the psycholinguist R. Harald Baayen.    {lme4} has in turn been built on by many other packages, such as for generalized additive modeling, which are thus compatible with linguistic data.  


\end{boxedtext}


\subsection{Two grouping factors, random intercepts}
\label{sec:two-grouping-random-intercepts}

We begin with a model with by-item and by-participant random intercepts, for  the \ttt{neutralization} case.  This data has $J=\Sexpr{n_neut_sub}$ participants and $K=\Sexpr{n_neut_item}$ items; it is fully crossed (every participant sees both \ttt{voicing} values from every item), with $n=\Sexpr{n_neut}$.\footnote{Note that this is a bit less than $2 \cdot \Sexpr{n_neut_sub} \cdot \Sexpr{n_neut_item} = \Sexpr{2*n_neut_sub*n_neut_item}$ due to some missing data.}    Let $j[i]$ and $k[i]$ now denote the participant and item for the $i$th observation. The simplest regression model for this case, with a single fixed-effect term ($x$: \ttt{voicing}), would be:

\begin{align}
y_i & = \beta_0 + \alpha_{P, j[i]} + \alpha_{I, k[i]} +
\beta_1 x_i + \epsilon_i \nonumber \\
\epsilon_i & \sim N(0, \sigma_E^2), \quad \text{for } i = 1, \ldots, 749 \nonumber \\
\alpha_{P, j} & \sim N(0, \sigma_{P,0}^2) , \quad \text{for } j = 1, \ldots, 16
\nonumber \\
\alpha_{I, k} & \sim N(0, \sigma_{I,0}^2) , \quad \text{for } k = 1, \ldots, 24
\label{eq:lmm2}
\end{align}

The first line looks similar to the previous model (equation~\ref{eq:lmm1}), except there are now two random-effect terms: $\alpha_{I,k[i]}$ is the offset of item $k[i]$ from the population-level intercept ($\beta_0$), while  $\alpha_{P,j[i]}$ is the offset of participant $j[i]$.  Participants' offsets (or `by-participant random intercepts') and items' offsets are each assumed to be normally distributed, with $\sigma_{P,0}$ and $\sigma_{I,0}$ capturing the degree of variability in participants' and items' intercepts, respectively.  (The 0 subscript is to distinguish these from random slopes, introduced later.)

This model predicts a different line for each participant/item pair, with differing intercepts but the same slope, as in Figure~\ref{fig:neut-0-preds-1}. For example, the relationship predicted for participant 5 for item 10 would be $y = \beta_0 + \alpha_{P,5} + \alpha_{I,10} + \beta_1 x$.  The population-level relationship is still $y = \beta_0 + \beta_1 x$, but now  $\beta_0$ is the intercept for an `average participant and item'.  

To fit and summarize this model, using \ttt{lmer()}:

<<output.lines=11:22>>=
neut_mod_0 <- lmer(vowel_dur ~ voicing + (1 | subject) +
    (1 | item_pair), data = neutralization)
summary(neut_mod_0)
@

<<neut-0-preds-1, echo=FALSE,  out.width='80%',fig.width=default_fig.width*.80/default_out.width, fig.asp=0.4, fig.cap='Predicted  \\ttt{voicing} effects for a subset of participants and items from model \\ttt{neut\\_mod\\_0}, where intercepts vary by participant and item.'>>=
neut_mod_0_plotting <- lmer(vowel_dur ~ voicing + (1 | subject) +
  (1 | item_pair), neutralization)

plotDf0 <- ggpredict(neut_mod_0_plotting, terms = c("voicing", "subject", "item_pair"), type = "re") %>%
  data.frame() %>%
  rename(subject = group, item = facet)

plotDf0 %>%
  mutate(x = factor(x, labels = c("voiced", "voiceless"))) %>%
  filter(subject %in% c("1", "2", "4", "8", "13") & item %in% c("10", "18", "21", "24")) %>%
  ggplot(aes(x = x, y = predicted, group = subject)) +
  geom_line(aes(color = subject), size=1) +
  scale_color_grey() + 
  xlab("Voicing") +
  ylab("Vowel duration") +
  facet_grid(~item, labeller = "label_both") + 
  theme(axis.text.x = element_text(angle = 45, vjust=0.5))
@



This model suggests 
that vowels before \tsc{voiced} stops are significantly longer than before \tsc{voiceless} ($|t|>2$).

Recall that the effect of stop voicing on vowel duration is expected to be small---the research questions are whether an effect exists at all, and its exact size. The answer to either could be affected by not accounting for other factors affecting vowel duration.

To get closer to a realistic model for this data, we refit the model with some control predictors as fixed effects: \ttt{prosodic\_boundary}, \ttt{place}, and \ttt{vowel} (factors with 2, 3, and 5 levels)---and include these controls in subsequent models.  This model, which is similar to the original model of this data reported by \citet{roettger2014assessing} (Experiment 1), just corresponds to adding additional fixed-effect terms to the model in equation \eqref{eq:lmm2} above (e.g.,\ $\beta_2 x_{2,i}$ for \ttt{prosodic\_boundary} for observation $i$).\footnote{These controls account for the presence of a prosodic boundary and the identity of the vowel and its following stop, all of which strongly affect the vowel's duration.}
%I have dropped a couple control predictors from \citeauthor{roettger2014assessing}'s  analysis (\ttt{accent\_type}, \ttt{norming\_voiceless\_count}) for simplicity, since neither significantly improves model likelihood.} 

This model is fitted using \ttt{lmer()} as follows:
<<output.lines=13:31>>=
neut_mod_1 <- lmer(vowel_dur ~ voicing + prosodic_boundary + place + 
    vowel + (1 | subject) + (1 | item_pair), data = neutralization)
summary(neut_mod_1)
@

The \ttt{voicing} effect remains significantly positive ($|t|>2$), and is in fact `clearer' once the control predictors are included: the fixed-effect coefficient estimate is further from 0 and has a higher $t$-value.  The control predictors also all have significant effects, 
%with \ttt{place} and \ttt{vowel} particularly important , 
but these are not of interest here (Exercise~\ref{ex:neut-1-eff-size}). 


Turning to the random effects, note that the estimated degree of by-item variability ($\hat{\sigma}_I = \Sexpr{as.numeric(tidy(neut_mod_1)[10,'estimate'])}$) is lower than the degree of by-participant variability ($\hat{\sigma}_P = \Sexpr{as.numeric(tidy(neut_mod_1)[11,'estimate'])}$).  This ``participants vary more than items'' pattern is common for data from laboratory experiments, because items are chosen from a constrained set by the researcher (here, words of a particular phonotactic shape) while participants are chosen from a less constrained set (e.g., all undergraduates).


% In this case participants tend to differ more in `baseline' \ttt{vowel\_dur} than items (after controlling for the item's \ttt{place} and \ttt{vowel}, as discussed below). This pattern, w
% 
% there is about as much by-item variability as by-participant variability in \ttt{vowel\_dur} ($\sigma_P$, $\sigma_I \approx 27$ msec), and 


\subsection{Types of predictors}
\label{sec:predictor-types}

Mixed models are also called `hierarchical' or `multilevel' models because they predict relationships at different \emph{levels}: individual observations, but also  for individual items or participants---any value of a grouping factor. 

 A predictor which describes a property of participants (e.g.,\ gender) is \emph{participant-level}, a predictor which describes a property of items (e.g.,\ word frequency) is \emph{item-level}, and so on.  Participant-level predictors are sometimes called `between-participant', because they do not vary within participants, while a predictor that varies within participants is `within-participant' (and similarly for `between-item', etc.). 
 %(We will not use within/between-X terminology, but it is widely used in behavioral sciences.)  
 A predictor which can vary within each grouping factor---for example, varies within both items and participants---is called \emph{observation-level}.

For the \ttt{neutralization} data, every item (\ttt{item\_pair}) corresponds to a pair of pseudowords ending in $VCe$ and $VC_2e$ (vowel-consonant-`e'), which are identical except for the voicing of the consonant. For example, item 24 is ``stauge'' and ``stauke'' ($V$ = /au/, $C$=/g/ or /k/).  Thus, \ttt{vowel} (the identity of $V$) and \ttt{place} (the place of articulation of $C$) are item-level: they are properties of the item.  All other predictors of interest are observation-level: \ttt{voicing} varies within item and within participant, while \ttt{prosodic\_boundary} relates to how this particular observation was produced.  (Speaker \ttt{gender} is a participant-level predictor in the dataset but it is not included in our analysis.)  As this example illustrates, working out the levels of predictors requires a good understanding of the structure of the data. 

% In a mixed-effects model, the by-X random effects capture variation among X levels \textbf{after} controlling for X-level predictors included as 


\subsubsection{Example: Changes in variance components}
\label{sec:changes-variance-components}

It is often useful for model fitting and interpretation to think of predictors in terms of their level.  This helps understand what random-effect terms mean, and why variance components differ between models of the same data.

First, consider what a random-effect term like ``by-item random intercept'' means. Above we described e.g., $\alpha_j$ as ``the offset for item $j$ from the mean'', but this is not quite right---it is the offset \textbf{after} controlling for item-level predictors.  For example in model \ttt{neut\_mod\_1}, which contains item-level predictors \ttt{place} and \ttt{vowel}, the by-item random intercept for the ``stauCe'' item is how much its \ttt{vowel\_dur} differs from what is expected given that \ttt{place}=\tsc{velar} and \ttt{vowel}=\tsc{au}.

This explains the main difference between models \ttt{neut\_mod\_0} and \ttt{neut\_mod\_1} above, from adding the control predictors: the estimated degree of by-item variability  ($\hat{\sigma}_I$) drops from \Sexpr{as.numeric(tidy(neut_mod_0)[3,'estimate'])} to \Sexpr{as.numeric(tidy(neut_mod_1)[10,'estimate'])}, while the estimated by-participant variability and the residual error change little.  This is because two item-level predictors were added to the model, so the meaning of $\hat{\sigma}_I$ has changed from ``degree of variability among word pairs'' to ``degree of variability among word pairs with the same \ttt{place} and \ttt{vowel} values''. In other words, some variance which looked like `random' variation among items turned out to be attributable to properties of the items.

Finally, consider the residual error, which quantifies `how much variance is left over'.  The residual error is similar for the two models fitted to the full \ttt{neutralization} dataset (\ttt{neut\_mod\_0}: \Sexpr{as.numeric(tidy(neut_mod_0)[5,'estimate'])}, \ttt{neut\_mod\_1}: \Sexpr{as.numeric(tidy(neut_mod_1)[12,'estimate'])}). This is because the models differ in the addition of control predictors, which did not change how much variance in the data was
%adding control predictors did not change how much variance in the data was 
`explained', but what source it is ascribed to: all `random', versus some `random' and some item-level predictors.  

In contrast, consider the two models fitted for participant 2's data from Section \ref{sec:lmm-intro}. The value for the mixed model (\ttt{neut\_2\_mod\_1}), $\hat{\sigma}_E = \Sexpr{sig_e_est}$, is much lower than the value for the simple linear regression:
%means (`how much variance is left over'), compare this value to the residual error for the simple linear regression model: 

<<>>=
glance(neut_2_mod_0)$sigma
@

The residual error is much smaller in the random-intercept model because the `left-over' variance of the simple linear regression model has been partitioned into item-level (the $\alpha_j$)  and observation-level ($\sigma_E$) variance. Intuitively, some `error' has been given to the participant level.


% Second, We illustrate here how this helps understand what the variance components mean, and why they change between models of the same data.

%Next, 



%neutralization %>% expand(item_pair, voicing) %>% left_join(model.frame(neut_mod_1))



% 
% We now consider the full \ttt{neutralization} dataset, across XX items and XX subjects, with the goal of estimating the (fixed) effect of \ttt{voicing}.  Before discussing how this is modeled, let's take a look at the different kinds of variability present in 


% 
% Figure~\ref{fig:neut-var-emp} shows the empirical estimates of the average \ttt{vowel\_dur} and the voiced/voiceless difference in \ttt{vowel\_dur}, for each item and each subject. These correspond to the two population-level effect of interest: the intercept and the slope of \ttt{voicing}. Because the data is fully crossed (every subject sees every item) these empirical estimates are precise.

\section{Random slopes}
\label{sec:random-slopes}

Models so far allowed the intercept to differ between groups---each participant or item's `baseline' is different. In addition, the slope of an effect could differ by group, which is captured in mixed models by \emph{random slope} terms. For the \ttt{neutralization} case, the empirical data suggests the effect of \ttt{voicing} might vary between both participants and items (Figure~\ref{fig:neut-var-emp}). 

\subsection{Introduction: One random slope}
\label{sec:one-random-slope}

To introduce random slopes, let's start with a simple example: adding a single random slope term to the random-intercepts model without controls (\ttt{neut\_mod\_0}), to account for by-participant variation: a `by-participant random slope of \ttt{voicing}'.  The regression model in this case is:

\begin{align}
y_i & = \beta_0 + \alpha_{P, j[i]} + \alpha_{I, k[i]} +
(\beta_1 + \gamma_{P, j[i]}) x_i + \epsilon_i \nonumber \\
\epsilon_i & \sim N(0, \sigma_E^2), \quad \text{for } i = 1, \ldots, 749 \nonumber \\
\alpha_{P, j} & \sim N(0, \sigma_{P,0}^2) , \quad \text{for } j = 1, \ldots, 16
\nonumber \\ 
\gamma_{P, j} & \sim N(0, \sigma_{P,1}^2)  \nonumber \\
\alpha_{I, k} & \sim N(0, \sigma_{I,0}^2) , \quad \text{for } k = 1, \ldots, 24
\label{eq:lmm3}
\end{align}

This is similar to equation \eqref{eq:lmm2}, but with an extra random slope term: $\gamma_{P,j}$ captures how much each participant's slope (for \ttt{voicing}) differs from the average slope across participants ($\beta_1$), and these by-participant offsets are normally distributed (variance $\sigma_{P,1}^2$).  All other terms have the same interpretations as before (equation~\ref{eq:lmm2}).

The model predicts a different line for each participant (averaging across items), differing in both intercept and slope.  For example, for participant 5:
$$ y = \beta_0 + \alpha_{P,5} + (\beta_1 + \gamma_{P,5})x$$  

These by-participant predictions (code in Section~\ref{sec:fixed-random-preds}) are shown in Figure~\ref{fig:neut-3-preds}, which are from the fitted model:

<<>>=
neut_mod_2 <- lmer(vowel_dur ~ voicing + (1 + voicing || subject) +
  (1 | item_pair), data = neutralization)
@

<<neut-3-preds, echo=FALSE, fig.asp=0.75, out.width='45%',fig.width=default_fig.width*.45/default_out.width, fig.cap='Predicted \\ttt{voicing} effect for each participant from model \\ttt{neut\\_mod\\_2} (averaging across items).'>>=
neut_mod_2_plotting <- lmer(vowel_dur ~ voicing + (1 + voicing || subject) + (1 | item_pair), neutralization)

plot_df_1 <- ggpredict(neut_mod_2_plotting, terms = c("voicing", "subject"), type = "re") %>% data.frame()

plot_df_1 %>%
  mutate(x = factor(x, labels = c("voiced", "voiceless"))) %>%
  ggplot(aes(x = x, y = predicted, group = group)) +
  geom_line() +
  xlab("Voicing") +
  ylab("Vowel duration")


beta <- fixef(neut_mod_2_plotting)[["voicing"]]
sig <- filter(tidy(neut_mod_2_plotting), term == "sd__voicing")$estimate
  @

In this formula, \texttt{(1 + voicing || subject)}, read ``uncorrelated by-subject random intercept and by-subject random slope'', is {lme4} notation for:
\begin{itemize}
\item Random intercept (\texttt{1} means ``intercept'')
\item
  Random slope of \texttt{voicing} (random effects go to the left of the `pipe' symbol \texttt{\textbar{}})
\item Grouped by participant (grouping factor goes to the right of the pipe)
\item The random intercept and slope are \emph{uncorrelated}---indicated by the double pipe \texttt{\textbar{}\textbar{}}
\end{itemize}

We discuss the last point further when we introduce correlated random effects (Section~\ref{sec:ranef-corrs}); until then, we assume uncorrelated random effects in all models.

The model's parameter estimates are:

<<output.lines=13:24>>=
summary(neut_mod_2)
@

% Here we have used \ttt{tidy} (from \ttt{broom.mixed}) which gives a more compact model summary compared to \ttt{summary(neut\_mod2)}.

Most of the rows here have the same interpretation as in the random intercepts-only model (\ttt{neut\_mod\_1}).  What is new is the random slope variance component, which is estimated to be $\hat{\sigma}_{P,1} = \Sexpr{filter(tidy(neut_mod_2), term=='sd__voicing')$estimate}$.  This is the degree of by-participant variability in the \ttt{voicing} effect.

The degree of variability in the slope of $x$ (the random slope term) is only meaningful relative to the size of the population-level slope of $x$ (the fixed-effect term): if $\hat{\sigma_{P,1}} = 0.2 \cdot \hat{\beta}_1$, then participants are predicted to have slopes that differ in magnitude but not sign (because 95\% of participants lie in $\hat{\beta}_1 \pm 1.96 \cdot \hat{\sigma}_{P,1}$), while if $\hat{\sigma}_{P,1} = 1.5 \cdot \hat{\beta}_1$, participant slopes are predicted to differ in magnitude \textbf{and} sign, and so on.  For this reason, if reporting a random-effect variance it is useful to summarize the distribution with a (approximate) 95\% or 99\% interval, like ``95\% of participants are predicted to have slopes between \Sexpr{beta-1.96*sig} and \Sexpr{beta + 1.96*sig}'' ($= \hat{\beta}_1 \pm 2 \cdot \hat{\sigma}_{P,1}$)---which is the prediction for \ttt{neut\_mod\_2}.

We return to how to interpret ``the \ttt{voicing} effect'' in a mixed model (i.e., the presence of both a fixed effect and random slopes) after building a more realistic model for this data, at the end of Section~\ref{sec:random-slopes}.

\paragraph{Notation: Uncorrelated random effects}
%\label{sec:uncorr-ranef-notation}

In {lme4}, uncorrelated random effects can be written in several ways in the model formula:
\begin{enumerate}

\item \ttt{(1 + speaking\_rate\_dev || speaker)}

\item \ttt{(1 | speaker) + ( 0 + speaking\_rate\_dev | speaker)}

\item \ttt{(1 | speaker) + (-1 + speaking\_rate\_dev | speaker)}

\end{enumerate}

(1) is useful shorthand when writing model formulas, but under the hood {lme4} formulas represent uncorrelated random slopes using (2). This is why you will see this model formula in the full \ttt{summary(neut\_mod2)} output: 

<<>>=
formula(neut_mod_2)
@

(3) and (2) are equivalent ways to exclude the intercept from R model formulas (Box~\ref{box:cell-means-coding}). When referring to just an uncorrelated random slope, for example in a hypothesis test,
%(examples in Section~\ref{sec:lmm-fixed-and-random} on), 
you use (2) or (3).


\subsection{Multiple random slopes}
\label{sec:lmm-multiple-random-slopes}

We now build a more realistic model, accounting for both by-participant and by-item variation in the effect of \ttt{voicing} (Figure~\ref{fig:neut-var-emp}), by adding by-participant and by-item random slopes to the random-intercepts model \textbf{with} control predictors (\ttt{neut\_mod\_1}).  The first line of the regression model in this case looks similar to equation \eqref{eq:lmm3}:
\begin{align}
y_i  = & \underbrace{\beta_0 + \alpha_{P, j[i]} + \alpha_{I, k[i]}}_{\text{intercept}} +
\underbrace{(\beta_1 + \gamma_{P, j[i]} + \gamma_{I, k[i]})}_{\text{\ttt{voicing} slope}} x_{1,i} + \\
& \text{(control predictors)} + \epsilon_i \nonumber
\end{align}

Here $x_{1,i}$ is the value of \ttt{voicing} for observation $i$, and we have omitted the control predictors $x_2$--$x_8$, which would correspond to $\beta_2$--$\beta_8$, for clarity.  
In this model, both the intercept and the \ttt{voicing} slope now differ by participant and by item, and each kind of offset is normally distributed (variances $\hat{\sigma}^2_{P,0}$, $\hat{\sigma}^2_{I,0}$, $\hat{\sigma}^2_{P,1}$, $\hat{\sigma}^2_{I,1}$). To fit this model in R:

<<output.lines=14:30>>=
neut_mod_3 <- lmer(vowel_dur ~ voicing + prosodic_boundary + place +
  vowel + (1 + voicing || subject) + (1 + voicing || item_pair),
data = neutralization)

summary(neut_mod_3)
@
  
  
<<neut-3-preds-2, echo=FALSE,  out.width='80%',fig.width=default_fig.width*.80/default_out.width, fig.asp=0.4, fig.cap='Predicted  \\ttt{voicing} effects for a subset of participants and items from model \\ttt{neut\\_mod\\_3}, where both intercepts and slopes vary by participant and items.'>>=
# hi
neut_mod_3_plotting <- lmer(vowel_dur ~ voicing + prosodic_boundary +
  place + vowel + (1 + voicing || subject) +
  (1 + voicing || item_pair), neutralization)

plot_df_3 <- ggpredict(neut_mod_3_plotting, terms = c("voicing", "subject", "item_pair"), type = "re") %>%
  data.frame() %>%
  rename(subject = group, item = facet)

plot_df_3 %>%
  mutate(x = factor(x, labels = c("voiced", "voiceless"))) %>%
  filter(subject %in% c("1", "2", "4", "8", "13") & item %in% c("10", "18", "21", "24")) %>%
  ggplot(aes(x = x, y = predicted, group = subject)) +
  geom_line(aes(color = subject), size=1) +
  scale_color_grey() +
  xlab("Voicing") +
  ylab("Vowel duration") +
  facet_grid(~item, labeller = "label_both")
@

As shown in Figure~\ref{fig:neut-3-preds-2} (prediction/plot code hidden), 
the model predicts a different relationship (intercept and slope) for each participant/item pair.
  
Focusing on the \ttt{voicing} slope (of primary interest) in particular, the model makes predictions at several levels.  At the population level,  vowel duration is greater for \tsc{voiced} than \tsc{voiceless} stops, for an average participant and item (\ttt{voicing} fixed effect: $\hat{\beta}_1 = \Sexpr{fixef(neut_mod_3)[['voicing']]}$, $|t|>2$).  

At the level of individual participants and items, the model predicts the \ttt{voicing} slope (= \tsc{voiced}-\tsc{voiceless} difference) for  participant $j$ (averaging over items) or item $k$ (averaging over participants).\footnote{These would be $\hat{\beta}_1 + \gamma_{P,j}$ for participant $j$ and $\hat{\beta}_1 + \text{(control predictors)} + \gamma_{I,k}$---where the control predictors would account for the \ttt{place} and \ttt{vowel} of item $k$ (both item-level predictors). In practice you can just make predictions with e.g.,\ \ttt{predict} without worrying about how they are calculated.}  For example, the \ttt{voicing} slopes for the first two participants and items are:

<<>>=
tidy(neut_mod_3, effects = "ran_coefs") %>%
  filter(level %in% c("1", "2"), term == "voicing")
@


The parameters $\hat{\sigma}_{P,1}=\Sexpr{extractRanefEst(neut_mod_3, 'subject.1', 'sd__(Intercept)')}$ and $\hat{\sigma}_{I,1}=\Sexpr{extractRanefEst(neut_mod_3, 'item_pair.1', 'sd__(Intercept)')}$ describe the degree of variability in these \ttt{voicing} slopes, across participants and items.  

To help get an intuition for these parameters,  Figure~\ref{fig:neut-3-preds-3} shows the predicted {distributions} of \ttt{voicing} slopes  predicted by the model across participants and items in the population (solid lines): normal distributions centered at $\hat{\beta}_1 = \Sexpr{fixef(neut_mod_3)[['voicing']]}$, with widths $\hat{\sigma}_{P,1}=\Sexpr{extractRanefEst(neut_mod_3, 'subject', 'sd__voicing')}$ and $\hat{\sigma}_{I,1}=\Sexpr{extractRanefEst(neut_mod_3, 'item_pair', 'sd__voicing')}$.  These are superimposed on the predicted  slopes for observed participants/items (histograms).  Every row in a mixed model's ``Random effects:'' table (i.e., a variance component estimate) can be thought of as the width of such a normal distribution, describing how much some predictor's effect (or the intercept) is predicted to vary in the population.

In this case, the \ttt{voicing} slope is predicted to vary substantially in {magnitude} across participants, but not direction, and to vary only slightly across items (Exercise~\ref{ex:neut-mod3-ranef-dist}). 

The overall picture is of a robust population-level \ttt{voicing} effect, of magnitude around 9 msec (95\% CI = \Sexpr{printable_ci(neut_mod_3, 'voicing')}).  Individual participants, and to a smaller extent items, vary in the magnitude but not direction of the effect.

<<neut-3-preds-3, echo=FALSE, out.width='45%',fig.width=default_fig.width*.45/default_out.width,  fig.cap='By-participant and by-item variation in the \\ttt{voicing} effect for \\ttt{neut\\_mod\\_3}.  Distribution of model-predicted \\ttt{voicing} coefficient estimates (histograms, rug plots) for each participant (left) and each item (right), with overlaid estimates for \\ttt{voicing} population-level effect (dotted lines) and predicted distribution of  \\ttt{voicing} effects across participants/items (solid lines).'>>=
neut_mod_3_plotting <- lmer(vowel_dur ~ voicing + prosodic_boundary +
  place + vowel + (1 + voicing || subject) +
  (1 + voicing || item_pair), neutralization)

plot_df_1 <- data.frame(
  pred = coefficients(neut_mod_3_plotting)$subject$voicing,
  emp = by_sub_df$mean_diff
)

plot_df_2 <- data.frame(
  pred = coefficients(neut_mod_3_plotting)$item_pair$voicing,
  emp = by_item_df$mean_diff
)

#  plot_df_2 <- ggpredict(neut_mod_2_plotting, terms = c('voicing', 'subject', 'item_pair'), type='re') %>% data.frame() %>% rename(subject=group, item=facet)



beta <- fixef(neut_mod_3_plotting)[["voicing"]]
sig_p <- filter(tidy(neut_mod_3_plotting), term == "sd__voicing" & group == "subject")$estimate
sig_i <- filter(tidy(neut_mod_3_plotting), term == "sd__voicing" & group == "item_pair")$estimate




plot_df_1 %>% ggplot(aes(x = pred)) +
  geom_histogram(aes(y = ..density..), bins = 10, alpha = 0.5) +
  geom_rug() +
  xlim(-2, 20) +
  geom_vline(aes(xintercept = fixef(neut_mod_3_plotting)[["voicing"]]), lty = 2, color = default_line_color, size=0.75) +
  stat_function(aes(x = x), fun = dnorm, n = 100, args = list(mean = beta, sd = sig_p), data = data.frame(x = c(-2, 20))) +
  xlab("Predicted participant voicing slopes")

plot_df_2 %>% ggplot(aes(x = pred)) +
  geom_histogram(aes(y = ..density..), bins = 30, alpha = 0.5) +
  geom_rug() +
  xlim(7, 13) +
  geom_vline(aes(xintercept = fixef(neut_mod_3_plotting)[["voicing"]]), lty = 2, color = default_line_color, size=0.75) +
  stat_function(aes(x = x), fun = dnorm, n = 100, args = list(mean = beta, sd = sig_i), data = data.frame(x = c(7, 13))) +
  xlab("Predicted item voicing slopes")
@





% Note how the histograms lie `inside' the distributions: the predicted degree of variability in the population is \textbf{greater} than the spread of predictions for observed participants (or items). This makes sense given shrinkage, and illustrates why random slopes guard against Type I errors: the model is assuming that the population




% \begin{align}
% y = \beta_0 + \alpha_{P,j} + (\hat{\beta}_1 + \gamma_{P,j}) \cdot \ttt{voicing} \quad \text{participant } j \\
% y = \beta_0 + \alpha_{I,k} + (\hat{\beta}_1 + \gamma_{I,k}) \cdot \ttt{voicing} \quad \text{item } j
% \end{align}



  
  
  
% This term,  together with the \ttt{voicing} fixed effect $\hat{\beta}_1 = \Sexpr{beta}$, describes the predicted \textbf{distribution} of participant \ttt{voicing} slopes in the population: a $N(\Sexpr{beta}, \Sexpr{sig})$ distribution.  Figure~\ref{fig:neut-3-preds} (right) shows this distribution, superimposed on the predicted slope for each speaker (the $\hat{\gamma}_j$: histogram) and the population-level effect ($\hat{\beta}_1$: dotted line).  Note how the histogram lies `inside' the distribution: the predicted degree of variability in the population is \textbf{greater} than the spread of predictions for observed speakers. This makes sense given XX.

% 
% Before turning to the effect of interest (\ttt{voicing}), let's XX the predictions this model makes at different levels. 
% 
% 
% The model predicts a different line for each participant/item pair, differing in both intercept and slope.  For example,  for participant 5 and item 2:
% 
% \begin{align}
% y = & \underbrace{\beta_0 + \alpha_{P,5} + \alpha{I,2} + \text{(control predictors)}}_{\text{intercept}} + \\
% & \underbrace{(\beta_1 + \gamma_{P,5} + \alpha_{I,2})}_{\text{slope}} x
% \end{align}


% - Discuss how the model can be explicitly rewritten as `modeled coefficients': observation (one line), intercept (one line), slope (one line).  See Gelman \& Hill for more exposition.  This can be very useful for conceptual understanding, and is a must for specifying more complex models than possible in lmer (e.g., bayesian) -- requires thinking about exactly which predictors are which level, how this affects fixed and random effects.
% 
% - An important feature of lmer is that it essentially does this for you -- you can always just write the model at the observation level, in one line, and it figures out automatically which predictors are participant-level, etc.
% 

\subsection{What does adding a random slope do?}
\label{sec:what-adding-random-slope}

 % - random slopes are crucial to using mixed models in practice...
 % here we give a sense of the issues involved (see comments, need an intro)
% they affect estimates of fixed effects (typically of most importance) and also 
% 
% primarily because which slopes to include is the biggest practical issue in fitting models. 
% we cover model selection in the next chapter, but here XX to get a sense of why random slopes are important.


Random slopes are crucial to using mixed models in practice, as they affect estimates of fixed effects (which are often of most interest) and which slopes to include is typically the hardest part of model selection. We discuss the first issue here, and model selection in a later chapter (Section~\ref{sec:lmm-model-selection}).

Consider models \ttt{neut\_mod\_0}, \ttt{neut\_mod\_2} fitted for the \ttt{neutralization} data. The models each have a single fixed effect (for \ttt{voicing}) and differ only in the by-participant random effects:
\begin{itemize}

\item Model \ttt{neut\_mod\_0}: By-participant random intercept (only)

\item Model \ttt{neut\_mod\_2}: By-participant random intercept, random slope of \ttt{voicing}

\end{itemize}

\ttt{neut\_mod\_0} models \texttt{vowel\_dur} as a linear function of \texttt{voicing}, with the \textbf{same slope} for every participant. Thus, every participant's data is contributing to estimating one number (related to \ttt{voicing}): \(\beta_1\), the population-level effect.


\ttt{neut\_mod\_1} models \texttt{vowel\_dur} as a linear function of \texttt{voicing}, with a \textbf{different slope} for every participant. Thus, every participant's data is contributing to estimating two numbers: $\beta_1$, and the offset of that participant's slope from $\beta_1$.  Intuitively, this results in the model being less certain about the population-level effect (\(\beta_1\)).

We can see the result by comparing the \ttt{voicing} fixed-effect rows in the two models (code hidden):
%(from \ttt{summary(neut\_mod1)}, \ttt{summary(neut\_mod\_2)}):

<<echo=FALSE>>=
rbind(
  tidy(neut_mod_1) %>% filter(term == "voicing"),
  tidy(neut_mod_2) %>% filter(term == "voicing")
) %>%
  select(estimate, std.error, statistic) %>%
  add_column(model = c("Intercept", "Int. + slope"), .before = 1) %>%
  rename(t = statistic)
@


% 
% \begin{center}
% \begin{tabular}{llll}
% \toprule
% Model & Estimate & Std. Error & \(t\) value \\
% \midrule
% Intercept-only & 9.27 & 1.48 & 6.25 \\
% Intercept + slope & 9.28 & \textbf{1.95} & 4.76 \\ 
% \bottomrule
% \end{tabular}
% \end{center}

Both models show similar estimated population-level effects (\(\hat{\beta}_1\)) for \texttt{voicing}. But the intercept + slope model has a larger standard error for this effect, resulting in a smaller \(t\) value.  It makes sense that the standard error of the \texttt{voicing} fixed effect goes up when a random slope is added: the model detects significant variability among participants in the effect of \texttt{voicing}, which makes it less sure of the {average} effect of \texttt{voicing}.  Typically \textbf{adding a random slope increases the standard error} of the corresponding fixed effect.  

Adding a random slope could also change the estimated population-level effect, especially if the data is highly imbalanced by level of the grouping factor (as in corpus data) or some levels have `outlier' effects, neither of which is the case for the \ttt{neutralization} data.

% FUTURE: maybe an example, in exercise?


\subsubsection{Example: Speaking rate for voiced stops for \ttt{vot} data}
\label{sec:vot-example-rate}


To see the practical relevance of all this, we turn to an example using the \ttt{vot} data where whether the random slope is included changes the qualitative conclusion from the model.  In addition this example uses corpus data, which is an important use case for mixed-effects models. Different issues tend to come up when fitting mixed-effects models to data from corpora or laboratory experiments, so we use examples of both going forward (here, \ttt{vot} and \ttt{neutralization}).

% 
% - to emphasize why this could matter in practice -- worth a new example, which also introduces corpus data: VOT.

\citet{sonderegger2017medium} model \ttt{vot} in this dataset for all speakers and words, as a function of many predictors, separately for voiceless and voiced stops.  To keep things relatively simple we will simplify the data a bit, to a subset \ttt{vot\_voiced\_core}: voiced stops, restricted to just 11 \textbf{speakers} with the most data and to \textbf{words} which are one syllable long (so we don't have to consider variables like  syllable \ttt{stress}):

<<>>=
core_speakers <- c(
  "dale", "darnell", "lisa", "luke", "michael",
  "mohamed", "rachel", "rebecca", "rex", "sara", "stuart"
)

vot_voiced_core <- vot %>%
  filter(
    speaker %in% core_speakers & syll_length == 1 & voicing == "voiced"
  ) %>%
  droplevels()
@

    Our research question will be: Is there a (population-level) speaking rate effect (predictor \ttt{speaking\_rate\_dev}) for voiced stops?   This is of interest for theories of the phonetics-phonology interface, %\citep[e.g.,][]{kessinger1997effects}, 
    %% FUTURE: JPK notes -- very unlikely to actually be zero.  Be precise.  (maybe I mean effectively zero, versus meaningfully negative?)
    and the `real' answer is actually unclear: the true effect size may be either zero, or negative (but small). 
Consider the following two models of VOT: both contain a reasonable set of predictors for this case (fixed effects), and differ in random effects---intercepts only, versus intercepts plus a random slope of the predictor of interest (\ttt{speaking\_rate\_dev}):\footnote{\label{fn:speaking-rate-dev} In this chapter `speaking rate' usually means column \ttt{speaking\_rate\_dev}, which is speaking rate relative to the speaker's mean rate, rather than column \ttt{speaking\_rate}, which is confounded with the speaker's mean rate (see )Section~\ref{sec:uncorr-ranef-preds}).  
The fixed effects in  models \ttt{vot\_mod\_1} and \ttt{vot\_mod\_2} are a subset of those in the full model of this data reported in \citet[][Table A1]{sonderegger2017medium}, to keep things simple.}

<<>>=
vot_mod_1 <- lmer(log_vot ~ speaking_rate_dev + foll_high_vowel +
  cons_cluster + log_corpus_freq + place + gender + 
    (1 | word) + (1 | speaker), data = vot_voiced_core)

vot_mod_2 <- lmer(log_vot ~ speaking_rate_dev + foll_high_vowel +
    cons_cluster + log_corpus_freq + place + gender + 
    (1 | word) + (1 + speaking_rate_dev || speaker), 
  data = vot_voiced_core)
@

The output for the model with the random slope is:

<<output.lines=14:32>>=
summary(vot_mod_2)
@

Compare the speaking rate effect in this model (row \ttt{speaking\_rate\_dev}) and the intercepts-only model:

<<>>=
filter(tidy(vot_mod_1), term == "speaking_rate_dev") %>%
  select(-group)
@

Again, the fixed-effect estimate is similar in each model (around -0.04), but its standard error is higher when a random slope is added, leading to a lower $|t|$ value (\Sexpr{tidy(vot_mod_2) %>% filter(term=='speaking_rate_dev') %>% pull(statistic)}, vs. \Sexpr{tidy(vot_mod_1) %>% filter(term=='speaking_rate_dev') %>% pull(statistic)}). 
By the $|t|>2$ heuristic we would conclude there is a significant speaking rate effect for the intercepts-only model but not for the intercepts+slope model.   It is not clear what the `real' \ttt{speaking\_rate\_dev} effect is for this case; the two possibilities illustrate the two possible outcomes of adding a random slope.  

\subsubsection{Type I/II error tradeoff}

If the true effect is null, then the effect detected in \ttt{vot\_mod\_1} is spurious---a \textbf{Type I error}.  From the perspective of avoiding Type I errors, including random slopes is a good thing.   In general   participants (and items) will differ in the effect of a given predictor $x$.  Thus, if a by-participant (or item) random slope for $x$  is not included, we are underestimating the uncertainty in the fixed effect for $x$, and can easily falsely conclude there is a significant effect---for example, based on a subset of participants who show large effects. In general, not including a possible random slope for $x$ is \textbf{anti-conservative} (for evaluating whether the fixed-effect coefficient of $x$ is 0).  

%Similar logic holds for by-item random slopes, and so on.  Whenever a predictor $X$ could vary among levels of a grouping factor $Z$, it is anti-conservative to not include a by-$Z$ random slope of $X$. 

On the other hand, if the true effect is not null, then \ttt{vot\_mod\_2} fails to detect it---a  \textbf{Type II error}. In other words, adding a random slope is \textbf{conservative}: it tends to lower power to detect a true (fixed) effect of $x$.  This is especially true if there is little by-participant (or item, etc.) variability in the effect of X.

\subsection{Random-effects model selection: Basics}
\label{sec:ranef-selection-1}

So adding a random slope term can be good (fewer Type I errors) or bad (lower power).  There are two corresponding philosophies for deciding on random-effect structure, which in practice usually means what random slope terms (and correlations: Section~\ref{sec:ranef-corrs}) to include in a model: 
\begin{itemize}

\item `Maximal': add as many random slopes as possible \citep{barr2013random}.
\item `Data-driven': add random slopes which significantly improve the model (e.g.,\ likelihood-ratio test with $\alpha=0.2$: \citealp{matuschek2017balancing})\footnote{\citet{matuschek2017balancing} discuss why $\alpha$ should be higher than 0.05, essentially to err towards more complex random effect structure,
%random-effect terms, 
and give some motivation for $\alpha = 0.2$ (though the exact value is arbitrary).}
% %and give some motivation for $\alpha=0.2$; the  given the purpose of model selection in this case, to assess ``the relative weight of model complexity and goodness-of-fit'', and suggest 
% %This sounds like model selection using AIC, which turns out to be equivalent to $\alpha=0.16$, so
% $\alpha=0.2$ is a convenient (if arbitrary) number that will select slightly more complex random-effect structures than AIC.}

% FUTURE: tie off discussion here from 7.7.2
\end{itemize}

We will return to this issue in a later chapter when discussing model selection (Section~\ref{sec:lmm-model-selection}), where  choosing random-effect structure is the biggest issue in practice.  

Until then, we fit models assuming the random-effect structure is known. There are three important guidelines on random-effect structure that are necessary to fit models which make sense at all (a lower bar than choosing the `best' random-effect structure, in model selection):
%
% 
% - The point here is just show the fundamental issue, which turns out to have signiifcant practical consequences in model selection:    
%  
% - We'll return to this issue when discussing model selection--which random slope terms are included is the biggest issue in practice.
% 
% ---
% 
% Until then, there are two  guidelines on random effect structure that are necessarily so we don't fit models that don't make sense: 
%
\begin{enumerate}

\item If the data is grouped by $z$ (e.g.,\ participants, items), a by-$z$ random intercept term should be included. 

\item Consider all possible random slope terms for effects of theoretical interest---either by including them in the model (`maximal' perspective) or testing whether they should be added (`data-driven' perspective).   Considering random slopes for predictors not directly related to research questions (`controls') is less important.


\item If a random slope for an interaction is included,  the corresponding random slopes for all subsets of the interaction should be included---for the same reason that you must include all subsets of an interaction as fixed effects (Box~\ref{box:lower-order-terms}). (For example, a by-participant \texttt{x1:x2} random slope would mean that by-participant \texttt{x1} and \texttt{x2} random slopes must be included.)
%\footnote{In special circumstances it can make sense to remove lower-order random slopes or random intercepts (\citealp{barr2013interactions}; \citealp[][16]{brauer2018linear}).}

\end{enumerate}

This requires clarifying two issues: what is a possible random slope (below), and how to test a random-effect term's contribution to a model, which we address in Section~\ref{sec:lmm-ht-ranef}.

\subsection{Possible and impossible random slopes}
\label{sec:possible-random-slopes}

Which random slope terms are possible for a predictor $x$ depends on its level (Section~\ref{sec:predictor-types}).  A by-speaker random slope of $x$ models speaker variability in the effect of $x$. Thus, this term only makes sense if $x$ \textbf{can} vary within speakers---it is not speaker-level.  More generally, by-$z$ random slopes are only possible for predictors which are not $z$-level.  (See \citealp[][13--15]{brauer2018linear} and \citealp{barr2013random} for more details.)
%discussion of possible random effects.)

For example, consider the speaker \ttt{gender} predictor in model \ttt{vot\_mod\_2}, which estimates the difference between male and female speakers.  The effect of this predictor cannot vary within speakers---each speaker has only one value of gender, so it doesn't make sense to estimate ``difference between \tsc{male} and \tsc{female} for speaker 3''. Thus, there can be no by-speaker random slope term for \ttt{gender}. More generally, there can be no by-speaker random slopes for participant-level predictors.  By similar logic, there can be no by-item random slopes for item-level predictors, and so on.

In model \ttt{vot\_mod\_2}, \ttt{speaking\_rate\_dev} is observation-level, \ttt{gender} is speaker-level, and all other predictors are word-level. Thus the possible random slopes are:
\begin{itemize}

\item By-speaker: all predictors except \ttt{gender}

\item By-word: \ttt{speaking\_rate\_dev}, \ttt{gender}

\end{itemize}
Thus, the `maximal' model in this case, assuming uncorrelated random effects, would have the following random-effect structure:
<<eval=FALSE>>=
(1 + speaking_rate_dev + foll_high_vowel + cons_cluster + 
    log_corpus_freq + place || speaker) +
  (1 + speaking_rate_dev + gender || word)
@

We instead consider a much simpler model, following the guidelines above.
By the first guideline, we should consider by-speaker and by-word random slopes of \ttt{speaking\_rate\_dev}, the predictor of theoretical interest. The by-word random slope turns out to not significantly contribute to \ttt{vot\_mod\_2} (Exercise~\ref{ex:vot-mod2-ex2}). We follow the data-driven approach and do not include this term, proceeding instead with \ttt{vot\_mod\_2},  which contains only the single (by-speaker, \ttt{voicing}) random slope term.
%% Actually, it turns out to matter *how* you do the data-driven method here.
%% Could come back to this in chap. 10.
%% there are two ways to do `data-driven' model selection. In one you do basically forward selection, in arbitrary order; by this path, doing speaker random slope first, adding word random slope doesn't contribute.
%% but if you do the other, harder path -- at each step try adding *every* random slope not currently in -- you'd add both by-word and by-speaker random slope (see Brauer & Curtin p. 17)
%% Here we've stuck with the former for simplicity...

\subsection{Multiple random slopes}
\label{sec:multiple-random-slopes-1}

%% needed to add this section because we've taken out explicitly building a `maximal' model, which was done in QMLD.

%The `maximal' model example above contains many random slopes.  
In this chapter we mostly consider models with just one random slope to keep things simple. %as we introduce mixed models. 
But models of real data typically contain multiple random slopes, so it is worth having an %(simpler) 
example in mind to think through how methods we cover apply to this case.

Consider the same model as \ttt{vot\_mod\_2}, but with all possible random slopes added (uncorrelated) for terms whose fixed effects are near the $|t|=2$ cutoff:

<<>>=
vot_mod_2_more_slopes <-
  lmer(log_vot ~ speaking_rate_dev + foll_high_vowel + cons_cluster +
      log_corpus_freq + place + gender +
      (1 + gender + speaking_rate_dev || word) +
      (1 + speaking_rate_dev + foll_high_vowel || speaker),
    data = vot_voiced_core)
@

This model's structure is somewhat arbitrary, to work with tools we have so far, but it could make sense as a candidate model if a research question was ``which factors significantly affect VOT?''

Note the notation used for multiple random slope terms: \ttt{(1 + A + B || speaker)} means ``by-speaker random intercept, random slope of A, and random slope of B, all uncorrelated''.

This model assumes six kinds of variability:
\begin{itemize}

\item Speakers differ in overall VOT, and in the effects of \ttt{speaking\_rate\_dev} and \ttt{foll\_high\_vowel}.

\item Words differ in overall VOT, and in the effects of \ttt{speaking\_rate\_dev} and speaker \ttt{gender}.

\end{itemize}

The new model's ``Random effects:'' block contains a row for each:

<<output.lines=15:24>>=
summary(vot_mod_2_more_slopes)
@


Comparing the output of this model to \ttt{vot\_mod\_2} (not shown---Exercise~\ref{ex:vot-mod2-ex}):

<<eval=FALSE>>=
tidy(vot_mod_2)
tidy(vot_mod_2_more_slopes)
@

We see that the model with multiple random slopes predicts substantial by-speaker variability in the effect of a following high vowel on VOT, and by-word variability in the effect of speaker gender.  Accordingly,  the standard errors of both fixed-effect terms (\ttt{foll\_high\_vowel}, \ttt{gender}) increase.

% \section{Evaluating linear mixed models}

\section{Hypothesis testing}
\label{sec:lmm-ht}

%As for non-mixed models, 
It is of interest for mixed models to obtain $p$-values corresponding to hypothesis tests that each fixed-effect and random-effect term is different from zero.  However these are not shown in the standard output of an \ttt{lmer()} model (see any \ttt{summary} or \ttt{tidy} output above).  Under \ttt{Random effects} there are no test statistics or $p$-values, and under \ttt{Fixed effects} there are test statistics but no $p$-values. 

%for example:
% 
% <<output.lines=10:22>>=
% summary(neut_mod_0)
% @

This is because how to calculate $p$-values for the contribution of a fixed or random-effect term to a model is somewhat contentious---especially for fixed-effect terms.\footnote{Essentially:
%which are typically of more interest.
 if mixed-effects models are thought of from a frequentist perspective it's unclear what $df$ should be for $t$/$F$ tests, and from a Bayesian perspective $p$-values are not a meaningful concept. For details see {lme4} documentation (\ttt{?pvalues}), \citet[][`Testing hypotheses']{glmmfaq}, and elsewhere  online (google ``{lme4} p-values'').}






% 
% - General discussion: 7.5.1
% 
% - up-to-date discussion of p-value options now in the lme4 manual: \url{https://rdrr.io/cran/lme4/man/pvalues.html}

The upshot is that for fixed or random effects, there are several methods to calculate $p$-values which trade-off between accuracy and computation time. 
It is good to be aware of these options and the issues in calculating $p$-values, as different methods come up in practice and computing them illustrates more generally-useful tools.  If you would like quick advice without going into details, you could read just about the `approximate $df$' methods (Section~\ref{sec:t-f-approximate-df}), which are a reasonable default  \citep[following][]{luke2017evaluating,meteyard2020best}.
%(personal opinion, elaborated in Box~\ref{box:p-values-mixed-opinion}).

It is important practical background for hypothesis testing that models with different fixed effects must be fitted using maximum likelihood (ML) for most model comparison methods to make sense (e.g.,\ likelihood-ratio test, AIC, BIC).  In all other cases REML fits are better (Box~\ref{box:reml-ml}). 


\begin{boxedtext}{Practical note: REML versus\ ML fits}
\label{box:reml-ml}

When using various functions to fit or compare (G)LMMs in R you may see messages related to refitting using ML.  This is because typical model comparison methods (likelihood-ratio tests, AIC, BIC) do not make theoretical sense for comparison of two LMMs \textbf{with different fixed effects} which were fitted using REML, because their likelihood functions are not comparable (e.g., \citealp[][\S10.2]{faraway2016extending}; \citealp[][\S2.4.5]{wood2017generalized}).
%\citealp[][XX]{zuur2009extensions}). 
Since this is the default, the models must be first refit using maximum likelihood (option \ttt{REML=FALSE}).  
%As of this current version of {lme4}, 
This is done automatically when you compare  two \ttt{lmer()} models with a likelihood-ratio test (\ttt{anova()}). Otherwise, for example when comparing non-nested models using AIC, you must refit the models yourself.   

Note that \textbf{any} model comparison of LMMs using \ttt{anova()} leads to the models being refit using ML by default, even in cases where REML fits would make more sense, like models differing only in random-effect structure.
%(Section~\ref{sec:lmm-ht-ranef}). 
(I think this is just because differing fixed effects is the most common case.)
%and it's too complicated for the \ttt{anova} function to figure out exactly which terms differ between models.  
In such cases you force a REML fit using \ttt{refit=FALSE} (see Section~\ref{sec:lmm-ht-ranef}).

While comparing \ttt{REML} fits when \ttt{ML} fits should be used doesn't make theoretical sense, in practice this usually just gives anti-conservative $p$-values (more so for smaller sample sizes).  Similarly, using \ttt{ML} fits when \ttt{REML} fits should be used gives conservative $p$-values or CIs.  So this technical-sounding issue is really just another grain of salt added to any $p$-values/CIs you report.

%\footnote{More precisely: to compare two models with different fixed-effect terms, using any likelihood-based method (such as a LR test or AIC), the models must be fitted using maximum likelihood \citep{faraway2016extending, zuur2009extensions}. See \href{https://stats.stackexchange.com/questions/116770/reml-or-ml-to-compare-two-mixed-effects-models-with-differing-fixed-effects-but}{here} for some discussion.}

\end{boxedtext}


\subsection{Hypothesis testing: Fixed effects}

% - For some reason this section was after ranef in QMLD; change text accordingly.

% - worth going into the different options, as they all come up in practice sometimes (and illustrate more generally-useful methods..)
% 
% - Knowles merTools docs quote (I like phrasing) ``... we have a number of options that represent a tradeoff between coverage and computation time.''

Options for  calculating \(p\)-values for fixed-effect terms include the following, listed in increasing order of accuracy and computation time:
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item Wald $z$-test
\item Likelihood-ratio test
\item $t$/$F$ tests with approximate $df$
\item Parametric bootstrap
\end{enumerate}

(1)--(4) are discussed in more detail, with a simulation study, by \citet{luke2017evaluating}. 

\subsubsection{Wald $z$-test}
\label{sec:wald-z-test}

The mixed-effects model table shows a $t$-statistic for each coefficient---its estimated value divided by its standard error.  It is unclear what the degrees of freedom should be for each $t$, which can be thought of as the number of independent `observations' that go into calculating the mean in the $t$-statistic.  $df$ could be thought of as $k$, because the mean is over $k$ groups (each of which has their own group mean), or $n-1$ (the answer in linear regression), the number of independent observations used to calculate the mean.  In reality $df$ is somewhere between $k$ and $n-1$, and can be approximated using different methods. 

% it's unclear how many independent pieces of information observations from $k$ groups give (e.g., $k$ participants): somewhere between $k$ and \(n-1\) (the answer in linear regression).

The first is to just assume that $df$ is large, so $t$ follows a normal ($z$) distribution, and calculate a $p$-value using a two-sided Wald $z$-test.  For example, for the fixed effects in model \ttt{vot\_mod\_2}:

<<>>=
tidy(vot_mod_2) %>%
  filter(effect == "fixed") %>%
  mutate(p = 2 * (1 - pnorm(abs(statistic))))
@

<<echo=FALSE>>=
vot_mod_2_tab <- tidy(vot_mod_2) %>%
  filter(effect == "fixed") %>%
  mutate(p = 2 * (1 - pnorm(abs(statistic))))

vot_mod_2_p_rate <- filter(vot_mod_2_tab, term == "speaking_rate_dev")$p
vot_mod_2_p_gender <- filter(vot_mod_2_tab, term == "gender")$p
@


For example, \Sexpr{formatP(vot_mod_2_p_rate)} for the \ttt{speaking\_rate\_dev} coefficient and  \Sexpr{formatP(vot_mod_2_p_gender)} for the \ttt{gender} coefficient.

This is not a great way to calculate a $p$-value---it is very approximate and in general the $p$-value will be anti-conservative (because $df$ is too high).  However, it has a useful corollary: observing that 95\% of a $t$-distribution with high \(df\) has \(|t|<2\) gives the rule of thumb that fixed effects with \(|t|>2\) are (roughly!) significant at the \(p<0.05\) level.

Thus, if \(|t|\) is much larger than 2 (say \(|t|>4\)), 
%and you only care about binary significance,
the effect ``is  significant'' and you don't need to bother using a more exact method to get a \(p\)-value.  Similarly if $|t|<1$ (say) the effect is unlikely to ``be significant'' no matter how $p$-values are computed.

% Recall that for a linear regression each row of the regression table shows a $t$-value ($\hat{\beta}/SE$), which follows a $t$ distribution with $df=n-1$ , where $n$ is the number of observations. Since a $t$-distribution is the same as a normal distribution for most datasets $n > 50$ (??) this is usually the same as a $p$-value from a $z$-test: just assuming $t$ follows a normal distribution (the \emph{$t$-as-$z$} approximation)

\subsubsection{Likelihood-ratio tests}
\label{sec:lr-test-mixed}

This works like testing the significance of 1+ terms for a logistic regression (Sec~\ref{sec:likelihood-logistic-reg}): fit models with and without $k$  terms, and compare them using a likelihood-ratio test (via \ttt{anova()} or \ttt{drop1()}). For large enough datasets, the difference in deviance between the two models follows a \(\chi^2(k)\) distribution under the null hypothesis that the fixed-effect coefficients (for the omitted predictors) are zero \citep[][\S2.4.2]{pinheiro2000mixed}.  Note that the $k$ terms could be fixed \textbf{or} random effects, so this test is very generally applicable. It is common in papers to use `likelihood-ratio test' to mean   this large-sample chi-squared test; we will often use this shorthand.\footnote{Technically  `likelihood-ratio test' could also mean
%ambiguous because
a hypothesis test using the likelihood 
%could be 
carried out using a more exact method, such as parametric bootstrapping.}
%but it is common practice to just use this as shorthand for the large-sample $\chi$-squared test.}

Likelihood-ratio test-based $p$-values are anti-conservative, but typically less so than Wald $z$-test $p$-values \citep[e.g.,][]{luke2017evaluating}. They take longer to compute because the two models must be refitted using ML (as their fixed effects differ) instead of REML.

\paragraph{Single terms}

For example, to test the contribution of the speaking rate and vowel height terms to \ttt{vot\_mod\_2}, separately:
%(full output not shown):

<<output.lines=7:9>>=
## Models are automatically refitted with ML
drop1(vot_mod_2, "speaking_rate_dev", test = "Chisq")
@
<<output.lines=7:9>>=
drop1(vot_mod_2, "gender", test = "Chisq")
@

Note that the $p$-values are slightly higher than those from Wald $z$-tests, as expected.


\paragraph{Multiple terms}

A likelihood-ratio test can also be used to test the contribution of multiple fixed-effect terms, e.g.,\ for a factor with 2+ levels. For example to test the contribution of \ttt{place} for our neutralization model:

<<output.lines=5:10>>=
## Refit model without the fixed effect of place
neut_mod_3_noplace <- update(neut_mod_3, . ~ . - place)
## Likelihood-ratio test
anova(neut_mod_3, neut_mod_3_noplace)
@

This would be summarized as ``\ttt{place} significantly contributes to the model, by a likelihood-ratio test (\Sexpr{lrTestReport(neut_mod_3, neut_mod_3_noplace)})''.
%$\chi^2(2)=15.7$, $p<0.001$)''.

<<echo=FALSE>>=
vot_mod_2_p_rate_lrt <- drop1(vot_mod_2, "speaking_rate_dev", 
  test = "Chisq")$`Pr(Chi)`[2]
vot_mod_2_p_gender_lrt <- drop1(vot_mod_2, "gender", 
  test = "Chisq")$`Pr(Chi)`[2]
neut_mod_3_p_place_lrt <- anova(neut_mod_3, 
  neut_mod_3_noplace)$`Pr(>Chisq)`[2]
@


\subsubsection{$t$/$F$-tests with approximate $df$}
\label{sec:t-f-approximate-df}

Another option to compute $p$-values uses the same logic as the Wald $z$ approach, but approximating $df$ rather than assuming $df = \infty$.  For single fixed-effect terms the \emph{Satterthwaite approximation} gives $df$ for  two-sided $t$-tests; for multiple terms it gives approximate $df$s for an $F$ test (as for linear regression: Section~\ref{sec:nested-model-comparison}).

These methods have become popular in part due to their handy implementation in the {lmerTest} package, which redefines the \ttt{lmer()} command to calculate (Satterthwaite) $df$ and $p$-values and show them in the model summary for each fixed effect, as well as in the output of \ttt{anova()} applied to a model.
%satisfying users' intuitive desire for $p$-values.  

The Satterthwaite approximation requires calculating additional information about the fitted model.\footnote{Intuitively, the Satterthwaite approximation estimates how large $df$ should be given the different sources of information that go into the estimate of a fixed effect, based on combining the estimated variances (= uncertainty) of each source.  For example for a participant-level predictor $df$ will be similar to the number of participants, all else being equal.
%(See e.g.,\ \url{https://www.reddit.com/r/statistics/comments/cuj33z/q_could_someone_eli5_how_the_satterthwaite/}.) 
These estimates require evaluating complex partial derivatives on the fitted model \citep{lmerTest}.}  Hence these $p$-values take a bit longer to compute than those from Wald $z$-tests or likelihood-ratio tests (see \ttt{?lmerTest}), but are more accurate (less anti-conservative).   
For example, to add $p$-values for each coefficient of the \ttt{vot\_mod\_2} model:

<<output.lines=22:33>>=
vot_mod_2_lmerTest <- lmerTest::lmer(log_vot ~ speaking_rate_dev + 
    foll_high_vowel + cons_cluster + log_corpus_freq + place + gender + 
    (1 | word) + (1 + speaking_rate_dev || speaker),
  data = vot_voiced_core)
summary(vot_mod_2_lmerTest)
@

(Note that because {lmerTest} redefines the \ttt{lmer()} command, we manually set \ttt{lmer()} to mean the {lme4} version at the beginning of the chapter, and now have to use \ttt{lmerTest::lmer()} here to get the {lmerTest} version.)

To test the contribution of \ttt{place} to the \ttt{neut\_mod\_3} model:

<<output.lines=5:6>>=
neut_mod_3_lmerTest <- lmerTest::lmer(vowel_dur ~ voicing + 
    prosodic_boundary + place + vowel + (1 + voicing || subject) +
    (1 + voicing || item_pair), data = neutralization)
## Uses an F test when applied to an lmerTest model
drop1(neut_mod_3_lmerTest, "place")
@


These $p$-values are in general higher than those from Wald $z$-tests or likelihood-ratio tests because they are more conservative (and more accurate, hence Type I errors are less likely).  For example, compare the $p$-values in this section for \ttt{speaking\_rate\_dev}, \ttt{gender} (in model table)  and \ttt{place} (from $F$-test) to the $p$-values from likelihood-ratio tests in the previous section.

Satterthwaite-based $p$-values are a good default for calculating $p$-values for fixed-effect terms in linear mixed-effects models, as an easy and reasonably accurate option \citep[][]{luke2017evaluating,meteyard2020best}.
%(Box~\ref{box:p-values-mixed-opinion}).

A couple practical aspects of using {lmerTest}:
\begin{itemize}
\item
  After loading {lmerTest}, your \texttt{lmer()} models will take about twice as long to run. This is only an issue if you are fitting complicated models or analyzing large datasets.
  \item Loading {lmerTest} (after loading {lme4}) redefines the \ttt{lmer()} command, so the fitted models are now of type \ttt{lmerModLmerTest} rather than \ttt{lmerMod}. 
This usually doesn't matter, but sometimes the small differences between these model types cause errors in a helper function that `should' work on an \ttt{lmer()} model.
\item If you want to use the {lme4} version of \texttt{lmer()} {after} loading {lmerTest} (e.g., to debug such an error, or to run a model that doesn't take twice as long), you use \texttt{lme4::lmer()}.
\end{itemize}

\begin{boxedtext}{Practical note: Testing the contribution of multiple fixed (and random) effects}
\label{box:testing-multiple-effects}

%% FUTURE: rewrite; this para is hard to parse (a reader notes).  The way to rewrite is: above we showed how lmerTest can be used to test the contribution of a multi-level factor, using an F test with approximate df.  This is a special case of testing the contribution of *multiple terms*---fixed effects, or fixed and random effects, but lmerTest doesn't work for the general case.  This can instead done with an F-test with K-R df...
Testing the contribution of multiple fixed-effect terms is conceptually the same whether the terms are part of a factor (e.g.,\ \ttt{place} in the example above) or are not. But {lmerTest} appears to only have functionality for the former case.

To test the contribution of multiple terms more generally (whether fixed-effect or fixed- and random-effect terms), an $F$-test can be carried out using a different approximation to $df$ (the `Kenward-Rogers approximation') by model comparison using the \ttt{KRmodcomp()} function, from the {pbkrtest} package (see \citealp{pbkrtest} for mathematical details).
 
For example to assess the contribution of all fixed-effect terms to model \ttt{neut\_mod\_3} (analogous to the $F$-test reported at the bottom of an \ttt{lm()} model), we compare to a model with the fixed-effect terms removed:

<<output.lines=6:7>>=
neut_mod_3_noFixef <- update(neut_mod_3, . ~ . -
    voicing - prosodic_boundary - place - vowel)
KRmodcomp(neut_mod_3, neut_mod_3_noFixef)
@

\end{boxedtext}
% 
% 
% - Kenward-Roger approximation for $F$-test (from \ttt{pbkrtest} package)
% 
% <<>>=
% neut_mod_3_noplace <- update(neut_mod_3, . ~ . - place)
% KRmodcomp(neut_mod_3, neut_mod_3_noplace)
% @
% 
% ---

% in this example p-values *higher*, illustrating how LRT is anti-conservative in general (PB).

\subsubsection{Parametric bootstrap}
\label{sec:parametric-bootstrapping}

\emph{Parametric bootstrapping} (PB) is a very accurate and very slow method for calculating $p$-values. One useful implementation of PB is in the {pbkrtest} package.
For more general PB computations the \texttt{bootMer()} function of {lme4} can be used.\footnote{Refitting the model with \ttt{mixed()} from the afex package is a useful shortcut to obtain PB $p$-values for all terms.}

Roughly, what PB is doing is simulating many new datasets from your model, assuming a given fixed-effect coefficient is set to {zero}, but all other coefficients are kept at their fitted values \citep[][\S5]{pbkrtest}. 
It then fits the original model to each of the new datasets, resulting in a distribution of values for the fixed-effect coefficient of interest. The proportion of these refitted models where the $t$-statistic of the coefficient (which {should} be zero) is as large or larger than the original estimate of $t$ for this coefficient---the first time you ran the model, on the real data---is the $p$-value. This is a  direct implementation of the meaning of Type 1 error (``how often would I wrongly conclude the coefficient was at least this large, if I redid the experiment many times?'').

Thus, PB effectively needs to re-run your model \texttt{nsim} times, and the higher \texttt{nsim} is, the more accurate the $p$-values are.  If your machine has multiple cores it is a good idea to use them here to speed up computation.\footnote{The code example uses all cores on my machine, which may not be a good idea if you have other programs running. A reviewer suggests always leaving two cores free (\ttt{nc <- max(1,detectCores()-2)}).}  For example, to calculate $p$-values for the \ttt{speaking\_rate\_dev} and \ttt{gender} terms for model \ttt{vot\_mod\_2}, with \ttt{nsim=1000}:



<<pb-sim-1, echo=FALSE>>=
library(parallel)
nc <- detectCores()
cl <- makeCluster(rep("localhost", nc))

f_name_1 <- "pb_vot_1.rds"
f_name_2 <- "pb_vot_2.rds"
f_name_3 <- "pb_neut_1.rds"

f_path_1 <- paste("objects/", f_name_1, sep = "")
f_path_2 <- paste("objects/", f_name_2, sep = "")
f_path_3 <- paste("objects/", f_name_3, sep = "")


do_sim <- !file.exists(f_path_1) || !file.exists(f_path_2) || !file.exists(f_path_1) || !file.exists(f_path_3)


if (do_sim) {
  vot_mod_2_no_rate_fixef <- update(vot_mod_2, ~ . - speaking_rate_dev)
  vot_mod_2_no_gender_fixef <- update(vot_mod_2, ~ . - gender)

  ## Takes a while to run with default nsim=1000
  pb_vot_1 <- PBmodcomp(vot_mod_2, vot_mod_2_no_rate_fixef, cl = cl)
  pb_vot_2 <- PBmodcomp(vot_mod_2, vot_mod_2_no_gender_fixef, cl = cl)
  pb_neut_1 <- PBmodcomp(neut_mod_3, neut_mod_3_noplace, cl = cl)

  saveRDS(pb_vot_1, file = f_path_1)
  saveRDS(pb_vot_2, file = f_path_2)
  saveRDS(pb_neut_1, file = f_path_3)
} else {
  pb_vot_1 <- readRDS(f_path_1)
  pb_vot_2 <- readRDS(f_path_2)
  pb_neut_1 <- readRDS(f_path_3)
}
@


<<eval=11:12>>=
library(parallel)
nc <- detectCores()
cl <- makeCluster(rep("localhost", nc))

vot_mod_2_no_rate_fixef <- update(vot_mod_2, ~ . - speaking_rate_dev)
vot_mod_2_noFhvFixef <- update(vot_mod_2, ~ . - gender)
## Parametric bootstraps
pb_vot_1 <- PBmodcomp(vot_mod_2, vot_mod_2_no_rate_fixef, cl = cl, 
  nsim = 1000)
pb_vot_2 <- PBmodcomp(vot_mod_2, vot_mod_2_noGender, cl = cl, 
  nsim = 1000)
## Full output of PBmodcomp
pb_vot_1$test
@
<<>>=
## Just the p-value for the PB test
pb_vot_2$test["PBtest", "p.value"]
@

%The first few lines enable parallel processing, using all of my laptop's available cores (8). 
The \ttt{PBmodcomp} calls take about 5 minutes, compared to a few seconds for previous methods.

The same parametric bootstrapping idea can be used to calculate a $p$-value for multiple fixed-effect terms (new datasets are now simulated assuming all these terms' coefficients are 0), e.g.,\ for \ttt{place} in model \ttt{neut\_mod\_3}:

<<eval=2>>=
pb_neut_1 <- PBmodcomp(neut_mod_3, neut_mod_3_noplace, 
  cl = cl, nsim = 1000)
pb_neut_1$test["PBtest", "p.value"]
@




\begin{boxedtext}{Broader context: $p$-values for mixed models}
\label{box:p-values-mixed-opinion}


% Calculating $p$-values for mixed-effects models is surprisingly contentious, for practical and philosophical reasons. The short version is that it is unclear  what the degrees of freedom should be to calculate $t$/$F$ tests (as discussed further in the text), and  mixed-effects models can be thought of as a kind of Bayesian model, and in Bayesian statistics $p$-values are not a meaningful concept.  Such issues are discussed at length
% in the {lme4} documentation (\ttt{?pvalues}), \citep[][`Testing hypotheses']{glmmfaq}, and elsewhere  online (just google ``{lme4} p-values''), the upshot being that there are several ways to define and calculate $p$-values.  


We have described several ways to calculate $p$-values which are used in practice and recommended `approximate $df$' methods as a default. But does it actually matter which method you use,
%to calculate $p$-values, 
especially for fixed effects?  Yes and no. 

% Thepractical” is: if we want to use a t test to calculate the significance of fixed effects—as we did for
% regression coefficients in a non-mixed-effects linear regression—it is unclear what the degrees of freedom should be, because it’s
% unclear how many independent pieces of information n observations from the same group level (e.g., from a single participant)
% give: somewhere between 1 (because they’re all from 1 participant) and n − 1 (the answer in linear regression). 

% All this discussion is one reason many people find mixed-effects models intimidating, or suspicious---it seems like a very basic issue is under debate.  So, does it actually matter which method you use to calculate $p$-values, especially for fixed effects (which are typically of most interest)?  Yes and no. 

Simulation studies \citep[e.g.,][]{luke2017evaluating} and theory \citep{pinheiro2000mixed} suggest that slower methods give $p$-values that are more accurate and less anti-conservative, especially for small enough $df$.  For mixed-effects models $df$ is smallest for participant/-item level effects, where it is primarily determined by 
%an effect is determined 
the number of groups (and secondarily by observations per group). My intuition is $df$ is `small enough' for many linguistic datasets  (e.g., less than 24 participants \textbf{or} items, in Luke's simulations; $<$40 levels is `small' in \citealp[][\S6.2]{snijders2011multilevel}).   Since we typically care more about Type I error, this seems like an important issue. 

% On the one hand: different methods tend to give different answers only for small enough $df$, and maybe $df$ is typically large.  This is the opinion of Barr et al.,  For mixed-effects models $df$ for an effect is determined by the numbe of groups and of observations (per group, and overall), which makes intuitive sense---for example, certainty about the a \ttt{foll\_high\_vowel} effect for the \ttt{vot} data should depend on the number of words in the data.  For example, a typical rule of thumb \citep[][\S6.2]{snijders2011multilevel} is that the method used matters for a $z$-level predictor if there are less than 40 groups for $z$.  S a likelihood-ratio test works fine for $p$-values for a $z$-level predictor if there 
% 
% 
% vowel height effect should depend on the number of words in the data. Ex: typical rule of thumb (S\&B 6.2) is LRT works fine for $p$-value for $Z$-level predictor if there are at least *40* groups for $Z$.   So unless you have a truly large and balanced dataset it could matter. Barr et al suggest for ling data often `large' enough this issue doesn't matter. i disagree, if we are taking all kinds of ling data into account -- copora, etc.  

However, $p$-values from different methods are rarely hugely different.  For example, in the simulations by \citet{luke2017evaluating}, which 
%a paper dedicated to
%examine this issue 
use the four methods covered here, `good' and `bad' methods give Type I errors of 0.045 to 0.08 at worst (compared to a `correct' value of $\alpha = 0.05$). You can also verify that the $p$-values we obtained by four methods for fixed-effect terms for the VOT model are similar---all within one decimal place of each other  (for the same term). This is not nothing, but small enough that if 
%(e.g., $p=0.01$ vs. $p=0.08$), 
%but not huge.   From this perspective, if 
it really matters what method you are using, your finding may be shaky anyway. 

Our practical advice is to take $p$-values with a grain of salt, as always, and try to use a `more accurate' method in the final write-up of your results.
%and use `approximate $df$' methods as a default. 

% 
% As a concrete example, this table  compares the $p$-values we obtained by four methods for the same fixed-effect terms for the VOT model:
% 
% {\footnotesize
% \begin{tabular}{llll}
% \toprule
% & \ttt{speaking\_rate\_dev} & \ttt{gender} & \ttt{place}  \\
% Method & $k=1$ & $k=1$ & $k=2$ \\
% \midrule
% Wald $z$ & \Sexpr{formatP(vot_mod_2_p_rate, pPrefix=FALSE)} & \Sexpr{formatP(vot_mod_2_p_gender, pPrefix=FALSE)} & N/A \\
% LR test (ML fits) & \Sexpr{vot_mod_2_p_rate_lrt} & \Sexpr{vot_mod_2_p_gender_lrt} & \Sexpr{neut_mod_3_p_place_lrt} \\
% Satterthwaite/KR & 
% \Sexpr{filter(tidy(vot_mod_2_lmerTest), term=='speaking_rate_dev')$p.value}
% & 
% \Sexpr{filter(tidy(vot_mod_2_lmerTest), term=='gender')$p.value}
% & 
% \Sexpr{drop1(neut_mod_3_lmerTest, 'place')$`Pr(>F)`}
% \\
% Parametric bootstrap & \Sexpr{pb_vot_1$test['PBtest','p.value']}
%  & \Sexpr{pb_vot_2$test['PBtest','p.value']}
%  & \Sexpr{pb_neut_1$test['PBtest','p.value']} \\
% \bottomrule
% \end{tabular}
% }
% 
% Note that the $p$-values are always within one decimal place of each other (for the same term). This is not nothing (e.g., $p=0.01$ vs. $p=0.08$), but not huge.  From this perspective, if it really matters what method you are using, your finding may be shaky anyway.  

% Our practical advice is to just take $p$-values with a grain of salt, as always, and try to use a `more accurate' method in the final write-up of your results.  Wald $z$ and likelihood-ratio tests are anti-conservative, while parametric bootstrapping is accurate but very slow. A sensible default, following \citep{luke2017evaluating,meteyard2020best} is $t$/$F$ tests with Satterthwaite/Kenward-Roger approximated $df$ for fixed-effect terms (i.e.,\ {lmerTest}), which lies between these.
%For random effects a sensible default is XX.
% 
% - No: it's also the case that p-values from these different methods aren't *that* different (e.g., in Luke simulations we're talking p=0.08 instead of 0.05, at worst).  So just take p-values with a grain of salt, as always, and try to use an accurate method when actually writing up.
\end{boxedtext}

% 
% 
% - Simulation studies suggest that slower methods here give *more accurate* p-values. Note that this also corresponds to *higher* p-values -- here, more accurate = less anti-conservative.  So it's an important issue to bear in mind when writing up results to avoid Type I errors.

% So, which to use? Wald $z$ and LRT are anti-conservative. However PB takes forever.  So *Satterthwaite* is a reasonable option for single terms---by far the most common case.

% combine w/ ``actual practical advice'' Box notes from above. But need to figure out what goes in the text versus the box -- what is *essential* for reader?
% 
% ---




%% p-values:

%% single:
%% Wald: 0.062, 0.019
%% LRT: 0.091, 0.022
%% Satterthwaite: 0.09, 0.022
%% PB: 0.084?, 0.023?

%% multiple:
%% LRT: 0.00039?
%% KR: 0.00293?
%% PB: 0.00599?



%% 
%%
%%


\subsection{Hypothesis testing: Random effects}
\label{sec:lmm-ht-ranef}

% 
% - adapt 7.5.2  It's actually more complicated than you make it sound...  
% 
% - (on the reviewer's comments -- see your notes on REML vs ML model comparison)

As for fixed effects there are several options which trade off accuracy and computation time: likelihood-ratio test, exact LRT, and parametric bootstrapping.  We use as an example testing the contribution of the by-speaker random slope of \ttt{speaking\_rate\_dev} to \ttt{vot\_mod\_2}---intuitively, ``does the speaking rate effect significantly differ by speaker?''  

\subsubsection{Likelihood-ratio test}
\label{sec:ranef-lrt}

By the logic from Section~\ref{sec:lr-test-mixed}, models differing in $k$ random-effect terms can be compared using a likelihood-ratio test, which gives an approximate $p$-value (more accurate for larger samples).  

For example, for the \ttt{speaking\_rate\_dev} random slope:
<<output.lines=5:7>>=
anova(vot_mod_1, vot_mod_2, refit = FALSE)
@


This could be reported as ``adding a by-participant random slope for speaking rate significantly improved the model, by a likelihood-ratio test (\Sexpr{lrTestReport(vot_mod_1, vot_mod_2, refit=FALSE)})''.

The \ttt{refit=FALSE} flag makes the model comparison use REML fits, which  have better estimates of the variance components \citep[e.g.,][\S5.7]{zuur2009extensions}.
%\footnote{Note that by default (and in many papers) \ttt{anova} will give a $p$-value from comparison of ML fits, which will tend to be conservative (because ML underestimates the degree of variability): \Sexpr{anova(vot_mod_1, vot_mod_2)$`Pr(>Chisq)`[2]} in this case.}


% 

% 
% 
% - From worse to better: LRT default, LRT with REML, exact LRT.
% 
% - maybe replace this example with by-sub slope in VOT model? needed below
% 
% - Example: to test whether the by-item intercept, slope contribute to model likelihood, in neut\_mod3 :
% 
% <<>>=
% neut_mod_3_noSubSlope <- update(neut_mod_3, . ~ . - (0+voicing|subject))
% neut_mod_3_noSubInt <- update(neut_mod_3, . ~ . - (1|subject))
% 
% anova(neut_mod_3, neut_mod_3_noSubSlope)
% anova(neut_mod_3, neut_mod_3_noSubInt)
% @

% - Issue 1: poor estimates of variance compoennts using ML, which affects accuracy of hypothesis test. Typically biased towards 0 $\rightarrow$ conservative ($p$-value too large). Minimally, use REML rather than ML fits (Zuur et al):
% 
% <<>>=
% anova(neut_mod_3, neut_mod_3_noSubSlope, refit=FALSE)
% anova(neut_mod_3, neut_mod_3_noSubInt, refit=FALSE)
% @


This method doesn't work for testing the significance of a random effect in a model with just {one} random effect, such as model \ttt{neut\_2\_mod\_1} (Section~\ref{sec:fitting-interpreting-model}), because the models being compared are not of the same type (one is a mixed model and one isn't, so the likelihoods are not directly comparable).   In this case, the exact restricted likelihood-ratio test (discussed below) must be used.  
%This is not a common case.

\subsubsection{Exact LRT}
\label{sec:exact-lrt}

The likelihood-ratio test turns out to give \textbf{conservative} $p$-values ($p$-values too high) when comparing models differing in random-effect terms.\footnote{This is related to the null hypothesis (e.g., $\sigma_{P,1}=0$) being on the boundary of the hypothesis space
%the reason why is not intuitive 
\citep[][\S2.4.1]{pinheiro2000mixed}. The idea is that because a variance component can only be positive, a $p$-value computed using a two-sided test is too high.}
% because the  doesn't make conceptual sense for random effect terms because the null hypothesis (e.g., $\sigma_{P,1}=0$) is on the boundary of the hypothesis space (REF: Pinheiro/Bates).   Intuitively, a likelihood-ratio test is two-sided (the parameter being tested could go up or down from zero), and thus gives conservative $p$-values (in reality the parameter can only go up).
% FUTURE: check that understanding and shorten
% 
% - Issue 2: can't really do a traditional hypothesis test because null is on the boundary of hypothesis space (P/Bates).  Intuitively, LRT is two-sided and thus conservative.

More accurate $p$-values can be calculated for excluding $k$ random-effect terms using an `exact' likelihood-ratio test, by simulation, implemented in the {RLRsim} package.  This test (in \ttt{exactRLRT})   requires three models: the models with and without the $k$ terms (as for a normal LRT), and the model with only these terms.  For example for the \ttt{speaking\_rate\_dev} slope:
% - Solution, takes longer: simulation-based `exact' restricted LR tests from RLRSim.  Uses three models: full, without this ranef term, with *just* this ranef term.
<<>>=
## Remove random effects except the by-speaker speaking_rate_dev slope
vot_mod_2_justRateSlope <- update(vot_mod_2, . ~ . - 
    (1 | word) - (1 | speaker))
exactRLRT(vot_mod_2_justRateSlope, vot_mod_2, vot_mod_1)
@

This method takes a bit longer than the likelihood-ratio test (fitting an extra model, simulation) but is much more accurate.

% LRT (REML): 0.00997
% ELRT: p=0.0033
% PB: p=0.0123

% <<>>=
% ## intercept:
% neut_mod_3_justSubInt <- update(neut_mod_3, . ~ . - (1 | item_pair) - (0 + voicing | item_pair) - (0+voicing|subject))
% neut_mod_3_justSubSlope <- update(neut_mod_3, . ~ . - (1 | item_pair) - (0 + voicing | item_pair) - (1|subject))
% 
% exactRLRT(neut_mod_3_justSubInt, neut_mod_3, neut_mod_3_noSubInt)
% exactRLRT(neut_mod_3_justSubSlope, neut_mod_3, neut_mod_3_noSubSlope)
% @

%Either of the two more accurate methods (LRT on REML fits; exact LRT) may be used in practice  (Box~\ref{box:testing-random-effects}).

% FUTURE: exercise with the stuff commented out.


\begin{boxedtext}{Practical advice: Testing random effects}
\label{box:testing-random-effects}

Note how more accurate methods for the \ttt{speaking\_rate\_dev} slope example lead to lower $p$-values (LRT (ML) $>$ LRT (REML) $>$ exact LRT), as expected---they are less conservative.   A parametric bootstrap could also be used, and would presumably give an even more accurate $p$-value.


Which method should be used in practice?  Even when random effects aren't of direct interest, this matters because of the role such tests play in model selection (Section~\ref{sec:lmm-model-selection}). Using the `data-driven' approach, more conservative tests will give sparser random-effect structure, which will (typically) give more Type I errors for \textbf{fixed}-effect terms.

We advise using likelihood-ratio tests with REML by default (which is also the fastest method), and exact LRTs or parametric bootstrapping any time the results matter (e.g., ``do speakers differ?'' is a research question) or in model selection when $p$ is near the $\alpha$ cutoff. 

\end{boxedtext}

% - comparing $p$-values from these: better method $\rightarrow$ lower $p$-value, as expected.
% 
% - Practical advice:
% 
% - This matters because of the role such tests play in model selection -- using conservative tests will lead to sparser ranef structure, (typically) Type I errors.
% 
% - Advice: *REML* by default (also the fastest method); exactLRT any time the results matter (e.g.,\ ``do speakers differ?'' is a research question) or p is near cutoff (in ranef selection).

\subsection{Fixed and random effects}
\label{sec:lmm-fixed-and-random}

A final case is assessing the contribution of fixed- \textbf{and} random-effect terms in a model, which requires getting into an issue we'll return to below: what is the best way to assess ``does predictor $x$ significantly improve the model?''

When a model contains both a fixed effect and random slope(s) for $x$, the answer is not clear.  Testing the contribution of the fixed effect only (as above) is the most common method in language sciences/psychology, and makes sense as this is the aspect of how $x$ affects the response (`overall') that we care about---but this does not take into account variability in the effect among participants or items. It is possible for a predictor to show no {overall} effect (across participants and items), but to show significant variability among participants or items---indeed, this may be {why} there is no significant overall effect \citep[][12]{rights2019quantifying}.

An alternative method is to assess the contribution of $x$ by comparing the full model with a model where all terms involving $x$ are excluded---fixed and random effects.  There are several options for obtaining a $p$-value for this comparison which will (again) trade off in accuracy and speed: a likelihood-ratio test, an $F$-test using approximate $df$ (using Kenward-Roger: Box~\ref{box:testing-multiple-effects}), or a parametric bootstrap. 

For example, for our VOT case, one way to address the research question ``does speaking rate matter for voiced VOT?'' is to test whether the fixed-effect and by-speaker random slope for \ttt{speaking\_rate\_dev} contribute to model \ttt{vot\_mod\_2}.
%which we use as a running example.

We can obtain a $p$-value for this example using a likelihood-ratio test comparing \ttt{vot\_mod\_2} to the model with the two \ttt{speaking\_rate\_dev} terms excluded: 
<<output.lines=5:10>>=
## speaking_rate_dev fixed effect and by-speaker random slope excluded
vot_mod_2_noRate <- update(vot_mod_2, . ~ . - speaking_rate_dev -
    (0 + speaking_rate_dev | speaker))
anova(vot_mod_2, vot_mod_2_noRate)
@

(Note that using ML fits is appropriate here because the models differ in fixed-effect structure.) 


Thus, adding information about speaking rate \textbf{does} significantly improve the model (\Sexpr{lrTestReport(vot_mod_2, vot_mod_2_noRate)})---even though there is not a significant `overall' effect (\ttt{speaking\_rate\_dev} fixed effect $p>0.05$).  This reflects substantial speaker variability (as shown above: Section~\ref{sec:lmm-ht-ranef}). In this case which model comparison we perform changes how we would answer  the research question (``does speaking rate matter for voiced stops?'').

% - So adding info about speaking rate does signif improve the model, even though no overall effect. Arguably *because* there is a lot of speaker variability (need to show somewhere)

A more accurate $p$-value could be computed using a slower method---an $F$-test using Kenward-Roger $df$  or a parametric bootstrap---but $p$ is low enough that the qualitative conclusion will not change (Exercise~\ref{ex:kr-pb-repeat}).
% 
% 
% - More exact method: bootstrapping with PBmodcomp (leave as exercise?)
% <<eval=FALSE>>=
% PBmodcomp(vot_mod_2, vot_mod_2_noRate, cl=cl)
% @




% FUTURE (later, but put here -- has to do w fixed + random effects): exercise, using \ttt{vot\_mod2\_more_slopes}: gender effect.  Ex is to do some tests and then describe qualitatively what's going on (gender matters; small overall effect with huge variability by-word. looks like disproportionately d words?)


\subsection{Confidence intervals}
\label{sec:conf-int-lmms}

Confidence intervals are easier 
%and less controversial
to compute for  mixed-model parameters (fixed and random effects), using the \ttt{confint()} function.

The options again trade off in speed and accuracy:
\begin{itemize}
\item \emph{Wald $z$-based} CIs  ($\hat{\beta} \pm 1.96 \cdot SE$)---for fixed-effect coefficients only
\item \emph{Profile} CIs based on the likelihood profile (as for logistic regression: Section~\ref{sec:likelihood-logistic-reg})
\item \emph{Parametric bootstrap} CIs: computed from the distribution of values for the coefficient(s) of interest, across bootstrap samples
\end{itemize}

The default is profile CIs, which take some time to compute (seconds--minutes) but are reasonably accurate.

For example, consider the \ttt{voicing} effect in \ttt{neut\_mod\_3}, which is of primary theoretical interest, calculated in these three ways:
% 
% \paragraph{Fixed effects}
% 
% For fixed effects he easiest is Wald $z$-based CIs 
% 
% - Confidence intervals are less controversial and easier to compute, using the \ttt{confint} function.
% 
% - Again, methods from more$\rightarrow$less approximate/fast to compute.
% 
% - Wald CI: estimate pm 2 SE
% 
% - profile: as for logsitic regression (REF back to section), using likelihood ratiotest
% 
% - parametric bootstrap
% 
% - Example:  `voicing' effect CIs, by 3 methods:
% 
<<>>=
confint(neut_mod_3, parm = "voicing", method = "Wald")
confint(neut_mod_3, parm = "voicing", method = "profile")
@


<<eval=FALSE>>=
## use multiple cores
## takes ~2 min on my 8-core laptop
confint(neut_mod_3,
  parm = "voicing", method = "boot",
  nsim = 5000, parallel = "multicore", ncpus = nc
)
@
<<echo=FALSE>>=
f_name <- "ci_neut_1.rds"

f_path <- paste("objects/", f_name, sep = "")

do_sim <- !file.exists(f_path)

if (do_sim) {
  ci_neut_1 <- confint(neut_mod_3,
    parm = "voicing", method = "boot", 
    nsim = 5000, parallel = "multicore", ncpus = 8
    )

  saveRDS(ci_neut_1, file = f_path)
} else {
  ci_neut_1 <- readRDS(f_path)
}
ci_neut_1
@

% Wald: 5.95, 13.25
% profile: 5.85, 13.36
% boot: 6.26, 13.09 or 5.97/13.09
% (actually need to run with high nsim)

The two more accurate methods give (very slightly) wider confidence intervals than the Wald method, corresponding to less anti-conservative $p$-values.

Confidence intervals can also be computed for random effects (Box~\ref{box:ranef-conf-int}).

\begin{boxedtext}{Broader context: Confidence intervals for variance components}
\label{box:ranef-conf-int}

Reporting confidence intervals for random effects is currently not common in practice (in language sciences), but seeing an example illustrates some useful points.
95\% profile confidence intervals for the variance components of model \ttt{vot\_mod\_2} are:
<<>>=
## `parm', `oldNames' arguments gives CIs only for variance components
## and give them intuitive names.
confint(vot_mod_2, method = "profile", parm = "theta_",
  oldNames = F)
@

All three CIs don't include zero. This should be functionally equivalent to significant ($p<0.05$) likelihood-ratio tests of the contribution of each random-effect term to the model, while a 95\% confidence interval including 0 would correspond to a non-significant test.
% (Exercise FUTURE: verify this. neut\_mod3 example by-sub slope for n.s.)

Note how large the CIs are: for example the degree of speaker variability in the \ttt{speaking\_rate\_dev} slope could be small enough that speakers effectively all have the same directionality for this effect, or large enough that they differ roughly 50/50 in the effect's direction.  In general, accurately estimating the degree of variability in effects  (random effects) requires much more data than estimating population-level effects (fixed effects).

A corollary is that we should be cautious about drawing inferences from the random-effect estimates reported in \ttt{lmer()} output, if these relate to research questions.  For example suppose it is of interest whether speakers or words differ more in VOT (after controlling for speaker- and word-level predictors).  Since the estimated degree of
by-speaker intercept variability (\Sexpr{tidy(vot_mod_2) %>% filter(term=='sd__(Intercept)' & group=='speaker') %>% pull(estimate)}) is larger than the by-word value (\Sexpr{tidy(vot_mod_2) %>% filter(term=='sd__(Intercept)' & group=='word') %>% pull(estimate)}), we might be temped to conclude that speakers differ more than words---but the 95\% confidence intervals for the estimates overlap, meaning we can't be certain.

% FUTURE: do I remember right that it's actually unclear how good the random effect estimates are? (mixed models sometimes terrible?) If so should cite here and say bayesian model better.

% 
% - Large CI on rate slope, including 0 = functionally equivalent to n.s. test above
% 
% - Both by-subject and by-item well away from 0
% 
% - The whole random effect conf int part could go in a box -- doesn't currently come up in practice.

\end{boxedtext}


%% possibly somewhere box... or not
%%
%% why it's good to be aware of the issues here -- accuracy vs runtime -- instead of just `calculate p-values/confidence intervals using X method, always''
%% runtime is actually a big issue.  Even for small laboratory experiments, realistic random effects => minutes for each model to fit.  Once have larger samples -- like most corpus data -- can take much longer.
%% - in reality you always fit many models building up to a final model -- model selection, find errors, etc.  And sanity checks or model selection depend on metrics (e.g., p-values).
%% - So it makes a big practical difference to know: for sufficiently small p-values, none of this matters / can use any method.
%% - example: have had a reviewer tell me, you need to use parametric bootstrapping to calculate p-values.  For some corpus data, the final model takes 1-2 hours to fit on a laptop.  So fitting all the models to calculate PB p-values would take all day

\section{Model summaries}
\label{sec:lmm-model-summaries}

As for non-mixed-effects models it is useful to define quantitative summaries of the model---goodness of fit (like $R^2$) or measures of model likelihood/`information' (like AIC)---which are used in variable selection or reporting the model.

\subsection{Goodness of fit}
\label{sec:r2-lmm}

We would like a measure of goodness-of-fit for linear mixed-effects models (LMMs) which intuitively relates to ``proportion of variation explained'' (like $R^2$ for linear regression). There is no such single measure for LMMs, because they contain more than one kind of variation that can be explained: residual variation (error in each observation), variation in the intercept (e.g.,\ by speaker, item: random intercepts), variation in predictor slopes (random slopes), and so on.  Thus many $R^2$ measures can be defined,  and in general capturing ``goodness of fit'' requires more than one measure. This is an active research area (Box~\ref{box:r2-lmms}), but two measures are most commonly-used in practice.

These are \emph{marginal $R^2$} ($R^2_m$), intuitively the proportion of variance explained by fixed effects, and \emph{conditional $R^2$} ($R^2_c$), the proportion of variance explained by fixed and random effects \citep{nakagawa2013general,johnson2014extension}; both lie between 0 and 1 (no/all variance explained).  A reasonable default
%(as of 2021) 
is to report these two measures, which convey different information. If only fixed effects are of interest or space is limited, it could make sense to report just marginal $R^2$. 


\begin{boxedtext}{Broader context: $R^2$ measures for LMMs}
\label{box:r2-lmms}

\citet{rights2019quantifying} summarize the tangled literature on $R^2$-like measures for mixed-effects models,
%(`variance explained'), 
and discuss 
%in detail
why the measure used matters  (see also \citealp{snijders2011multilevel}), and
present 
%They develop 
a general framework which encompasses many slightly different $R^2$ measures proposed across different fields. \citet{rights2020new} extend this discussion to 
%the 
%more general 
%question of 
assessing the effect size of one or several predictors. 

% 
% - A useful recent review is by \citealp{rights2019quantifying}, who summarizes the tangled literature on diff $R^2$ measures and shows why the measure used matters for assessing models (intuitively, deciding ``how much does $X$ matter''?; also \citealp{rights2020new}). 

The central issue is whether only variance attributable to fixed effects is treated as `explained', or if random effects count as well (and if so how to calculate their contribution).  This is conceptually similar to the issue we saw in Section~\ref{sec:lmm-fixed-and-random} in defining whether a predictor $x$ ``has an effect'': Should we assess just its fixed effect, or by-$x$ random slopes as well?

Our `marginal $R^2$' and `conditional $R^2$' are columns 3 and 7 of \citet[][Table 4]{rights2019quantifying}.  $R^2_m$ is the proportion of variance in $y$ explained by just using the fixed-effect coefficient estimates,
%(i.e., making model predictions as if this were a linear regression with these coefficients), 
which can be thought of as `marginalizing' across random effects.  $R^2_C$ is the  proportion of variance explained when intercepts and slopes are adjusted using the estimated random effects (the BLUPs), i.e.,  model predictions are `conditioned' on random-effect values. 

% 
% These are the most commonly used (incl in lang sci).    
%  The idea behind the terminology is:  `marginal' R2 assesses model predictions after marginalizing across random effects; `conditional' R2 uses predictions conditioned on random effect values (the BLUPs). 
 
% Calculating $R^2_c$ requires estimating the contribution of different kinds of random effects, which is tricky to do correctly; thus some implementations in R packages do not work in all cases. In particular, as of this writing (mid-2021) the implementation which comes up first if you google relevant terms (in the {performance} package) is incorrect.

One option for $R^2$ which should probably \textbf{not} be used is the squared correlation of an LMM's predictions (\(\hat{y}_i\)) with the observations (\(y_i\)). This recipe is widely used (e.g., \citealp{glmmfaq}: `Simple/crude solutions'), and recommended in an earlier version of this book \citep[][\S7.5.4]{qmld}, but does not properly account for the contribution of random effects. (Intuitively, this measure uses BLUPs, which are not fitted model parameters, rather than the variance components, which are.)

% FUTURE: find ref? see comments
% 
% - Also sometimes used for R2 is the correlation between y and prediction. This probably isn't a good idea: uses BLUPs, which are biased (on purpose), not model parameters, and introduce additional source of error for each observation.  So you're computing a measure of model quality that uses non-fitted parameters (random effects) insead of fitted params (variance components).   (can cut this down)
%% (REF for this, I'm implying from Rights & Sterba 2018 p. 11, plus Snijders/Bosker somewhere... )

% notes: I think intuitively the issue is predicted outcomes for each observation are noisy because these use BLUPs — that’s a significant source of error. Whereas using just estimated variance components uses params actually estimated by the model.


\end{boxedtext}

% 
% 
% \begin{itemize}
% \item
%   Residual variation: error in each observation
% \item
%   Variation among speakers in intercept
% \item
%   Variation among speakers in the slope of $x$
% \item
%   etc.
% \end{itemize}


% 
% - Adapt 7.5.4
% 
% - built to definitions of commonly used R2: conditional, marginal

These can be calculated using  \ttt{r.squaredGLMM} from the {MuMIn} package.  For example, for the simplest model  of the \ttt{neutralization} data (\ttt{neut\_mod\_0}: Section~\ref{sec:two-grouping-random-intercepts}):
<<>>=
r.squaredGLMM(neut_mod_0)
@

Here  $R^2_m$ is low (near 0) because the single fixed effect  (\ttt{voicing}) explains little variance, while  $R^2_c$ is much higher because by-item and by-participant variability explain much of the variance in \ttt{vowel\_dur} in the data.  In other words, most variance is explained by the participant and item, but we don't know \textbf{why}: what properties of participants and items matter, or whether this is `random' variation.  $R^2_c$ is  often rather high for LMMs, simply because much
variability in the data comes down to (e.g.,) by-participant and by-item variability---even if the model would not actually have much predictive power on new data (unseen participants or items).
 
% (Note: as of this writing \ttt{performance} implementation doesn't work.)

To get a better sense of what the two $R^2$ measures mean, we compare their values for all four \ttt{neutralization} models fitted above (code hidden):

<<echo=FALSE>>=
## Make dataframe of R2m and R2c for each model,
## by applying r.squaredGLMM then row-binding results.
r2_df <- list(neut_mod_0, neut_mod_1, neut_mod_2, neut_mod_3) %>% 
  map_dfr(~as.data.frame(r.squaredGLMM(.)))

bind_cols(
  model = c("neut_mod_0", "neut_mod_1", "neut_mod_2", "neut_mod_3"),
  fixed = c("voicing", "v. + controls", "voicing", "v. + controls"),
  random = c("intercepts", "intercepts", "I + by-participant slope", 
    "I + by-par/item slopes"),
  r2_df
)
@

Comparing models 0 and 1: $R^2_m$ increases because the control predictors have been added (to \ttt{voicing}) as fixed effects. $R^2_c$ is similar because the total variance explained (by fixed+random effects) hasn't changed---the control predictors are mostly item-level, so some variance is now just explained by fixed instead of random effects.

Comparing models 1 and 3: $R^2_m$ is the same because the fixed effects are the same. Adding a random-effect term (the by-participant random slope) leads to a small increase in $R^2_c$.
% 
% - References here: \citet{rights2019quantifying} -- good review of R2 measures proposed for MMs and the relevant issues.  
% 
% - R2-marginal/conditional used in practice: \citet{nakagawa2013general}, extended to models w random slopes by \citet{johnson2014extension}.

\subsection{Information criteria}
\label{sec:lmm-information-criteria}

As for non-mixed models (Section~\ref{sec:non-nested-model-comparison}), various information criteria can be defined which quantify the trade-off between model likelihood ($L$) and model complexity ($k$: number of parameters); see \citet{vaida2005conditional,muller2013model} for details.  We consider, AIC, AICc, and BIC, which are defined using the same equations as for non-mixed models (equations~\ref{eq:AIC}--\ref{eq:AICc}). For example, AIC is still:
\begin{equation}
AIC = -2 \cdot L + 2 \cdot k
\label{eq:lmm-aic-1}
\end{equation}

How to define $L$ and $k$ depends on the ``level of focus'' at which predictions are being made \citep{vaida2005conditional,cAIC4}, similarly to defining marginal versus conditional $R^2$. 

If interest is in the fixed effects (prediction for an `average' participant/item), you use \emph{marginal AIC}, where $L$ is just the likelihood (ML or REML) of the fitted model and $k$ is the number of fixed-effect coefficients and variance components.  If interest is in the random effects (predictions for individual participants/items), you use \emph{conditional AIC}, whose definition is more complex; a correct implementation is in the {cAIC4} package (see \citealp{cAIC4} for discussion of this case).  These measures use  the `marginal likelihood' and `conditional likelihood';  see \citet[][\S13.3.2]{bolker2015linear} or \citet{glmmfaq} for an intuitive explanation.  Corrected (marginal) AIC (AICc), used for smaller samples,  can be calculated with \ttt{AICc()} from MuMIn.
Here we only consider marginal AIC, which is what is calculated by default for {lme4} models.
%presumably
%by the \ttt{AIC} function. Presumably this is 
%because fixed effects are often of primary interest.
%here we assume *marginal AIC*,  which is what is calculated by default (AIC method), presumably because fixed effects usually of primary interest (which is the case we're assuming in this chapter). 

% - How to define L and df depends on ``level of focus'' -- very similar to marginal vs. conditional R2---at which predictions are made (Vaida \& Blanchard 2005), which affecfts both L and df.  If interest is in the fixed-effects / ``average'' cluster--marginal AIC.    If interest is in the random effects / values for individual cluster levels (e.g., one subject)---conditional AIC.  This is discussed further in chap. 10; here we assume *marginal AIC*,  which is what is calculated by default (AIC method), presumably because fixed effects usually of primary interest (which is the case we're assuming in this chapter).  df = number of fixed effects + variance compoennts.

Similarly, (marginal) BIC is defined by substituting $\log(n)$ for 2 in equation \eqref{eq:lmm-aic-1}, and is the default for {lme4} models.
%BIC penalizes larger models more than AIC for any sample size ($n$).   AICc (`corrected AIC') is defined by substituting $k$ and $L$`Corrected AIC' (AICc; use \ttt{AICc} in \ttt{MuMIn}) is a variant of AIC, recommended for small enough sample size ($n/k < 40$: \citealp{burnham2002model}), which penalizes more than AIC and less than BIC, and reduces to AIC for large $n$.\footnote{\citet{burnham2002model} argue AICc should be the default, as AIC = AICc anyway if the dataset is large enough.}
%and it is the most widely used option for ecological data  using (G)LM(M)s (REF Gruber et al 2011). It should probably be more widely used for linguistic data.}  
Many other *IC options exist (e.g., DIC, a Bayesian interpretation of AIC: \citealp[][\S24.3]{gelman2007data}).

% TODO: change Chap 5 on model criteria to incorporate AICc; then here, can make discussion shorter.  Also now have references to cite in Chap 5 -- basically can see ecological literature for these issues.

%Many other *IC variants exist, such as DIC, a  
%Finally,  a popular option for mixed-effects models is DIC, which is a Bayesian interpretation of AIC \citep[][\S24.3]{gelman2007data}, implemented in the \ttt{arm} package.
As for non-mixed models, when used for model selection, AIC/AICc/BIC tend to select progressively smaller (mixed-effects) models.

For example, to calculate the three criteria for the neutralization models \ttt{neut\_mod\_0} and \ttt{neut\_mod\_2}, which differ in whether the by-speaker random slope of voicing is included:

<<>>=
AIC(neut_mod_0, neut_mod_2)
MuMIn::AICc(neut_mod_0, neut_mod_2)
BIC(neut_mod_0, neut_mod_2)
@

We would keep this term if using AIC or AICc, but not if using BIC.  

%Note that in this case, $df \approx 7$ (for \ttt{neut\_mod\_2}) and $n=749$, so $n/df$ is well above 40, and the AICc and AIC are almost equal.  
%Section XX shows a smaller-sample example.





\section{Random-effect correlations}
\label{sec:ranef-corrs}

In examples so far we have always used `uncorrelated' random effects, which assume there is no relationship between different random-effect terms for the same grouping factor.  For example, in model \ttt{vot\_mod\_2} it is assumed there is no relationship between (1) a speaker's offset from the overall intercept (random intercept) and (2) their offset from the overall slope of \texttt{speaking\_rate\_dev}.


\begin{boxedtext}{Broader context: Uncorrelated random effects and centering predictors}
\label{box:uncorr-caveat}

There is an important caveat to models with uncorrelated random effects: predictors for which random slopes are included should typically be centered, meaning continuous variables have mean zero and all factors use a `centered' (and ideally orthogonal) coding scheme.  There are two reasons for this. First, this helps minimize collinearity between the predictors, and between the predictors and the intercept, which makes zero correlations  between random effects a more plausible assumption. 
%between random  for which the model contains random-effect terms should be minimally collinear. In practice this means that \textbf{all predictors with random slopes should be centered}, meaning continuous variables have mean zero and all factors use a `centered' (and ideally orthogonal) coding scheme.  Otherwise, non-zero correlations between random effects are expected, so a model forcing correlations to be zero makes less sense. 
A more subtle reason is that: assuming uncorrelated random effects makes the fitted models \textbf{not} invariant to linear shifts in the predictors (e.g., adding 1 to \ttt{speaking\_rate\_dev}), which means the model predictions become tied to the particular scale each predictor was measured on   (\citealp[][\S5.1.2]{snijders2011multilevel}; \citealp{bates2015fitting}). As a result, \citeauthor{snijders2011multilevel} recommend never forcing correlations to be zero, and \citeauthor{bates2015fitting} recommend only using uncorrelated random effects for predictors which are ``measured on a ratio scale... i.e.,\ the zero point on the scale is meaningful.''  The simplest way to do this is just to center all predictors.  This is another argument for centering your predictors by default.  

Sometimes a subset of predictors are intrinsically related, and uncorrelated random effects make less sense. An example is a nonlinear effect of $x$ coded as several polynomial or spline terms.  The contrasts in a factor could also be thought of as intrinsically related, even when an orthogonal coding scheme is used \citep[][\S6.4]{snijders2011multilevel}. Using uncorrelated random effects in these cases---or indeed, when predictors have not been centered---is not wrong per se, as the model can still learn random effects that happen to be correlated if the data supports this, but special hacks are needed to code the model     (Section~\ref{sec:random-slopes-factors}, \ref{sec:nonlin-mems}). Alternatively, it could make sense for your  simplest model to have correlated random effects for random slopes for such intrinsically-related predictors (and the intercept), and uncorrelated random slopes for other predictors.

\end{boxedtext}



% 
% (written using the \ttt{}) written using the \texttt{\textbar{}\textbar{}} notation in {lme4}. For example, in Model 1C, the random effect term is \texttt{(1\ +\ relDuration.std\ \textbar{}\textbar{}\ participant)}.
% 
% Uncorrelated random effects assume that there is no relationship between different random-effect terms for the same grouping factor. In Model 1C, it is assumed that there is no relationship between
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   a participant's offset from the overall intercept (random intercept)
% \item
%   a participant's offset from the overall slope of \texttt{relDuration} (random slope)
% \end{enumerate}

If there were a relationship between (1) and (2), it would take the form of a positive or negative correlation (`correlated random effects'), e.g.,\ ``speakers with higher average VOT have higher \ttt{speaking\_rate\_dev} effects'' (positive correlation).

<<uncorr-ranef-1, echo=FALSE, out.width='45%',fig.width=default_fig.width*.45/default_out.width, fig.cap='Empirical effect of speaking rate on  log VOT (linear smooth) for each speaker (left) in the \\ttt{vot\\_voiced\\_core} data, and predicted intercept and speaking rate coefficient for each speaker from model \\ttt{vot\\_mod\\_2} (right, with linear smooth).'>>=
vot_voiced_core %>% ggplot(aes(x = speaking_rate_dev, y = log_vot)) +
  geom_smooth(aes(color = speaker), method = "lm", se = F) +
  xlab("Speaking rate") +
  ylab("VOT (log)") +
  scale_color_grey() +
  theme(legend.position = "none")
coefficients(vot_mod_2)$speaker %>%
  select(1:2) %>%
  ggplot(aes(x = `(Intercept)`, y = `speaking_rate_dev`)) +
  geom_point() +
  geom_smooth(color=default_line_color, method = "lm", se = F) +
  xlab("Speaker intercept") +
  ylab("Speaker speech rate coefficient")
@

To see if the uncorrelated random effect assumption is realistic in this case, consider the by-speaker plot of \ttt{vot} versus\ \ttt{speaking\_rate\_dev} (Figure~\ref{fig:uncorr-ranef-1} left).
A non-zero correlation here would correspond to a relationship between the height of a line (= speaker's intercept) and its slope (= speaker's slope). A relationship doesn't jump out, but we  can check this more carefully by plotting the relationship between  speaker's random intercepts and their \ttt{speaking\_rate\_dev} random slopes (Figure~\ref{fig:uncorr-ranef-1} right).

% FUTURE: redo right plot of rand effects with correlation plot from a prev chapter?

%% correlation:
%% coefficients(vot_mod_2_gender)$speaker %>% rename(intercept=`(Intercept)`) %>% select(1:2) -> k
%% cor.test(k$intercept, k$speaking_rate_dev)
%% cor=0.56, p=0.07

There may be a weak positive correlation here, where speakers with higher average VOT may also have larger speaking rate effects:

<<>>=
coeffs <- coefficients(vot_mod_2)$speaker
cor.test(coeffs[["(Intercept)"]], coeffs[["speaking_rate_dev"]]) %>%
  tidy(conf.int=TRUE) %>% 
  ## Show correlation, p-value, 95% CI
  select(estimate, p.value, conf.low, conf.high)
@

%there may be a weak positive correlation (\(r = 0.56, p = 0.07\)): speaers with 
Alternatively, this may be a spurious correlation, not significantly different from no correlation. We can test whether there's evidence for a ``real'' correlation between participants' slopes and intercepts by including this term in the mixed model.  %(Box~\ref{box:ranef-corrs-1} gives details.)

There is a single new term in this model, compared to model \ttt{vot\_mod\_2}: $\rho$, the correlation between participants' intercepts and slopes of \ttt{speaking\_rate\_dev}.
%\(\rho\) is the only new parameter in this model, compared to model \ttt{vot\_mod\_2} 
The only difference is that \ttt{vot\_mod\_2} assumes that $\rho=0$, while \ttt{vot\_mod\_3} fits \(\rho \in (-1,1)\).  


\begin{boxedtext}{Broader context: Random-effect correlations}
\label{box:ranef-corrs-1}

To see how random-effect correlations change a model's definition, it is easier to start from a simpler model: \ttt{neut\_mod\_2}, the model for the \ttt{neutralization} data with a single fixed effect (\ttt{voicing}) and a by-participant intercept and random slope (as well as a by-item random intercept), which are assumed to be uncorrelated.  This model's formula is in equation \eqref{eq:lmm3}.  Removing the assumption of no correlation, the lines of this formula involving by-participant random intercepts ($\alpha_{P,j}$) and \ttt{voicing} slopes ($\gamma_{P,j}$) become:
%\begin{align}
% y_i & = \beta_0 + \alpha_{P, j[i]} + \alpha_{I, k[i]} +
% (\beta_1 + \gamma_{P, j[i]}) x_i + \epsilon_i \nonumber \\
% \epsilon_i & \sim N(0, \sigma_E^2), & \text{for } i = 1, \ldots, 749 \nonumber \\
\begin{equation*}
\begin{pmatrix}
  \alpha_{P,j}\\
  \gamma_{P,j}
\end{pmatrix} 
\sim
N \left( 
\begin{pmatrix}
  0 \\
  0
\end{pmatrix}, 
\begin{pmatrix}
  \sigma_{P,0} & \sigma_{P,01} \\
  \sigma_{P,01}& \sigma_{P,1}
\end{pmatrix}
\right), \quad j = 1, \ldots, 16
\end{equation*}
% 
% \begin{equation}
% 
% \begin{pmatrix}
%   \alpha_{P,j} \\
%   \gamma_{P,j}
% \end{pmatrix}
%  \sim N 
%  (
% \begin{pmatrix}
%   0 \\
%   0
% \end{pmatrix},
% 
% \begin{pmatrix}
% \sigma_{P,0} & \sigma_{P,01} \\
%   \sigma_{P,01} & \sigma_{P,1}
% \end{pmatrix}
% 
% )  \quad \text{for } j = 1, ..., 16
% 
% \end{equation}

%\nonumber \\
% \alpha_{P, j} & \sim N(0, \sigma_{P,0}^2) , \quad \text{for } j = 1, \ldots, 16
% \nonumber \\ 
% \gamma_{P, j} & \sim N(0, \sigma_{P,1}^2)  \nonumber \\
% \alpha_{I, k} & \sim N(0, \sigma_{I,0}^2) , & \text{for } k = 1, \ldots, 24
% \label{eq:lmm3}
% \end{align}

These random effects 
now  
%by-participant random intercepts ($\alpha_{P,j}$) and \ttt{voicing} slopes ($\gamma_{P,j}$) 
follow a multivariate normal distribution. The new random-effect parameter, $\sigma_{P,01}$, is their covariance. That is, a covariance matrix is being fitted for the by-participant random effects:
$$
\Sigma_{P} = \begin{pmatrix}
\sigma_{P,0} & \sigma_{P,01} \\
  \sigma_{P,01} & \sigma_{P,1}
\end{pmatrix}
$$

Any covariance matrix for $k$ variables is equivalent to a \textbf{set of variances} (one for each variable) \textbf{and correlations} (between distinct variables).
%which is conceptually easier to think about. 
These are the parameters \ttt{lmer()} reports under `Random effects:' (here: $\rho = \sigma_{P,01}/(\sigma_{P,0} \sigma_{P,1})$ is the single correlation term).   For each additional grouping factor with correlated random effects (e.g.,\ by-item), another covariance matrix is fitted.

When $k>1$,  $\Sigma$ will correspond to $k(k+1)/2$ terms: the random intercept variance, $k-1$ random-slope variances, and $k(k-1)/2$ correlation terms.  For example, for the model \ttt{vot\_mod2\_more\_slopes}, which contains correlated by-item and by-participant random slopes, each with $k=2$, there are two random intercepts, four random slope variances, and three correlation terms.   The number of correlation terms grows quickly as $k$ increases (e.g.,\ 15 correlations for $k=5$ random slopes). This greatly affects the speed of model fitting, and---at least for typically-sized linguistic datasets---whether the model converges at all, when correlated random effects are used. 

\end{boxedtext}

In R, the ``single pipe'' notation (\texttt{\textbar{}}) is used for correlated random effects.  To fit \ttt{vot\_mod\_3}:

<<>>=
## Change from (1+speaking_rate_dev||speaker)
## to (1+speaking_rate_dev|speaker)  (uncorr. to correlated)
vot_mod_3 <- update(vot_mod_2, . ~ . - (1 | speaker) -
  (0 + speaking_rate_dev | speaker) + 
    (1 + speaking_rate_dev | speaker))
@

Note the syntax for correlated random effects: \ttt{(1 + A | speaker)} (or \ttt{(1 + A + B | speaker)}, etc.).  Compare the formulas for the two models:

<<>>=
formula(vot_mod_2)
formula(vot_mod_3)
@

The new model's output is:
<<output.lines=14:32>>=
summary(vot_mod_3)
@

(Compare to \ttt{vot\_mod\_2} output in Section~\ref{sec:vot-example-rate}.)

We see a positive correlation term under \texttt{Random\ effects}/\texttt{Corr} ($\rho = \Sexpr{tidy(vot_mod_3) %>% filter(term=='cor__(Intercept).speaking_rate_dev') %>% pull(estimate)}$)---the direction guessed from the scatterplot of random effects for \ttt{vot\_mod\_2} (Figure~\ref{fig:uncorr-ranef-1} right).

We can repeat that plot using the random effects from \ttt{vot\_mod\_3}, to see the relationship predicted by this model between participant offsets for intercept and \texttt{speaking\_rate\_dev} slope (Figure~\ref{fig:corr-ranef-1}).

<<corr-ranef-1, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Predicted intercept and speaking rate coefficient for each speaker from model \\ttt{vot\\_mod\\_3}, with linear smooth.'>>=
coefficients(vot_mod_3)$speaker %>%
  select(1:2) %>%
  ggplot(aes(x = `(Intercept)`, y = `speaking_rate_dev`)) +
  geom_point() +
  geom_smooth(color='darkgrey', method = "lm", se = F) +
  xlab("Speaker intercept") +
  ylab("Speaker speaking rate\ncoefficient")
@

%% correlation:
%% coefficients(vot_mod_3)$speaker %>% rename(intercept=`(Intercept)`) %>% select(1:2) -> k
%% cor.test(k$intercept, k$speaking_rate_dev)
%% cor=0.81, p=0.002

It seems that there is in fact a stronger relationship 
%($r=0.82$)
%, p=0.002) 
which is detected by including a correlation term.  To evaluate whether this correlation is significantly different from zero, we can do a likelihood-ratio test of the models:

<<output.lines=5:7>>=
anova(vot_mod_2, vot_mod_3, refit = FALSE)
@

(Here the \ttt{refit=FALSE} flag makes REML fits be used, because the models differ only in random-effect terms: Section~\ref{sec:ranef-lrt}.)
This test suggests that
%correlation (just) significantly contributes to the model, meaning 
speakers' rate effects and intercepts are not significantly correlated.
% (FUTURE: put in exercise or footnote? Given the caveats in Sec XXX-ranef-modcomp, we should use a more exact method. Parametric bootstrap would give 0.079 (XXX: exercise).  So it's actually questionable whether correlated)

%% LRT: p=0.045
%% PB: p=0.079
%% ## PBmodcomp(vot_mod_2, vot_mod_3, cl=cl)


Despite the correlation term, note that the fixed effects for the two models (with and without this term) are very similar, in terms of coefficients and standard errors---including the intercept and \ttt{speaking\_rate\_dev} terms:

<<>>=
## Model without correlation
tidy(vot_mod_2)[1:2, ] %>% select(-group)
## Model with correlation
tidy(vot_mod_3)[1:2, ] %>% select(-group)
@
 
The random-effect terms don't differ much, beyond the obvious addition of a correlation \(\rho > 0\) for model \ttt{vot\_mod\_3}.
%, while the other terms (e.g., \ttt{speaking\_rate\_dev} slope) differ slightly.

\subsection{Multiple random slopes}

Correlated random effects extend straightforwardly to the case of multiple random slopes. For example we can refit model \ttt{vot\_mod2\_more\_slopes} (Section~\ref{sec:multiple-random-slopes-1}) with correlated random effects:
%- All extends straightforwardly to the case of multiple random slopes---for ex votmod2more_slopes: 

<<output.lines=15:35>>=
vot_mod_2_more_slopes_corr <-
  lmer(log_vot ~ speaking_rate_dev + foll_high_vowel + cons_cluster +
    log_corpus_freq + place + gender +
    (1 + gender + speaking_rate_dev | word) +
    (1 + speaking_rate_dev + foll_high_vowel | speaker),
  data = vot_voiced_core)
summary(vot_mod_2_more_slopes_corr)
@

This model contains six correlation terms: one for each possible relationship between by-participant random intercepts/slopes (3) and similarly for by-item random effects.  For example, the correlation between the by-speaker random intercept and \ttt{speaking\_rate\_dev} random slope is 
\Sexpr{filter(tidy(vot_mod_2_more_slopes_corr), term=='cor__(Intercept).speaking_rate_dev' & group=='speaker')$estimate}; the correlation between the by-word random slopes of \ttt{gender} and \ttt{speaking\_rate\_dev} is \Sexpr{filter(tidy(vot_mod_2_more_slopes_corr), term=='cor__gender.speaking_rate_dev')$estimate}.

These terms have significantly improved the model, by a likelihood-ratio test:

<<output.lines=5:10>>=
anova(vot_mod_2_more_slopes_corr, vot_mod_2, refit = FALSE)
@


Compare to the output of \ttt{vot\_mod2\_more\_slopes} (shown in Section~\ref{sec:multiple-random-slopes-1}):

<<eval=FALSE>>=
summary(vot_mod_2_more_slopes)
@

The estimated random effects are again similar (except for the addition of correlation terms).  However the fixed-effect estimates are a bit different: most have slightly higher magnitudes and lower standard errors in the model with correlations (and thus slightly lower $p$-values, by any method). In other words adding correlations has led to {clearer} effects. 

% FUTURE: exercise.  Fit these models and calculate p-values somehow.  Suppose you're using a $p<0.05$ binary significance cutoff. Do any conclusions change about which predictors affect VOT for this data?

% (FUTURE exercise: what's the interpretation here, for one such term?  For gender effect: once it's accounted for that words with lower VOT are *more* affected by speaker gender, `overall' gender effect is clearer.  perhaps gender effect is absent for words which tend to be articulated more clearly.)

\subsection{Singular models}
\label{sec:singular-models}

The preceding examples illustrate how adding correlation terms to random-effect structure tends to change a model's estimates, especially fixed effects:
%(which are usually of primary interest): 
very little, or towards `clearer' estimates.\footnote{This statement is purely empirical: provided predictors are `centered' (Box~\ref{box:uncorr-caveat}), I have rarely seen a case where adding random-effect correlations to a model changed a qualitative conclusion. (Except about the degree of correlations between random effects, of course.)}  In addition, using uncorrelated random effects technically only makes sense provided all predictors are conceptually `centered' (Box~\ref{box:uncorr-caveat}), which takes some extra effort.  So it makes sense that correlated random effects are the default in {lme4}---the `true' model likely does have some non-zero correlations.

However in practice it is common for models with correlated random effects to not converge, or to have a related issue.
%even very simple models. 
For example, consider adding correlations to model \ttt{neut\_mod\_3}, which contains random slopes only for the predictor of primary interest (\ttt{voicing}):

<<message=TRUE, output.lines=13:34>>=
neut_mod_4 <- lmer(vowel_dur ~ voicing + prosodic_boundary +
  place + vowel + (1 + voicing | subject) +
  (1 + voicing | item_pair), data = neutralization)
summary(neut_mod_4, correlation = FALSE)
@


This model is \emph{singular}, meaning that a `dimension' of the random effects has been estimated as zero.  
%(See \ttt{?isSingular}, which we summarize here, for more details.)  
Typically this means variances that are (near) zero, or correlation parameters near 1 or -1.
%(in which case there is a linear combination of random effects which does not vary e.g.,\ across participants).  Intuitively these values are unlikely to be correct: in reality there is probably \textbf{some} by-group variation in the intercept and every predictor, which will never be \textbf{perfectly} correlated.
We discuss singularity further in Section~\ref{sec:singular-ch10}.
%, as part of the general topic of model selection for MEMs. 
For now, the crucial points are that  singular fits are (1) suspect, especially if due to perfect correlations, and (2) common when modeling linguistic data, especially using correlated random effects---typically  because the random-effect structure is too complex to be estimated given the data. 



% 
% These values are unrealistic: 
% 
% - In either case recommended practice is to remove this term (REF).
% 
% - explain why this happens: typically *because model is too complex for data*, combined w/ quirk of likelihood that perfect correlations are `better'.
% 
% - Note again that the fixed effects are essentially unchanged between models w and w/o correlations, while random effects differ a bit.

\subsection{Correlated versus uncorrelated random effects}
\label{sec:corr-vs-uncorr}

% FUTURE: lots of repetition here and in prev sections.  Also there should be some way to say the really essential points in text (if you use correlated random effects recommendation is to remove offending terms; since it rarely affects qualitative conclusions in practice it's sensible to default to uncorr ranefs; see box for details)



As we saw in the examples above, adding random-effect correlations rarely changes qualitative conclusions, but often seriously affects model fitting (since the number of correlation terms grows quadratically with the number of random-effect terms).
% \footnote{The answer here by C.\ Dalzell and B.\ Bolker reaches a similar but more detailed conclusion via simulation: \url{https://stats.stackexchange.com/questions/49832/in-a-multi-level-model-what-are-the-practical-implications-of-estimating-versus}.  In a more thorough simulation study, \citet[][]{seedorff2019maybe} find that dropping the correlation term (in case of non-convergence) is highly preferable to dropping a random slope, in terms of Type I error rate and power.}
% 
% In the examples above, coefficient estimates (fixed and random effects) were similar in models with and without random-effect correlations, while the model with correlation terms was slightly `better' in terms of likelihood (significant increase).

% Empirically, this often happens when random-effect correlation terms are added to a model:

%%, at least when all predictors have been centered (Box)
%%% FUTURE: and are orthogonal?: 
% adding correlation terms can lead to a significantly better model fit, but has little effect on the fixed-effect estimates, which are usually or primary interest.

% However, because the number of correlation terms grows quadratically with the number of random effect terms, models with correlated random effects are often singular, or do not converge, or fit extremely slowly.  In practice excluding correlation terms selectively is time-consuming, and sometimes impossible  (due to limitations of {lme4} syntax).  In short, adding random-effect \textbf{correlations} rarely changes qualitative conclusions but often seriously affects model fitting.

In contrast, which random \textbf{slope} terms are included is very important, as described above (Section~\ref{sec:what-adding-random-slope})---in particular, excluding a random slope term is anti-conservative and could change qualitative conclusions from a model.

To deal with these issues, it is often useful to \textbf{first try models with uncorrelated random effects}: 
including random slopes but no correlations.\footnote{But remember, to do this your predictors should be centered.}  This makes it less likely the random-effect structure will be too complex for the data, while avoiding dropping random slopes.\footnote{\citet{seedorff2019maybe} reach a similar conclusion, in a simulation study. \citet[][Online Appendix]{barr2013random} also conclude from a simulation study that ``... removing correlations will not harm the overall performance of the model''; but they do not recommend \textbf{starting} with a model without correlations.}  You can then add in correlations as needed (e.g.,~significantly increases model likelihood), or fit a final `full' model with correlated random effects if your data supports it.  
%In practice, even a `maximal' model with all possible random slopes but without correlation terms typically fits fairly quickly, and 
%If the model is singular due to random-effect variances estimated as zero these are straightforward to exclude.  

We elaborate on this `uncorrelated first' strategy later (Section~\ref{sec:model-selection-possible-procedures}).  Note that this strategy 
%``Try uncorrelated random effects first'' 
is my personal opinion (shared by \citealp{seedorff2019maybe}). Other sources 
\citep[e.g.,][]{brauer2018linear,barr2013random} 
recommend fiting models with correlated random effects by default, then removing correlations as needed; 
%a possible step if the model is singular or non-convergent;
this is also a reasonable default.
%doesn't converge 
%Although this all sounds esoteric, 
Model-fitting issues due to complex random-effect structure is one of the most frequent issues that researchers 
%using mixed-effects models scientists 
have in practice using mixed-effects models, and are the topic of ongoing debate.
%These issues are the topic of ongoing debate,
% (e.g.,\ \citealp{glmmfaq} `Singular models'; \citealp{matuschek2017balancing}; \citealp{barr2013random}; \citealp{brauer2018linear}; \citealp{seedorff2019maybe}), 
We return to these issues when we discuss model fitting and model selection in Chapter~\ref{chap:mem-3}.
Until then we will usually fit models with uncorrelated random-effect structure and assume this is OK.  

%Not everyone agrees with this opinion; for example \citet{brauer2018linear} lists removing random-effect correlations as 15th best of 20 possible remedies for non-convergent LMMs.   
% It is also a reasonable choice to fit models with correlated random effects by default, then try removing correlations as a possible step if the model doesn't converge;  If you take this route almost everything in the remainder of our mixed-models chapters will still apply.



% 
% - These points are crucial for model selection, and we return to the issue of ranef correlations there (REF); until then we are just using uncorrelated ranef structure and assuming that models w/ this OK.
% 
%




% 
% - *When* correlation term(s) added, and model fits OK, often v. little change -- assuming all predictors conceptually `centered'.
% 
% - In short, ranef correlations usually don't change qualitative conclusions -- but do seriously affect convergence, and excluding them selectively from a model is tricky.

% - In contrast, excluding random *slopes* can easily change qualitative conclusions (Ex above).  

% - some version of logic in 7.8.2, but without model selection -- just lead up to, so as a default we recommend *uncorrelated* random effect structure (following XX).

% - These points are crucial for model selection, and we return to the issue of ranef correlations there (REF); until then we are just using uncorrelated ranef structure and assuming that models w/ this OK.

%% NOTE/XX: - (you want to discuss maximal vs data-driven there, not here)


\subsection{Random slopes for factors}
\label{sec:random-slopes-factors}

Technical issues, related to R's implementation of factors, come up when factors are used as predictors in models with uncorrelated random effects \citep{singmann2019introduction}. This is an important issue in practice if you're building models with uncorrelated random effects (which we are, by default, but if you are not this section is skippable).
% We advise skipping this section until this issue actually comes up in practice for you.

% - Adapt 7.10.
% 
% - This issue comes up with *any* factor, when we want to add random slopes to the model w/o correlations.

\subsubsection{Two-level factors}

First, consider two-level factors.  Suppose we modeled the \ttt{neutralization} data using voicing coded as a two-level factor (\ttt{voicing\_fact}) rather than as numeric (\ttt{voicing}).   We first refit the simplest model (\ttt{neut\_mod\_0}):

<<>>=
neut_mod_0_fact <- lmer(vowel_dur ~ voicing_fact + (1 | subject) + 
    (1 | item_pair), data = neutralization)
@

The output of this model is identical to \ttt{neut\_mod\_0} (\ttt{summary(neut\_mod\_0\_fact)}) as expected.\footnote{Modulo the \ttt{voicing} coefficient being divided by 2 due to how \ttt{voicing\_fact} is coded.}
We now try to add a by-participant random slope to the model, with uncorrelated random effects, as in \ttt{neut\_mod\_2}:

% 
% 
% - Subsection (currently missing): two-level factor
% 
% - Suppose \ttt{voicing} coded as factor for neutralization data---consider adding a random slope, going from neut\_mod\_0 to neut\_mod2:
% 
% 
% - Trying to refit mod2 w/ uncorrelated ranef doesn't work:

<<output.lines=14:26>>=
## Change from (1|item_pair) + (1|subject) to
## (1+voicing_fact||subject) + (1|item_pair)
neut_mod_2_fact <- update(neut_mod_0_fact, . ~ . + 
    (0 + voicing_fact || subject))
summary(neut_mod_2_fact)
@

The random effects in this model do not make sense. (Sometimes you'll get an opaque warning, like \ttt{Model is nearly unidentifiable...}.) Rather than the  random-effect table showing one row for the intercept and one row for the single  \texttt{voicing\_fact} contrast, it contains one row per \textbf{level} of \texttt{voicing\_fact} (in addition to the intercept).   This happens because R is interpreting the random-effect formula as referring to all levels of the factor, rather than its contrast.\footnote{This follows from the way uncorrelated random effects are treated in \ttt{lmer()}, and how R deals with factors in models without intercept terms. A \ttt{(1 + X+ ... | Z)} term  becomes a \ttt{(1 | Z) + (0 + X | Z) + ...} term in the model formula. For factors \ttt{0 + X} means cell-means coding (Box~\ref{box:cell-means-coding}).}

This issue will arise whenever a factor X is used in an uncorrelated random-effect formula (whether using \ttt{||}, \ttt{(-1 + X | Y)}, or \ttt{(0 + X | Y)} notation---Section~\ref{sec:one-random-slope}).

%\textbf{whenever a factor is used in an uncorrelated random effect formula} (one that contains \texttt{\textbar{}\textbar{}}, or that uses \texttt{(0+X\textbar{}group)} notation). 

% 
% 
% 
%  This is because of how R treats factors without intercept term (REF to where discussed in earlier chapter).
% 
% - adapt text from 7.10... and clarify that this issue will arise whenever a factor used in an uncorrelated ranef, such as:
% 
% (1+X||subject)
% 
% (0+X||subject)
% 
% (1+X+ ... ||subject)  (where ... = other terms)

To include factors in a model with uncorrelated random effects, we must first turn them into numeric variables corresponding to the contrasts. (This is what R usually does under the hood for factors, but not in this case.)  One way to do this is to use the model matrix of the random-intercepts-only model, which contains these contrasts:
%\footnote{Another way to do this, without fitting a new model, is shown in \citet[][\S7.10.2]{qmld}.
%}

<<output.lines=1:5>>=
mm <- model.matrix(neut_mod_0_fact) %>% data.frame()
mm
@

We then extract the numeric variable for each contrast, add it to the original data frame, then fit the model with random slopes using these variables in the random-effect formula rather than the actual factor:

<<output.lines=14:25>>=
neutralization <- add_column(neutralization, 
  voicing_fact1 = mm$voicing_fact1)

neut_mod_2_fact <- lmer(vowel_dur ~ voicing_fact +
  (1 + voicing_fact1 || subject) + (1 | item_pair),
data = neutralization)
summary(neut_mod_2_fact)
@

The random effects for this model look fine.  This is essentially the same model as \ttt{neut\_mod\_2}, as we'd expect.\footnote{An even easier route to fit models with factors and uncorrelated random effects is the \ttt{lmer\_alt} function from the \ttt{afex}, which automates the process of extracting numeric predictors for factors and fitting the model. This function doesn't extend to GLMMs, or to models where you want some random effects correlated and others uncorrelated, so it's still good to know our `by-hand' method.}

% \begin{boxedtext}{Practical note: Alternative method}
% 
% An even easier route to fit models with factors and uncorrelated random effects is the \ttt{lmer\_alt} function from the \ttt{afex}, which automates the process of extracting numeric predictors for factors and fitting the model. For example:
% 
% <<output.lines=14:25>>=
% neut_mod_2_afex <- lmer_alt(vowel_dur ~ voicing_fact + 
%                       (1+voicing_fact||subject) + (1|item_pair),
%                        data=neutralization, check_contrasts=FALSE)
% summary(neut_mod_2_afex, correlation=FALSE)
% @
% 
% The \ttt{check\_contrasts} option makes the numeric predictors be the actual contrasts, so in theory this does the same thing as our `by-hand' process.   This function doesn't extend to GLMMs, or to models where you want some random effects correlated and others uncorrelated, so it's still good to know the by-hand method.
% 
% \end{boxedtext}


\subsubsection{Multi-level factors}

The same issue arises for factors with 3+ levels. Suppose that we wanted to add by-speaker random slopes of \ttt{place} to model \ttt{vot\_mod\_2}, keeping uncorrelated random effects. 
%(We might do this because place of articulation is one of the major factors affecting VOT, so accurately estimating its effect by accounting for by-group variability could lead to a much better model.)  
Doing this with the usual notation for an uncorrelated random slope again gives a model with nonsensical random effects:

<<output.lines=13:21>>=
# change from (1|item_pair) + (1|speaker) to
# (1|item_pair) + (1|place||speaker)
vot_mod_2_place_slope <- update(vot_mod_2, . ~ . + 
    (0 + place | speaker))
summary(vot_mod_2_place_slope)
@

There is one row per level of \ttt{place} rather than one row for each of its (two) contrasts.  We can again extract these contrasts manually, then use them in the random-effect structure to refit the model.  (Here we show a second way to extract the contrasts, which doesn't require having an intercepts-only model fitted.)
% 
% The 
% (similar issue if you used \ttt{(1+speaking\_rate\_dev+place|speaker)}. This is because of how R treats factors without intercept term (REF to where discussed in earlier chapter)
% 
% - adapt text from 7.10... and clarify that this issue will arise whenever a factor used in an uncorrelated ranef, such as:
% 
% (1+X||subject)
% 
% (0+X||subject)
% 
% (1+X+ ... ||subject)  (where ... = other terms)
% 
% - Again, extract the contrasts manually, then use them in random effect structure:

<<echo=1:5, output.lines=16:23>>=
mm <- model.matrix(~place, vot_voiced_core)
# Contrasts 1 and 2  (column 1 of `mm' is the intercept)
vot_voiced_core$place1 <- mm[, 2] ## contrast 1
vot_voiced_core$place2 <- mm[, 3] ## contrast 2

## alternatively, could do:
## mm <- model.matrix(vot_mod_1) %>% data.frame()
## vot_voiced_core <- add_column(vot_voiced_core,
##                              place1=mm$place1, place2=mm$place2)

## add uncorrelated by-speaker random slope of place:
vot_mod_2_place_slope <- update(vot_mod_2, . ~ . + 
    (0 + place1 + place2 || speaker))
summary(vot_mod_2_place_slope)
## (you get a convergence error here, which led me down a rabbit hole -- it goes away if you use `bobyqa' optimizer, which was default before 2/2019 and after changing which there are a lot more `false positive' warnings. in allFit only the default optimizer looks different. various issues noted with newer default in lme4 github issues. thinking we should set default optimizer to bobyqa in this chapter, then discuss convergence issues in next chapter)
## deal with this -- either need to contextualize in text that we're using `control', or set default optimizer early and discuss there, or just punt to next chapter. see comments above
## ... May 2020: this somehow stopped being an issue
@

The random-effect estimates for this model look fine.  Note that it was OK to use uncorrelated random effects here because \ttt{place} is coded with Helmert contrasts (a `centered' coding scheme), so the \ttt{place1} and \ttt{place2} predictors are centered.

\begin{boxedtext}{Practical note: Random slopes for factors with correlations}
\label{box:random-slopes-with-correlations}

To include random slopes for factors in a model with \textbf{correlated} random effects, the usual formula can be used (\ttt{(1 + X + ... | Y)}) and no tricks are needed. For example, to fit a correlated version of the first model above:

<<echo=1>>=
neut_mod_2_fact_corr <- lmer(vowel_dur ~ voicing_fact +
  (1 + voicing_fact | subject) + (1 | item_pair),
data = neutralization)
#
#vot_mod_2_place_slope_corr <- lme4::lmer(log_vot ~ speaking_rate_dev +
#    foll_high_vowel + cons_cluster + log_corpus_freq + place + 
#    gender + (1 | word) + (1 + speaking_rate_dev + place | speaker), 
#  data = vot_voiced_core)
@

However the possible issues in models with multiple random slopes (and correlated random effects) discussed above apply: (near)-singular models (see \ttt{summary(neut\_mod2\_fact\_corr)}) or very slow fits.
%---model \ttt{vot\_mod2\_place\_slope\_corr} takes 4.5x as long to fit as the model without correlations (on my laptop).

\end{boxedtext}


\section{Model predictions}
\label{sec:lmm-model-predictions}

%% FUTURE: revisit this intro; working quickly while revising chapter 3/12/20
As for non-mixed-effects models it is very useful to generate model predictions for LMMs and use these to visualize the model's results. We consider cases here which are most common in practice:
\begin{itemize}
\item \emph{Fixed effects only}: partial-effect plots or interaction plots, showing the model's predictions as 1+ fixed-effect predictors are varied. 

\item \emph{Fixed and random effects}: predictions for different levels of a grouping factor (e.g., different participants)---in the partial effect of a given predictor (`slope'),  the average effect (`intercept'), or something more complex.

\end{itemize}

We first cover getting predicted values, which is straightforward and similar to non-mixed-effects-models, then turn to adding uncertainties, where different options exist.

\subsection{Predicted values (only)}
\label{sec:pred-vals-only}

\subsubsection{Fixed effects}
\label{sec:pvo-fixed-effects}

For partial-effect and interaction plots, we can use functionality we've seen already to extract model predictions as specified fixed-effect predictors are varied, either holding other fixed-effect predictors constant or marginalizing over them. This can be done using existing packages (e.g., \ttt{ggpredict()} or \ttt{ggemmeans()} from ggeffects/effects/emmeans),
%/{effects} package, }/{emmeans} package, respectively), 
or by hand (using e.g.,\ \ttt{predict()}: Box~\ref{box:lmm-preds-hand}).  Typically we want to marginalize over random effects as well, so the prediction is for an `average participant', etc.

For example, we could use \ttt{ggpredict()} to compute the partial effect of speaking rate for the VOT model (\ttt{speaking\_rate\_dev}, model \ttt{vot\_mod\_2}):

<<>>=
## `type' argument: ``fe'' = fixed-effects
vot_pred_1 <- ggpredict(vot_mod_2, terms = c("speaking_rate_dev"),
  type = "fe")
vot_pred_1
@

This is the speaking rate effect for an average speaker and word, with other predictors held constant (Figure~\ref{fig:preds-nocis-1} left).

To exemplify an interaction plot, we first fit a new \ttt{neutralization} model by adding a \ttt{voicing}:\ttt{prosodic\_boundary} interaction to \ttt{neut\_mod\_3} (as in Section~\ref{sec:ixn-example}):

<<>>=
neut_mod_3_ixn <- update(neut_mod_3, . ~ . + 
    voicing:prosodic_boundary)
@

We could then use ggeffects functions (\ttt{ggemmeans()} or \ttt{ggpredict()}) to compute the model's predictions as \ttt{voicing} and \ttt{prosodic\_boundary} are varied, for an average participant and item, marginalizing over other predictors (place, vowel).
Figure~\ref{fig:preds-nocis-1} (right) shows the corresponding interaction plot.
%the voicing effect as prosodic boundary is varied, for an average participant and item, marginalizing over other predictors (place, vowel).



% - varies specified parameter while holding others fixed; average word and speaker. Could also marginalize over other vars, one implementation is \ttt{ggemmeans()}:

<<>>=
neut_pred_1 <- ggemmeans(neut_mod_3_ixn, 
  terms = c("voicing", "prosodic_boundary"))
@


<<preds-nocis-1, echo=FALSE,  fig.asp=0.4, out.width='90%', fig.width=default_fig.width*.90/default_out.width,  fig.cap='Partial effect of \\ttt{speaking\\_rate\\_dev} in model \\ttt{vot\\_mod\\_2} (holding other predictors constant); interaction plot of \\ttt{voicing} and \\ttt{prosodic\\_boundary} for model \\ttt{neut\\_mod3\\_ixn} (marginalizing over other predictors).'>>=
p1 <- vot_pred_1 %>%
  mutate(speaking_rate_dev = x, log_vot = predicted) %>%
  ggplot(aes(x = speaking_rate_dev, y = log_vot)) +
  geom_line(size=0.75) +
  xlab("Speaking rate") +
  ylab("Predicted log VOT")
#+ geom_point(data=vot_voiced_core, size=0.5, alpha=0.1)

temp_df <- neutralization %>% mutate(voicing = voicing_fact, prosodic_boundary = factor(prosodic_boundary, labels = c("no", "yes")))

p2 <- neut_pred_1 %>%
  data.frame() %>%
  mutate(voicing = factor(x, labels = c("voiced", "voiceless")), prosodic_boundary = factor(group, labels = c("no", "yes")), vowel_dur = predicted) %>%
  ggplot(aes(x = voicing, y = vowel_dur)) +
  geom_line(aes(group = prosodic_boundary, lty = prosodic_boundary), size=0.75) +
  xlab("Voicing") +
  ylab("Predicted vowel\nduration") +
  labs(color='Prosodic\nboundary')

p1 + p2  + plot_layout(widths = c(1.75, 2))
#+ geom_jitter(data=temp_df, width=0.1, height=0.1, size=0.5, alpha=0.25)
@


Note that you can make quick partial-effect plots by just plotting the output of \ttt{ggpredict()}/\ttt{ggemmeans()}, for example \ttt{plot(vot\_pred\_1)}, 
\ttt{plot(neut\_pred\_1)}.  Figure~\ref{fig:preds-nocis-1} is just these plots customized to look nicer.\footnote{As always you can see the plot code in the source file for this chapter. For the interaction plot, where we are varying two categorical predictors coded as numeric, it would be better to first refit the model with the \ttt{voicing} and \ttt{prosodic\_boundary} fixed effects coded as factors. (as in Section~\ref{sec:viz-effects-logistic}).}
%where we did this for another case.}
% 
% <<eval=FALSE>>=
% plot(vot_pred_1)
% plot(neut_pred_1)
% @

%(including the empirical data overplotted to give a sense of scale, which is not necessary).

\subsubsection{Fixed + random effects}
\label{sec:fixed-random-preds}

The same functionality can be used to compute group-level predictions, e.g., for each participant, by just treating the grouping factor (e.g.,\ \ttt{subject}) as another predictor being varied.  For example Figure~\ref{fig:neut-3-preds} above showed the predicted voicing effect for each participant for model \ttt{neut\_mod\_2}, averaging across items; these predictions were made using \ttt{ggpredict()}:

<<output.lines=1:10>>=
## `type' argumnt: make predictions for each level of `subject'
ggpredict(neut_mod_2, terms = c("voicing", "subject"), type = "re")
@


\begin{boxedtext}{Practical note: Model predictions `by hand'}
\label{box:lmm-preds-hand}

The text mostly uses pre-existing packages to make model predictions. It is also useful to know how to make model predictions yourself, as pre-existing packages make various assumptions, don't always give exactly what you want, and sometimes are buggy.  Making predictions by hand is also the best way to understand what 
%also lets you understand what 
such packages are doing, which is crucial to figure out how to use future packages. (By the time you read this, our {ggeffects} code may no longer work!) 

%% FUTURE: link back to relevant section in previous chapter? (JPK)
The basic steps are the same as for non-mixed models: (1) make a new dataframe of values you want to predict at; (2) get the model's predictions for these values (using e.g.,\ \ttt{predict}); (2a) calculate and add  confidence intervals (if using); (3) make plots using the predictions.

Some examples are in \citet[][\S9.4, 9.6]{qmld}, \citet[][`Predictions...']{glmmfaq}, or many other places online. An example using simulation from the fitted model for (2)/(2a) rather than \ttt{predict()} is shown below (Section~\ref{sec:pred-val-uncert-lmm}).
\end{boxedtext}
%FUTURE:  other places in QMLD?


%- while I'm assuming in the text you use pre-existing packages to make model predictions, useful to know how to `roll your own' predictions to not be at the mercy of packages, and to understand what they're doing and have more flexibility.
% 
% - Three steps (as in prev chapter ex??): set up dataframe, make predictions, plot.  
% 
% - To add confidence intervals, insert before plotting: add manually-computed CIs
% 
% Examples in QMLD 9.4, 9.6 (?)

%FUTURE: revise box to be useful, or maybe we don't need it


\subsection{Predicted values and uncertainties}
\label{sec:pred-val-uncert-lmm}

Generally we want to show model predictions with a measure of uncertainty, such as the 95\% confidence intervals computed for predictions from linear regressions (Section~\ref{sec:model-predictions-intro}).

Obtaining uncertainties is less straightforward than obtaining just model predictions because (analogously to our discussion of $R^2$ measures) mixed models capture  several kinds of uncertainty which could be accounted for.  Should we take into account just uncertainty in fixed effects (similarly to CIs for non-mixed-effects models)?  Or random effects as well?  If the latter, how?  The issue is complex enough  that {lme4} simply does not provide errors on predictions (see \ttt{?predict.merMod}), and best practices are still evolving (details in Box~\ref{box:conf-pred-int}).   

That said, there are two basic options, versions of which are  straightforward to compute with existing packages: confidence and prediction intervals.  

\paragraph{Confidence and prediction intervals}
%\label{sec:conf-pred-int}

The simplest option is to incorporate just uncertainty in the fixed-effect coefficients. The resulting \emph{confidence intervals} use the error in the population-level prediction, e.g., ``for an average participant/item''  and are analogous to CIs for a non-mixed model.
One implementation which calculates these automatically is \ttt{ggpredict()} (or \ttt{ggemmeans()}) with the \ttt{type=`fe'} option.  These are the confidence intervals shown above in \ttt{vot\_pred\_1} and \ttt{neut\_pred\_1} (Section~\ref{sec:pvo-fixed-effects}).

The other option is to calculate uncertainty for \textbf{predictions}, meaning new observations, by incorporating not just the uncertainty in the fixed-effect coefficients, but also in individual observations.  There are different ways to define this uncertainty: either just using the residual variance, or the random-effect variances (the variance components), or both.
The resulting \emph{prediction interval}, however defined, captures the error in a new observation as opposed to an `average' observation, and is thus always larger than the confidence interval.  

One useful method, which works very generally,
%, which is relatively quick and works very generally, 
is to \textbf{simulate from the fitted model} using the \ttt{simulate()} method of {lme4} models.   This function  uses the model's definition in terms of probability distributions (e.g., equation~\ref{eq:lmm1}), which gives a probability distribution over possible $y$ for an observation given its values for the predictors and coefficient values for the model.  Suppose we have set up a `prediction dataframe', which specifies the predictor values for observations for which $y$ is to be predicted (as in Section~\ref{sec:mlr-ex-1} for linear regression).
New model coefficients are simulated $N$ times, then used to get $N$ predictions of $y$ for row $i$ of the dataframe; the 95\% quantiles are the prediction interval. (See \ttt{?simulate.merMod} for more.)  

It is possible to generate simulation-based prediction intervals using functions from other packages which are `wrappers' for \ttt{simulate.merMod()}, such as \ttt{ggpredict()} (from {ggeffects}) using the \ttt{type=`sim'} option. However, given the very different ways prediction error can be defined it is safer to use \ttt{simulate()} directly so you know what is being computed, as we show here.\footnote{This advice differs from our usual practice in this book of using functionality from existing packages when available, as we have found that 
%as opposed to doing things by hand,  and is based on personal experience.   Essentially I have found that 
existing functions for automatically computing prediction intervals 
%(which make use of lower-level functions like \ttt{simulate.merMod}) 
are not yet stable enough to recommend.}
%different implementations don't work across all models (e.g., with uncorrelated random slopes) and best practices for what `observation-level error' should mean are in flux.}
% across dif While writing this book the number of functions available for `automatically' computing prediction intervals has dramatically increased, almost all of which are wrappers on lower-level functions like \ttt{simulate.merMod}.  These methods can give very different results (i.e.,\ wider/narrower intervals) and it takes a lot of digging into documentation and technical knowledge to understand what they are actually calculating (which can be very different definitions of observation-level error). I decided it is better to use a method which requires more coding but where I know what the results mean. In addition }

For example, for the partial effect of \ttt{speaking\_rate\_dev} in \ttt{vot\_mod\_2}, we would first set up a dataframe varying this predictor holding others fixed:

<<>>=
vot_pi_df <- data.frame(
  speaking_rate_dev = seq(-3, 2, by = 0.1),
  gender = 0,
  foll_high_vowel = 0,
  cons_cluster = 0,
  log_corpus_freq = 0,
  place = "labial",
  word = "XXX",
  speaker = "XXX"
)
@

Here we have made the grouping factors \ttt{word} and \ttt{speaker} have new (unobserved) levels because this makes \ttt{simulate.merMod()} return population-level predictions (i.e., ``values of zero will be used for the random effects'').

We then simulate many ($N=10000$) predictions for each row, and use the 95\% quantiles as the prediction intervals (and the median as the predictions):

<<output.lines=1:6>>=
## allow.new.levels=TRUE : together with speaker=XXX and word=XXX,
## makes the predictions population-level ("average speaker/word").
sims <- simulate(vot_mod_2, newdata = vot_pi_df, nsim = 10000, 
  allow.new.levels = TRUE)

vot_pi_df$conf.low <- apply(sims, 1, function(x) {
  quantile(x, 0.025)
})
vot_pi_df$conf.high <- apply(sims, 1, function(x) {
  quantile(x, 0.975)
})
vot_pi_df$predicted <- apply(sims, 1, mean)

vot_pi_df %>% head(n=2)
@


<<echo=FALSE>>=
## Get PIs and CI dataframes into the same format
vot_pi_df <- vot_pi_df %>%
  select(speaking_rate_dev, predicted, conf.low, conf.high) %>%
  add_column(type = "PI")

vot_pred_1 <- vot_pred_1 %>%
  data.frame() %>%
  add_column(type = "CI") %>%
  rename(speaking_rate_dev = x) %>%
  select(-group, -std.error)

rate_pred_df <- bind_rows(vot_pi_df, vot_pred_1)
@
%
%Both

These prediction intervals and the 
%This code sets up a dataframe including both these prediction intervals and the
previously-computed confidence intervals, for the \ttt{speaking\_rate\_dev} effect in \ttt{vot\_mod\_2},
are shown in Figure~\ref{fig:ci-pi-fig-1} (left).  We see that the prediction intervals are much larger.  The same is true for the \ttt{voicing}:\ttt{prosodic} boundary effect (right), for which prediction intervals are generated similarly 
(code not shown).\footnote{The CIs and PIs in the VOT example have slightly different `predictions' (compare dotted/dashed lines in Figure~\ref{fig:ci-pi-fig-1} left); this is because the PIs do not marginalize over multi-level factors, here \ttt{place}.  We could do this by defining \ttt{vot\_pi\_df} to contain all levels of \ttt{place} for each \ttt{speaking\_rate\_dev} value, then average the predictions across \ttt{place} levels.  This doesn't matter for our focus here which is the relative width of CIs and PIs.}

<<echo=FALSE>>=
neut_pi_df <- neutralization %>%
  tidyr::expand(voicing, prosodic_boundary, vowel, place) %>%
  mutate(item_pair = "XXX", subject = "XXX")

sims <- simulate(neut_mod_3_ixn, newdata = neut_pi_df, nsim = 10000, 
  allow.new.levels = TRUE)

neut_pi_df$conf.low <- apply(sims, 1, function(x) {
  quantile(x, 0.025)
})
neut_pi_df$conf.high <- apply(sims, 1, function(x) {
  quantile(x, 0.975)
})
neut_pi_df$predicted <- apply(sims, 1, function(x) {
  mean(x)
})

## get CI predictions into readable form
neut_pred_1 <- data.frame(neut_pred_1) %>%
  mutate(voicing = factor(x, labels = c("voiced", "voiceless")), prosodic_boundary = factor(group, labels = c("no", "yes")), vowel_dur = predicted) %>%
  select(-group, -x, -predicted) %>%
  add_column(type = "CI")


neut_pi_df <- neut_pi_df %>%
  mutate(
    voicing = factor(voicing, labels = c("voiced", "voiceless")),
    prosodic_boundary = factor(prosodic_boundary, labels = c("no", "yes"))
  ) %>%
  group_by(voicing, prosodic_boundary) %>%
  summarise(
    vowel_dur = mean(predicted),
    conf.low = mean(conf.low),
    conf.high = mean(conf.high)
  ) %>%
  add_column(type = "PI")

voicing_pred_df <- bind_rows(neut_pi_df, neut_pred_1)
@


<<ci-pi-fig-1, echo=FALSE, fig.height=3, fig.width=4, out.width='45%', fig.cap='Partial-effect and interaction plots from Figure \\ref{fig:preds-nocis-1}, with confidence intervals and prediction intervals added.'>>=
rate_pred_df %>% ggplot(aes(x = speaking_rate_dev, y = predicted, ymin = conf.low, ymax = conf.high)) +
  geom_line(aes(lty = type)) +
  geom_ribbon(aes(fill = type), alpha = 0.25) +
  ylab("Predicted log(VOT)") +
  xlab("Speaking rate")

#
# voicingPredDf <- bind_rows(
#   ggpredict(neut_mod_3, terms = c('voicing'), type='fe') %>% data.frame() %>% add_column(type='CI'),
#   ggpredict(neut_mod_3, terms = c('voicing'), type='sim') %>% data.frame() %>% add_column(type='PI')
# )
#
# voicingPredDf <- bind_rows(
#   ggpredict(neut_mod_3_ixn, terms = c('voicing', 'prosodic_boundary'), type='fe') %>% data.frame() %>% add_column(type='CI'),
#   ggpredict(neut_mod_3_ixn, terms = c('voicing', 'prosodic_boundary'), type='sim') %>% data.frame() %>% add_column(type='PI')
# ) %>%
#   mutate(voicing=factor(x, labels=c('voiced', 'voiceless')),
#          prosodic_boundary = factor(group, labels=c('no', 'yes')))

voicing_pred_df %>% ggplot(aes(x = voicing, y = vowel_dur, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange(aes(shape = prosodic_boundary), position = position_dodge(width = 0.5)) +
  ylab("Predicted vowel duration") +
  labs(shape='Prosodic\nboundary') +
  facet_wrap(~type) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
@


% sims <- simulate(neut_mod_3, newdata=newdata, nsim=1000, 
%                  re.form=~(1+voicing|subject))
% 
% newdata$lower <- apply(sims, 1, function(x){quantile(x,0.025)})
% newdata$upper <- apply(sims, 1, function(x){quantile(x,0.975)})
% newdata$pred <- apply(sims, 1, function(x){quantile(x,0.5)})
% 
% 
% <<>>=
% ratePredDf <- bind_rows(
%   ggpredict(vot_mod_2, terms = c('speaking_rate_dev'), type='fe') %>% data.frame() %>% add_column(type='CI'),
%   ggpredict(vot_mod_2, terms = c('speaking_rate_dev'), type='re') %>% data.frame() %>% add_column(type='PI')
% )
% @


% 
% OnOne implementation is \ttt{ggpredict()} using the \ttt{type=''} option, which (I think) uses an estimate of uncertainty from all variance components (the same one used for conditional $R^2$: Johnson-REF). 
% 
% done, I think?  : decide if we're going to use type=re or type=sim, and change.  probably simulation is good as this also covers the by-subject prediction example (comments)
% ## `re' flag: make predictions for each level of `subject'
% ggpredict(neut_mod_2, terms = c('voicing', 'subject'), type='sim')

\begin{boxedtext}{Broader context: Confidence and prediction intervals}
\label{box:conf-pred-int}

Confidence intervals (as calculated by \ttt{ggpredict()} or \ttt{ggemmeans()}, here) involve computing a standard error $SE_i$ for each prediction $\hat{y}_i$; an approximate 95\% interval is then $[\hat{y_i} - 1.96 \cdot SE_i, \hat{y_i} + 1.96 \cdot SE_i]$.

If you know some linear algebra, it  can be useful to understand how these standard errors are defined.  The simplest way, 
%The simplest way to define these standard errors,
which requires no computation beyond the fitted model, is done similarly to a linear regression.  Let $\hat{\vec{\beta}}$ be the estimated fixed-effect coefficients; $\hat{\Sigma}$ be the variance-covariance matrix of the fixed-effect coefficient estimates; and $X$ the design matrix describing new data for which predictions will be made (each row = one observation, each column = one fixed effect).  Then $\hat{\vec{y}} = X \hat{\vec{\beta}}$ are the predictions and $X \hat{\Sigma} X^{t}$ is their variance-covariance matrix.  The diagonal elements of this matrix are approximately the (squared) error of each prediction, based just on error in the fixed-effect coefficients.  In other words:
\begin{eqnarray*}
SE_i &=& \text{diag}(\text{var}(\hat{\vec{y}}))_i 
= 
\text{diag}(\text{var}(X \hat{\beta}))_i \nonumber \\
 & =& \text{diag}(X \Sigma X^{t})_i  
 %\label{eq:pred-se}
\end{eqnarray*}

This is the source of formulas like the following, which you will see in online recipes \citep[e.g.,][]{glmmfaq}, or the CIs returned by \ttt{ggeffects} functions (see `Short technical note' in the `ggeffects' vignette). 
<<eval=FALSE>>=
predvar <- diag(DesignMat \%*\% vcov(model) \%*\% t(Designmat))
@

The equation above is useful to know, 
%Eq.~\ref{eq:pred-se} is a good fact to know 
for constructing (approximate) confidence intervals for most statistical models.

%(This follows by some linear algebra, for which we could not find a good reference:  if $\Sigma$ is the variance-covariance matrix of a vector $\vec{x}$, $A \Sigma A^t$ is the V-C matrix of a linear transformation $\vec{x}' = A \vec{x}$, whose diagonal elements are the SEs of $\vec{x}'$.  In this case $A$ is the design matrix and $\vec{x}$ are the fixed-effect coefficient estimates.)  

Prediction intervals incorporate an estimate of observation-level error, which can be defined in different ways.  The simplest option just adds the residual variance to each $SE_i$ (as in \citealp{glmmfaq}) and can be done by hand. The gold standard 
%(as for fixed-effect coefficient CIs, which are a special case of model predictions) 
is parametric bootstrapping (using \ttt{bootMer()}) to calculate a 95\% PI for each observation, which  takes all sources of error into account but is very computationally intensive. Section~\ref{sec:parametric-bootstrap-example} shows an example.  (If only error from fixed effects is taken into account, these are 95\% CIs which are more precise than the approximation described above.)  The `Prediction Intervals from merMod Objects' vignette in the {merTools} package \citep{merTools} gives useful discussion (and yet another prediction interval implementation).  Simulation from the fitted model, which we describe in the text,  is a middle way.
% and a more complex implementation which is still not computationally intensive (but is significantly more involved to use as of this writing, hence we don't use it as our default) . .
%(making it infeasible for models which take e.g., minutes to fit)
%and typically overkill.
%REF-GLMM FAQ). A better option is to add on the mean variance due to \textbf{all} variance components (= all random-effect variances plus the residual variance); the value \ttt{ggpredict()} uses here is the value used in computing conditional $R^2$ (where ``variance due to random effects'' must be defined), from REF Johnson-2014 (XX discussed above, so simplify here?).
% 
% Another option is to compute a confidence interval for each point via simulation, using \ttt{simulate.merMod} (see XXX for what this means). This is what
% 
% 
% There are different possible ways to define prediction intervals; the \ttt{predictInterval} vignette gives useful discussion and a more complex implementation which is still not computationally intensive (but is significnatly more involved to use as of this wriitng, hence we don't use it as our default) . .
% 
% For both confidence and prediction intervals there are more precise options available than calculating observation-level standard errors; see XX for discussion.  The gold standard (as as for coefficient CIs, which are a special case of model predictions) is parametric bootstrapping (using \ttt{bootMer()}), which is extremely computationally intensive  (making it infeasible for models which take e.g., minutes to fit) and typically overkill.

\end{boxedtext}


%%%%
%%%%

% A useful implementation is ggeffect/ggpredict: use `type='re'`---uncertainty in all variance parameters. 
% (The implementation in predictInterval also useful, but doesn't seem to work for models used here---uncorr random effects?)
% 

% For example, this code sets up a dataframe including both confidence and prediction intervals for the \ttt{speaking\_rate\_dev} effect in \ttt{vot\_mod\_2}:
% 
% <<>>=
% ratePredDf <- bind_rows(
%   ggpredict(vot_mod_2, terms = c('speaking_rate_dev'), type='fe') %>% data.frame() %>% add_column(type='CI'),
%   ggpredict(vot_mod_2, terms = c('speaking_rate_dev'), type='re') %>% data.frame() %>% add_column(type='PI')
% )
% @


\begin{boxedtext}{Practical advice: Which uncertainty to show?}
\label{box:which-uncertainty}
%% TODO: REF needed for `sources'
Which of confidence or prediction intervals should you use in practice to convey uncertainty in model predictions depends on the case.  Confidence intervals are more commonly used and more intuitive, and are OK as long as we care mostly about fixed-effect estimates, i.e., average values.  But as soon as we care about anything besides average values, (some version of) prediction intervals will be more appropriate.  For example, if we were calculating the degree of overlap between \ttt{voicing}=\tsc{voiced} and \tsc{voiceless} stops predicted by a model of VOT, it would make sense to use prediction intervals incorporating all sources of uncertainty, which would capture the model's predicted VOT range for new observations.
% FUTURE: some references!
%Sources tend to recommend (some version of) prediction intervals, which better represent the full amount of uncertainty in model predictions (e.g.,\ \citealp[][\S3.5]{faraway2015linear}). 
% TODO: more refs!  maybe this isn't true..
% However PIs are currently not standard (at least in language sciences), and don't correspond to uncertainties commonly shown for \textbf{non}-mixed models---where it would also be possible to show observation-level variability.  So we recommend confidence intervals as a default at present.

It is good to be aware of the basic distinction between confidence and prediction intervals because different packages for making model predictions (e.g.,\ {ggeffects}, {emmeans}, {merTools}) do different things by default, and what a model's effects `look like' when plotted has a big impact on the conclusions you draw from the model.
\end{boxedtext}
% 
% - Which to use?   Most common and intuitive is confidence intervals, since we care mostly about fixed effects. sources tend to recommend (some version of) prediction intervals, which better represent uncertainty; however non-standard (don't correspond to CIs for non-mixed models) no single way to get them.  so we typically show confidence intervals.  
% 
% - Good to be aware of the basic distinction between confidence and prediction intervals because different packages (ggpredict, emmeans, plotInterval) do different things by default.
% 
% - using pre-existing method to calculate predictions/uncertainties is great but limits what we can actually predict.  Have seen most common case here (partial effects/interaction plots). more complex cases in next chapter.


We've now seen how to add uncertainties to predictions for the most common case (partial-effect and interaction plots).  This is also possible for by-participant predictions (e.g.,\ Figure~\ref{fig:neut-3-preds}), but this is a less common case so we leave it to a later chapter (Section~\ref{sec:by-sub-uncert}).






% 
% %% from my digging into ggeffects code:
% 
% %% - for merMod type='re' includes residual variance,
% %%   but 'fe' doesn't (I think). so 'fe' by default
% %%   is just incorporating SE of fixed effects
% %% https://rdrr.io/github/strengejacke/ggeffects/src/R/standard_error_predictions.R
% 
% 
% 
% ---
% -- *confidence interval*:   *population-level* prediction (across many populations, what values are likely for this param)? just incorporate uncertainty from fixed effects---analagously to R2-conditional.
% 
% -- *prediction interval*:  uncertainty about a new *observation*: incorporates uncertainty from fixed effects, but also individual observations -- either just residual variance, or all variance components (including residuals).  thus (sort of) incorporates fixed + random, analagous to R2-marginal (?) (this is the one that is harder to define and there are different ways; see predictInterval discussion.)
% 
% - Thus, confidence interval always narrower than prediction interval.

% 
% \section{Visualizing and reporting}
% 
% - Mixed models can give diff kinds of ``results'':
% 
% -- *population-level*: fixed effects (coeffiicents, or something based on them -- like post-hoc tests)
% 
% -- *group-level*: predictions for indiviudal group levels (uses fixed + ranefs)
% 
% -- *variability*: variance components
% 
% - For each of these, visualizing and reporting the model requires predicted values, and uncertainty (conf int, or p-values).
% 
% - predicted values fairly simple: use *predict*, etc. 
% 
% - uncertainty trickier, -- as for R2, the underlying issue is there are diff kinds of uncertainty that *could* be accounted for, depending on exactly what is being predicted.  Good discussion and packages available, and best practices evolving (Box).  (Or maybe just paste this from merMod predict: ``There is no option for computing standard errors...'' -- you can do it with bootstrapping, or various apprxoimat methods exist.)
% 
% - We focus in this section on a single case which is by far the most common at present in lang sciences:
% 
% -- of interest are 1+ *population-level* effects (\#1)
% 
% -- variance components (\#3) of less interest -- report values only
% 
% - for other cases, e.g.,\ where indiv variability of primary interest, what visualization and reporting makes sense will differ; (maybe) give an example in next chapter.

% 
% 
% \subsection{Model predictions}
% 
% - Adapt 9.4, but taking into account that we introduced model predictions long ago.  this section is actually pretty good... shouldn't have rewritten what's below :(
% 
% - ploting model *predictions* useful for fixed effects (as in previous models)
% 
% - How to put uncertainties on predictions depends on what is being predicted.  surprisingly sublt issue with good discussion elsewhere (Box above).   We will outline one option which is commonly used, then return to this issue next chapter (prediction vs. confidence intervals).
% 
% - Adapt text about this from 9.4 ``Predictions with confidence intervals''
% to define a confidence interval for  *population-level* prediction (across many populations, what values are likely for this param)? just incorporate uncertainty from fixed effects.
% 
% - As for other predictions: can use existing methods, or roll our own.
% 
% - One implementation is \ttt{ggpredict()}, with option `fe':
% 
% <<>>=
% ggpredict(vot_mod_2, terms = c('speaking_rate_dev'), type='fe')
% @
% 
% - varies specified parameter while holding others fixed; average word and speaker. Could also marginalize over other vars, one implementation is \ttt{ggemmeans()}:
% 
% <<>>=
% ggemmeans(neut_mod_3, terms = c('voicing'))
% @
% 
% plot showing these effects with CIs (calculated w and w/o marginalization)?
% 
% - Can also do predictions ourselves, for any input dataframe.  Simplest way is simulation from fitted model, using \ttt{simulate}.   (adapt text from 9.4.2).  Advantage is we aren't limited to pre-cooked options.



% <<>>=
% ## FUTURE: adjust covariates to be right for item 1
% newdata <- data.frame(expand.grid(voicing=c(-0.5, 0.5),
%                                   subject = unique(neutralization$subject),
%                                   prosodic_boundary='no',
%                                   place='alveolar',
%                                   vowel='a',
%                                   item_pair=1) )
% 
% 
% sims <- simulate(neut_mod_3, newdata=newdata, nsim=1000, 
%                  re.form=~(1+voicing|subject))
% 
% newdata$lower <- apply(sims, 1, function(x){quantile(x,0.025)})
% newdata$upper <- apply(sims, 1, function(x){quantile(x,0.975)})
% newdata$pred <- apply(sims, 1, function(x){quantile(x,0.5)})
% 
% ggplot(aes(x=voicing, y=pred, ymin=lower, ymax=upper), data=newdata) + geom_line(aes(group=subject)) +  geom_ribbon(aes(group=subject), alpha=0.1)
% @







% Two two basic options,similarly to R2:
% 
% ---
% -- *confidence interval*:   *population-level* prediction (across many populations, what values are likely for this param)? just incorporate uncertainty from fixed effects---analagously to R2-conditional.
% 
% -- *prediction interval*:  uncertainty about a new *observation*: incorporates uncertainty from fixed effects, but also individual observations -- either just residual variance, or all variance components (including residuals).  thus (sort of) incorporates fixed + random, analagous to R2-marginal (?) (this is the one that is harder to define and there are different ways; see predictInterval discussion.)
% 
% - Thus, confidence interval always narrower than prediction interval.
% 
% Example: 
% 
% - ggpredict implementation of confidence intervals:
% 
% <<>>=
% ggpredict(vot_mod_2, terms = c('speaking_rate_dev'), type='fe')
% @
% 
% - varies specified parameter while holding others fixed; average word and speaker.
% 
% 
% Practical notes:
% 
% - different implementations evolving, and do slightly different things.  at present: to get predictions which *marginalize* over other variables, use ggemmeans---this is usually what I do in practice, when showing model predictions.




\section{Reporting the fitted model}
\label{sec:reporting-lmms}

The following discussion assumes that you have arrived at a final model to be reported.  For broader discussion of what to report about the entire analysis process when using mixed-effects models---including the software used, your model selection process, etc.---we recommend \citet[][around Table 5]{meteyard2020best}, on which the discussion below is mostly based.
% FUTURE: also cite JARS-Quant / APA manual?
%(Box~\ref{box:reporting-lmms}).   

In general the minimal report of a fitted model should include:
\begin{enumerate}

\item A \textbf{definition of the model}, including specifying  its fixed and random-effect terms, and any additional information needed (e.g., whether random-effect correlations are included). A simple way to do this is providing the model formula in {lme4} format. Alternatively the definition could be written using equations (e.g.,\ equation~\ref{eq:lmm1}), especially for readers not familiar with {lme4} syntax. Verbal description of the model is useful, but not sufficient to unambiguously define the model.

\item \textbf{Fixed effects}: a table with one row per coefficient, containing its estimate, standard error and/or confidence interval, the associated test statistic, and $p$-value.   

\item \textbf{Random effects}: a table with the standard deviations and/or variances for each variance component, and the values of any correlations.  Reporting the residual term (SD and/or variance) is optional.  This information is important to report even when the random effects are not of direct interest, as is often the case.\footnote{It is fine to report such information in an appendix or online supplement.}
%(Box~\ref{box:reporting-lmms}).  
%In addition to providing estimates of group-level variability (which are criticial for subsequent studies of the same phenomenon, via meta-analysis or power analysis), this table makes unambiguous the random-effect structure of your model and can be important for critically evaluating \textbf{fixed} effects.
% this point is in the box:
% These are essential for future utility of the results, for power analysis or meta-analysis.

\item \textbf{Sample size}: number of observations, as well as the number of levels for each grouping factor.

\item One or more \textbf{quantitative summaries} of the model, including goodness of fit (e.g., marginal and conditional $R^2$).  It is common to include AIC, BIC, log-likelihood, and/or deviance as measure(s) of model fit; however these are only informative relative to other models of the same data.

\end{enumerate}


Although everything needed for this minimum report is shown by  various functions for summarizing the model we have discussed (\ttt{summary()}, \ttt{tidy()}, \ttt{glance()}, \ttt{r.squaredGLMM()}), none of these functions actually outputs \textbf{all} of this information, and none of them outputs well-formatted tables.\footnote{Various packages, such as {stargazer} and {texreg}, exist for formatting model output as tables for input into LaTeX or Word documents.}
%At the time of writing it takes some effort to get any package to output the kind of tables reported in language sciences (e.g.,\ Tables~\ref{tab:lmm-ex1-fixef}, \ref{tab:lmm-ex1-ranef}).} So you should not just paste the output of e.g.,\ \ttt{summary(vot\_mod3)} into a paper.

As an example, Tables~\ref{tab:lmm-ex1-fixef} and \ref{tab:lmm-ex1-ranef} shows all this information for \ttt{vot\_mod\_3} as `fixed-effect' and `random-effect' tables. 

Reporting mixed-effects models is more complex than non-mixed-effects models, and the exact form of your report will depend a lot on your data and research questions.  Additional information to the minimum above may be appropriate in your report: more detail (especially visualizations) of effects relevant for your research questions; effect-size measures or confidence intervals for the most important effects; descriptive statistics of the whole dataset or individual groups (e.g.,\ by-participant means for the effect of interest); and so on.

<<echo=FALSE>>=
vot_mod_3_lmerTest <- lmerTest::lmer(formula(vot_mod_3), data = vot_voiced_core)
vot_mod_3_summ <- vot_mod_3 %>% glance()
vot_mod_3_r2 <- r.squaredGLMM(vot_mod_3)
@

%A possible regression table for the model in Section~\ref{sec:ixn-example}:

\begin{table}
\textbf{Fixed effects:}

\begin{tabular}{lcrrrr}
\toprule
Coefficient & $\hat{\beta}$ & $SE(\hat{\beta})$ & $t$ & $df$ & $p$ \\
<<echo = FALSE, warning=FALSE, message=FALSE, results = 'asis'>>=
tidy(vot_mod_3_lmerTest) %>%
  filter(effect == "fixed") %>%
  dplyr::select(one_of("estimate", "std.error", "statistic", "df", "p.value")) %>%
  add_column(coeff = c("Intercept", "Speaking rate deviation", "Foll. high vowel", "Cons. cluster", "Corpus frequency (log)", "Place1 (alv. - lab.)", "Place2 (velar - non-vel.)", "Gender"), .before = 1) %>%
  printWrapper(pCol = "p.value")
@
\multicolumn{6}{p{10cm}}{Number of obs.: \Sexpr{nobs(vot_mod_3)}, groups:
word (\Sexpr{summary(vot_mod_3)$ngrps[['word']]}),
speaker (\Sexpr{summary(vot_mod_3)$ngrps[['word']]}). $p$-values/$df$ calculated using the Satterthwaite approximation.   } \\
    \multicolumn{6}{p{10cm}}{Model formula:
\ttt{log\_vot $~$ speaking\_rate\_dev + foll\_high\_vowel + cons\_cluster + 
    log\_corpus\_freq + 
    place + gender + (1 | word) + (1 + speaking\_rate\_dev | 
    speaker)}. } \\
\multicolumn{6}{p{10cm}}{Marginal $R^2$ = \Sexpr{vot_mod_3_r2[1]}, adjusted  $R^2$ = \Sexpr{vot_mod_3_r2[2]}.}
\end{tabular}
\caption{Example report of fixed effects and model fit, for \ttt{vot\_mod\_3}.  The model formula and $R^2$ values could alternatively be given in the text.} 
\label{tab:lmm-ex1-fixef}

\end{table}


% TODO-JT: un-hard-code this table (NB: extractRanefEst function))

\begin{table}
\textbf{Random effects:}

\begin{tabular}{llrrr}
\toprule
Group & Term & Variance & SD & Correlation \\
\midrule
Word & Intercept & \Sexpr{extractRanefEst(vot_mod_3, group = 'word', term='sd__(Intercept)')^2} & \Sexpr{extractRanefEst(vot_mod_3, group = 'word', term='sd__(Intercept)')} & \\
Speaker & Intercept & \Sexpr{extractRanefEst(vot_mod_3, group = 'speaker', term='sd__(Intercept)')^2} & \Sexpr{extractRanefEst(vot_mod_3, group = 'speaker', term='sd__(Intercept)')} & \\
& Speaking rate dev. & \Sexpr{extractRanefEst(vot_mod_3, group = 'speaker', term='sd__speaking_rate_dev')^2} & \Sexpr{extractRanefEst(vot_mod_3, group = 'speaker', term='sd__speaking_rate_dev')} & \Sexpr{extractRanefEst(vot_mod_3, group = 'speaker', term='cor__(Intercept).speaking_rate_dev')} \\
\midrule
Residual & & \Sexpr{extractRanefEst(vot_mod_3, group = 'Residual', term='sd__Observation')^2} & \Sexpr{extractRanefEst(vot_mod_3, group = 'Residual', term='sd__Observation')} & \\
\bottomrule
\end{tabular}
\caption{Example report of random effects, for \ttt{vot\_mod\_3}.}
\label{tab:lmm-ex1-ranef}

\end{table}

% \begin{boxedtext}{Broader context: Proper reporting of mixed-effects models}
% \label{box:reporting-lmms}
% 
% % FUTURE: revisit this box.. anything necessary? took out after moving some to text in 1/21, on Michaela advice, because it's repetitive from what's in text, and finger-wagging.
% 
% LMMs, and mixed-effects models generally, are more complex than vanilla regression models, and more decisions are required to end up at a final model, from the fitting algorithm used to the exact model structure chosen.  This is OK: ``there is no single correct way to implement an [MEM], and the choices [you] make during analysis will comprise one path, however justified, among multiple alternatives'' \citep{meteyard2020best}. This means reporting  MEMs is  more complex, and the exact form of your report will depend a lot on your data and research questions.  Additional information to the minimum above may be appropriate in your report: more detail (especially visualizations) of effects relevant for your research questions; effect-size measures or confidence intervals for the most important effects; descriptive statistics of the whole dataset or individual groups (e.g.,\ by-participant means for the effect of interest); and so on.  And for a given model your report will naturally focus on aspects of the results most relevant for your research questions.
% 
% However, it is crucial to meet the \textbf{minimum} standards above; ``to ensure the future utility of our findings, the community must adopt a standard format for reporting complete model outputs'' \citep{meteyard2020best}. The complexity of mixed-effects models means that it is not in general possible to reconstruct what the author did from a partial report (unlike non-mixed-effects models). For example, it is very common to not report the random effects   (variance components),  but without these terms it is impossible to use the results for meta-analysis or power analysis for future studies. In addition, the random-effect estimates can inform interpretation of the \textbf{fixed} effects. 
% %It is also common to not report the model equation (especially the structure of random effects), without which it is impossible to assess the fixed-effect estimates (i.e., is a random slope included for the crucial term for a research question?). 
% In a survey of 400 papers from 2013--16 citing \citet{baayen2008mixed},  \citeauthor{meteyard2020best} found that at least one of pieces (1)--(5) above is missing or incomplete in most papers they surveyed. (The same is true for my own publications.)  Proper reporting takes space, but elements less important for your research questions (e.g.,\ full random-effect tables) can be shunted to appendices or code/documents posted to online repositories, as necessary. In a short enough paper, you might even just report a couple of fixed-effect coefficients in the text, after a brief verbal description of the model and sample size.  What is important is that the full details are available somewhere.  The easiest way to do this is to just make your analysis code and the outputs of models it fits publicly available (e.g., in an OSF project).
% 
% %  
% % - While best practices for reporting mixed models are evolving, recent papers (Meteyard, etc.) offer an excellent starting point, and are worth reading in detail if you use MEMs regularly.  Still, there is unlikely to ever be a one-size-fits-all option given how flexible these models are.  For example, if your research questions focus on \textbf{random} effects, much of the advice above won't make sense.  Existing guidelines for language scientists/psychologists focus on analyses of data from experiments, and will require adjustment for other settings (e.g.,\ corpora, typological data).  
% % 
% % So it is important for every researcher who regularly uses mixed-effects models to learn over time how to best report models for *their* data: by following general guidelines, using relevant papers as models, and seeking guidance from colleagues or written sources that you
% % %(e.g.,\ quant-savvy colleagues, or written sources)
% % you trust.  For example, my own work primarily analyzes phonetic corpus data, and I have developed a style over time for writing these up, (hopefully) improving with each paper.
% 
% \end{boxedtext}
% % 
% 
% \emph{Here is a start:}
% 
% \paragraph{Default case}
% 
% Assuming the most common case, where fixed effects are of primary interest,reporting a mixed-effects model  entails (at minimum):
% 
% \begin{itemize}
% \item A coefficient table showing all fixed effects, with standard errors, test statistic (if using), $p$-values (if using)
% \item Number of observations ($n$) and number of levels per grouping factor.
% \end{itemize}
% 
% As for the coefficients of non-mixed-models, other information may be appropriate in your report: more detail (especially visualizations) of effects relevant for your research questions; effect-size measures or confidence intervals for the most important effects; and so on.
% 
% Probably 1+ measures of model performance should be reported (e.g.,\ AIC, deviance, $R^2_m$, $R^2_c$), but there is not currently a standard set.
% 
% Random-effect terms are not typically reported in detail \textbf{if} your research questions relate to fixed effects (as is usually the case). However it is a good idea to include the entire table of estimated variance components (`Random effects' in \ttt{lmer()} output), e.g.,\ as an appendix.\footnote{It is common (at least in language sciences) to omit discussion/reporting of random effects entirely, but these are a central part of the fitted model, without which the report is technically incomplete.} In addition to providing estimates of group-level variability (which we should as a field care more about; e.g., in subsequent studies of the same phenomenon), this table makes unambiguous the random-effect structure of your model and can be important for critically evaluating \textbf{fixed} effects.
% 
% 
% \paragraph{Other cases}
% 

% 
% Reporting conventions 
% 

\section{Model validation}
\label{sec:model-validation-lmms}

LMMs model $y$ at different levels---individual \textbf{observations}, or among \textbf{groups} (e.g.,\ by-participant variability)---and potential problems in the data and model may exist at each level.  This means there are more possible ways to validate a mixed-effects model, but the goal of model validation remains the same: to ``check whether the model is not grossly wrong'' (Section~\ref{sec:lr-assumptions}).

\subsection{Observation level}

Possible problems with LMMs at the observation level, and the diagnostic tools used to address them, are  mostly similar to (non-mixed-effects) linear regression (Section~\ref{sec:lr-assumptions}--\ref{sec:lr-problems-with-observations}): Q-Q plots to evaluate normality of residuals, examining Cook's distance for influential observations, etc.
%quantifying the degree of collinearity, etc.  

Here we show some examples using our final model of the VOT data (\ttt{vot\_mod\_3}). Note that this is not a full model validation, because the concepts are similar to the non-mixed-effects case.

It will be useful to refer to the model equation for observation $i$:
\begin{align}
y_i & = \beta_0 + \alpha_{S, j[i]}  + \alpha_{W, k[i]} +
\underbrace{(\beta_1 + \gamma_{S, j[i]}) x_{1,i}}_{\text{speaking rate}} + \nonumber \\
& + \underbrace{(\beta_2 x_{2,i} + \cdots + \beta_7 x_{7,i})}_{\text{other predictors}}
+ \epsilon_i ,
\label{eq:lmm4}
\end{align}
where the residuals $\epsilon_i$ are normally distributed, as are the by-word random intercepts $\alpha_{W,k}$ for word $k$; the by-speaker random intercepts and slopes $\alpha_{S,j}$, $\gamma_{S,j}$ for speaker $j$ follow a bivariate normal distribution.  $\beta_1$ is the fixed-effect coefficient for \ttt{speaking\_rate\_dev}, the predictor of primary interest; $x_{1,i}$ is the value of this predictor for observation $i$.  ``Other predictors'' refers to the other fixed effects (\ttt{foll\_high\_vowel}, etc.).

We add information useful for model validation to the dataframe: fitted values and residuals for each observation (columns \ttt{.fitted}, \ttt{.resid}, from \ttt{fortify.merMod()}), followed by Cook's distance and leverage:
% Important but in-the-weeds note:
%\footnote{In previous chapters we used \ttt{augment()} from {broom} for this purpose.  As of this writing there was a bug in how {broom.mixed} interacts with the {car} package which makes \ttt{augment()} (from broom.mixed) take prohibitively long to run, so we don't recommend using \ttt{augment()}.}


% 
% - As for non-mixed models \ttt{augment} in broom is a quick way to add information useful for model validation to the dataframe (e.g.,\ \ttt{.fitted}, \ttt{.residuals}, \ttt{.hat}, \ttt{.cooksd}):\footnote{The \ttt{augment} command should execute quickly.  As of this writing, this requires you to not have loaded the \ttt{car} package \textbf{after} {lme4}. Otherwise \ttt{broom.mixed} will compute influence measures (\ttt{.hat}, \ttt{.cooksd}) using functions from \ttt{car} which take a very long time, rather than the {lme4} version. This issue may be fixed by the time you read this, but it cost me hours!}
% 

% major bug spent 1-2 hours on -- if you load car before lme4 and broom.mixed, augment uses car methods for influence.merMod and it takes forever. we want broom.mixed:::augment to act as if car wasn't loaded.
% So I now load car, lmer, broom.mixed in Ch 2 to make sure in right order
% Edit, 1/21: no you don't (load in that order)... I think b/c augment has just been taken out of the current chapter entirely.
<<>>=
vot_mod_3_df <- fortify.merMod(vot_mod_3)
vot_mod_3_df$.cooksd <- cooks.distance(vot_mod_3)
vot_mod_3_df$.hat <- hatvalues(vot_mod_3)
@
 
% 
%  including predicted values (\ttt{.fitted}), residuals (\ttt{.residuals}), the predicted values using just fixed effects (\ttt{.fixed}), Cook's distance (\ttt{.cooksd}).  

\begin{boxedtext}{Broader context: More on LMM diagnostics}
Note that, like everything for mixed-effects models, residuals, leverage, and influence measures (e.g.,\ Cook's distance) can be defined in more than one way. The values used here are the {lme4} defaults, which are the simplest option (i.e., residuals are just $y_i - \hat{y}_i$) and evaluate quickly; see \citet{fox2019r} for details.  \citet{HLMdiag} (and references therein) discuss other possibilities.  This paper describes the excellent {HLMdiag} package, which contains the most extensive functionality for mixed-effects model diagnostics, but (as of this writing) does not fully work for models with crossed random effects. 

% Thsi paper describes the \ttt{HLMdiag} package, which currently contains the most extensive functionalityis excellent and Vignette is a great place to read more about model validation diagnostics for LMMs, but doesn't currently apply to models with crossed ranefs.) 

\end{boxedtext}


% 
% - Note (somewhere?): residuals, influence, and thus CD can be defined in $>1$ way; residuals, hatvalues, cooks.distance methods for lme4, which is what `augment' does, use the simplest possible (see Fox \& Weisberg, I think)---approximations.  (See HLMdiag Vignette, elsewhere for discussion of other possibilities.  Say somewhere: HLMdiag is excellent and Vignette is a great place to read more about model validation diagnostics for LMMs, but doesn't currently apply to models with crossed ranefs.)  The reason it matters to say *what* these do is that it changes over package versions -- e.g., hatvalues/cooks.distance didn't used to be calculated at all in lme4.

% - Residual plots: Q-Q plots, fitted-versus-residual plots, fixed-effect-predictor vs.\ residual plots
% 
% - Checking for influential/outlier observations: histograms of Cook's distance, hat values, etc.

\subsubsection{Residual plots}
\label{sec:lmm-resid-plots}

The augmented data frame can then be used to make residual diagnostic plots for the observation-level residuals ($\epsilon_i$ in equation~\ref{eq:lmm4}) such as those in Figure~\ref{fig:resid_plot_lmm_1}.  Also possible would be  fixed-effect-predictor versus\ residual plots, which are not shown.  

<<resid_plot_lmm_1,  out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Histogram of residuals (top-left), and three residual diagnostic plots for \\ttt{vot\\_mod\\_3}: Q-Q plot of residuals (top-right), fitted value-residual plot (bottom-left), scale-location plot (bottom-right).', echo=1>>=

vot_mod_3_df %>% ggplot(aes(x = .resid)) +
  geom_histogram() +
  xlab("Residuals")

vot_mod_3_df %>% ggplot(aes(sample = .resid)) +
  geom_qq(color='darkgrey', alpha=0.5) +
  geom_qq_line(color = default_line_color, size=0.75) +
  xlab("Theoretical quantiles") +
  ylab("Residuals")

vot_mod_3_df %>% ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(color='darkgrey', size = 0.1, alpha=0.5) +
  geom_smooth(color = default_line_color, se = F) +
  geom_hline(aes(yintercept = 0), lty = 2) +
  xlab("Fitted values") +
  ylab("Residuals")

vot_mod_3_df %>% ggplot(aes(x = .fitted, y = sqrt(abs(.resid)))) +
  geom_point(color='darkgrey', size = 0.1, alpha=0.5) +
  geom_smooth(color = default_line_color, se = F) +
  xlab("Fitted values") +
  ylab(expression(sqrt(abs("Residual"))))
@


The Q-Q plot (top-right) suggests the residuals are not normally distributed.  Inspecting a histogram of the residuals (top-left) suggests the distribution is slightly right-skewed, but not unreasonably far from normal (Exercise~\ref{ex:qq-plot-expl}).
%discusses a potential explanation due to how the data was coded, which affects observations with low values of \ttt{log\_vot}.

A fitted value-residual plot (bottom-left) looks OK---reasonably close to a horizontal line.  A scale-location plot (bottom-right) suggests a small deviation from model assumptions; the residuals may have lower variance for very high \ttt{log\_vot}.

So there are some small issues to look into, particularly for very high or low $y$. These may not signal any problem---we are looking for large deviations, and residual plots for (G)LMMs tend to show some deviation from the ideal even for correctly-specified models \citep{loy2017model}. Alternatively the issues we observe might be related to other predictors/interactions predictive of log(VOT), which are used in the model reported in \citet[][Table A1]{sonderegger2017medium} but omitted here for simplicity.  

%% citation for that: look somewhere in HLMdiag author papers, DHARMa vignette 


%%  vot_mod_3_df %>% ggplot(aes(x=.fitted, y=sqrt(abs(.resid)))) + geom_point() + 
%% #   geom_smooth(se=F) + 
%% #   xlab("Fitted values") + ylab(expression(sqrt(abs('residual'))))


%%
% Notes for exercise:
% 
% Examine histogram of log_vot:
% vot_mod_3_df %>% ggplot(aes(x=log_vot)) + geom_histogram()
% 
% why is there a spike, then distribution looks different to the left? trns out this is log(5).  Measurement procedure involved a default of 5, which annotators could adjust up or downwards, but in practice annotators tended to only adjust upwards, so 5 usually meant ``at most 5''.  Unclear how we should deal with this issue, but note that the residuals issue goes away if considering rest of data
% 
% filter(vot_mod_3_df, log_vot>log(5)) %>% ggplot(aes(sample=.resid)) + 
%   geom_qq() + geom_qq_line() + 
%   xlab("Theoretical quantiles") + ylab("Residuals")
% 
% So perhaps this is effectively a good model of VOT for VOT>5

\subsubsection{Checking collinearity, influential observations}

The metrics introduced for non-mixed-effects models, condition number ($\kappa$) and the SIF (Section~\ref{sec:condition-number}, \ref{sec:collin-diagnostics-2}), generalize to mixed-effects models for assessing the effect of collinearity on \textbf{fixed-effect} estimates 
\citep[][\S8.8]{fox2019r}.

For example, for \ttt{vot\_mod\_3}, we can calculate the condition number for the fixed-effect predictors ($x_1, \ldots, x_7$ in equation~\ref{eq:lmm4}):
<<>>=
## Matrix of fixed-effect predictor values, minus the intercept
my_data <- data.frame(model.matrix(vot_mod_3))[, -1]
collin.fnc(my_data)$cnumber
@


Or a SIF value for each of their coefficients ($\beta_1, \ldots, \beta_7$):
<<>>=
## SIF = rightmost column
car::vif(vot_mod_3)
@

Both measures indicate low collinearity ($\kappa<5$, highest SIF$<$3.2).

To assess unusual observations, Figure~\ref{fig:influence_plot_lmm_1} shows histograms of leverage (\ttt{.hat}) and influence (\ttt{.cooksd}) of each observation, with the $y$-axis cut off for legibility given the large number of observations (most points have \ttt{.hat}/\ttt{.cooksd} near 0). Neither plot suggests a subset of observations which are much more influential than others.  


<<influence_plot_lmm_1,  echo=FALSE, out.width='40%',fig.width=default_fig.width*.40/default_out.width,  fig.cap='Distribution of observation-level leverage and influence measures for model \\ttt{vot\\_mod\\_3}.'>>=
## FUTURE: make the y-axes log scale, but deal with the issue that ## log 1 counts not dealt with correctly

##
vot_mod_3_df %>% ggplot(aes(x = .hat)) +
  geom_histogram(bins = 50) +
  ylab("Count (log)") +
  xlab("Leverage") +
  coord_cartesian(ylim = c(0, 100))

vot_mod_3_df %>% ggplot(aes(x = .cooksd)) +
  geom_histogram(bins = 50) +
  ylab("Count (log)") +
  xlab("Cook's distance") +
  coord_cartesian(ylim = c(0, 100))
@



\subsection{Group-level}

The new aspect of model validation for LMMs concerns group-level variability.  There are three kinds of assumptions to be checked here (\citealp{HLMdiag}; \citealp[][\S4.6, 10.2, 10.6--10.7]{snijders2011multilevel}; \citealp{antonakis2021ignoring}):
\begin{enumerate}
\item Random-effect terms are normally distributed (e.g., the by-word random intercepts: $\gamma_{W,j}$ in equation~\ref{eq:lmm4}).
\item No group level is overly influential (e.g., observations from one word).
\item Random-effect terms are uncorrelated with the predictors (e.g.,\ \ttt{speaking\_rate\_dev} is uncorrelated with by-speaker random intercepts, across observations: $cov(x_{1,i}, \gamma_{S, J[i]})=0$ in equation~\ref{eq:lmm4}).
\end{enumerate}


% FUTURE: Michaela -- can this section be shortened? gets repetitive especially in second half.
\subsubsection{Random-effect distributions}
\label{sec:lmm-ranef-distributions}

LMMs always assume that random effects are drawn from a normal distribution, with mean 0 and some variance.  We can check this assumption by examining the distribution of the random-effect estimates (the BLUPs) for a model using histograms or Q-Q plots.  This check is imperfect, because this distribution often looks non-normal even when the model is correct (e.g., simulated data),
%from data with normally-distributed random effects), 
due to confounding between different random-effect terms \citep{loy2015you}. In addition,
 LMMs are surprisingly robust to non-normal random-effect distributions (e.g.,\ skewed, bimodal): simulation studies suggest that fixed-effect coefficient estimates/standard errors at least are typically not affected
 %, while random effects can be
% , while random-effect parameters (i.e., the variance components) can be, and estimated random effects are greatly affected 
 (Box~\ref{box:non-normal-ranefs}). So if we are mostly interested in fixed effects, some non-normality of random effects is OK.

Nonetheless,  like checking residual plots, checking normality of random effects has benefits besides making sure your coefficient estimates are correct---it also often shows issues with the data or the model
%(unusual participants/items/etc.), or the model (e.g.,\ omitted predictors) 
which should be investigated further.
% (Suggests we can't say much except from extreme deviations in Q-Q plots)

For example, this code extracts BLUPs for the by-word random intercepts for 
%types of random effects in 
model \ttt{vot\_mod\_3}:
<<ranef_norm_plot_lmm_1, echo=1:7, output.lines=1:4, fig.asp=1, out.width='65%',fig.width=default_fig.width*.65/default_out.width, fig.cap='Q-Q plots for estimated random effects (BLUPs) from \\ttt{vot\\_mod\\_3}: by-word (top) and speaker (bottom-left) random intercepts, and by-speaker \\ttt{speaking\\_rate\\_dev} random slopes (bottom-right).'>>=
## broom.mixed method to extract BLUPs in tidy format
vot_mod_3_ranef <- tidy(vot_mod_3, effects = "ran_vals")

vot_mod_3_ranef

# alternatively can use e.g., ranef(vot_mod_3)$word
# to get by-word random effect BLUPs

# Q-Q plot for by-word random intercepts
p1 <- vot_mod_3_ranef %>%
  filter(group == "word") %>%
  ggplot(aes(sample = estimate)) +
  geom_qq() +
  geom_qq_line() +
  xlab("Theoretical quantiles") +
  ylab("Est. by-word\nrandom intercepts")

p2 <- vot_mod_3_ranef %>%
  filter(group == "speaker" & term == "(Intercept)") %>%
  ggplot(aes(sample = estimate)) +
  geom_qq() +
  geom_qq_line() +
  xlab("Theoretical quantiles") +
  ylab("Est. by-speaker\nrandom intercepts")

p3 <- vot_mod_3_ranef %>%
  filter(group == "speaker" & term == "speaking_rate_dev") %>%
  ggplot(aes(sample = estimate)) +
  geom_qq() +
  geom_qq_line() +
  xlab("Theoretical quantiles") +
  ylab("Est. by-speaker\nrandom slopes") 

p1 / (p2 + p3) + plot_layout(heights=c(1.75,2))
@
%%## by-speaker : looks OK.  By-word: some odd lower points

The Q-Q plots for all three types of random effects in \ttt{vot\_mod\_3} are shown  in Figure~\ref{fig:ranef_norm_plot_lmm_1}.
%(Plotting code for the mid/right panels not shown.)
%
%%data.frame(ranef(vot_mod_3)$word) %>% dplyr::arrange(`X.Intercept.`) %>% head()
%% ## these are 'dude', 'glad'
%
%%## refitting model without these it looks a bit better:
%% update(vot_mod_3, data=filter(vot_voiced_core, !word%in%c('dude', 'glad'))) %>% plot_ranef()
%
%% ## but still seems clear there is some word-level variability we're not accounting for properly
%
The distributions of the by-speaker random effects (bottom panels) look basically normal.  The distribution of the by-word random intercepts (top panel) looks further from normal, due especially to the two words with the lowest values:
<<output.lines=1:3>>=
filter(vot_mod_3_ranef, group == "word") %>% arrange(estimate)
@

These words should be flagged for further consideration.  Excluding observations corresponding to these words from the model does not change any qualitative results (Exercise~\ref{ex:exclude-words}), but still gives non-normally distributed by-word random intercepts; the histogram and Q-Q plot are shown in Figure~\ref{fig:ranef_norm_plot_lmm_2} (top, bottom left).  It looks like there are `tails' on both the left and right, suggesting there may be word-level variability which we are not accounting for in this data (i.e.,\ missing word-level predictors). 

<<echo=FALSE>>=
## Models for the next chunk
vot_voiced_core %>%
  filter(!duplicated(speaker)) %>%
  select(speaker, gender) %>%
  rename(level = speaker) -> spkr_info

vot_mod_3_no_dude_glad <- update(vot_mod_3, 
  subset = !word %in% c("dude", "glad"))

vot_mod_3_no_gender <- update(vot_mod_3, . ~ . - gender)
@


<<ranef_norm_plot_lmm_2, echo=FALSE, fig.asp=1, out.width='65%', fig.width=default_fig.width*.65/default_out.width, fig.cap='Q-Q plot (top) and histogram (bottom left) of estimated by-word random effects for \\ttt{vot\\_mod\\_3} after excluding observations with \\ttt{word} = \\tsc{dude} or \\tsc{glad}; Q-Q plot (bottom right) of estimated by-speaker random intercepts for the model refit without the \\ttt{gender} fixed effect.'>>=
p1 <- vot_mod_3_no_dude_glad %>%
  tidy(effects = "ran_vals") %>%
  filter(group == "word") %>%
  ggplot(aes(x = estimate)) +
  geom_histogram() +
  xlab("Est. by-word\nrandom intercepts")

p2 <- vot_mod_3_no_dude_glad %>%
  tidy(effects = "ran_vals") %>%
  filter(group == "word") %>%
  ggplot(aes(sample = estimate)) +
  geom_qq() +
  geom_qq_line() +
  xlab("Theoretical quantiles") +
  ylab("Est. by-word\nrandom intercepts")

## FUTURE: replace with Q-Q plot but showing gender as color
p3 <- vot_mod_3_no_gender %>%
  tidy(vot_mod_3, effects = "ran_vals") %>%
  filter(group == "speaker" & term == "(Intercept)") %>%
  ggplot(aes(sample = estimate)) +
  geom_qq() +
  geom_qq_line() +
  xlab("Theoretical quantiles") +
  ylab("Est. by-speaker\nrandom intercepts")

p2 / (p1 + p3) + plot_layout(heights=c(1.75,2))
@


Such deviations from normality can be useful for finding problematic data, e.g.,\ a participant who didn't do the task correctly (in experimental data).  They may also signal an important omitted speaker-level (or item-level, etc.) variable. For example, Figure~\ref{fig:ranef_norm_plot_lmm_2} (bottom right) shows the (non-normal) distribution of by-speaker random intercepts when a model without a \ttt{gender} term is fitted to this data. This plot suggests a possible missing speaker-level variable. Supposing we didn't know which one was missing, we could make different plots of candidate variables versus the random intercepts, and eventually find that the random intercepts divide almost perfectly by speaker \ttt{gender}, suggesting this is an important omitted variable.
% (supposing we didn't know when we fit the model that the predictor was ``missing'', we would figure this out by seeing ).  thus, if we had fitted a model without this predictor, the  The random intercepts turn ou Plotting the random intercept values against speaker-level predictors would show that the values divide almost perfectly by speaker gender It turns out that the random intercepts here in a non-normal distribution for by-speaker intercepts, whose structure suggests trying adding \ttt{gender} to the model. 
% 
% However, it is common to have non-normal random effect distributions which do not signal obvious `problems'---how much does this actually matter?
% The simple answer is that fixed-effect coefficient estimates/standard errors are typically not affected, while random-effect parameters (i.e., the variance components) can be (Box XXX). So if we are mostly interested in fixed effects, some non-normality of random effects is OK.


% FUTURE: compress this box?
\begin{boxedtext}{Broader context: The effect of non-normal random effects}
\label{box:non-normal-ranefs}
Simulation studies have addressed how robust LMMs, and to a lesser extent GLMMs, are to non-normal random-effect distributions (and to violations of distributional assumptions generally: \citealp{schielzeth2020robustness,maas2004robustness}, and references therein), such as skewed or bimodal distributions. In general: fixed-effect coefficient estimates are little-affected (at least for LMMs), which makes intuitive sense---the population-level effects, which average across group levels, should be roughly independent of the exact structure of group-level variability.  Random-effect variance estimates are unbiased but less reliable (higher standard errors), and estimated random effects (BLUPs for individual levels) can be greatly affected.  This is all based on simulations which may not generalize to your data; for example, most simulation studies assume that 30 levels for a grouping factor is `low', while much linguistic data has $<$30 levels.  

But the general points seem likely to hold, so it is a reasonable working assumption that non-normality of random effects doesn't matter much if you are just interested in fixed effects.  This does not absolve you of checking random-effect distributions, but should offer reassurance that your model's results are robust to an assumption which is difficult to check and often doesn't hold in practice.


% 
% 
% Mass \& Hox (REF) examine via simulations how much various non-normal residual distributions (e.g.,\ chi-squared) affect coefficeint estimates for a simple LMM, with varying numbers of groups. Based on their results, In general, Meteyard \& Davies state that ``non-normally distributed random effects do not substantially affect the estimation of fixed effect coefficients but do affect the reliability of the variance estimates for the random effects themselves''.  This seems like a good working assumption, and makes intuitive sense: the population-level effects, which average across group levels, should be roughly independent of the exact structure of group-level variability.  However, note that this is based on just one simulation study, and may not be generally applicable. (For example, to my knowledge there is no work examining how coefficeint estimates are affected in LMMs with crossed random effects.)  
% 
%
% - Also incorporate Loy and Hoffman 2015: often get non-normal-looking ranef distributions *even for simulated data* (from normally-distributed ranefs) due to confounding between the diff error terms.  (Suggests we can't say much except from extreme deviations in Q-Q plots)

As a concrete example, consider  model \ttt{vot\_mod\_3} (Exercise~\ref{ex:exclude-words}): when data from just the words \tsc{dude} and \tsc{glad} are excluded (out of \Sexpr{length(unique(vot_voiced_core$word))} words), the estimated variance of the by-word random intercepts decreases by 1/3, while the confidence interval narrows (lower standard error).  This makes sense: because the model assumes random effects are normally distributed, the only way to account for a few outlier words is to increase the random-effect variance.  However, the fixed effects (estimates, standard errors) barely change.   \citet[][\S7.9]{qmld} shows another example.

% 
% If we assume that these words are outliers,  
% %from a `true' normally-distributed 
% %
% %- Conf int on SD: 0.08-0.17 for mod3, 0.069-0.142 
% the full model's estimate of this variance is too high. (Because the model assumes random effects are normally distributed, the only way to account for a few outlier words is to increase the random effect variance.)  Even if we don't assume this, the non-normality will cause the estimate of $\sigma_{W,0}^2$ to have a high standard error.

\end{boxedtext}




\subsubsection{Influence}
\label{sec:lmm-ranef-influence}

For non-mixed models (Section~\ref{sec:influence}) we used Cook's distance as a measure of influence, roughly defined as ``how much do the model's coefficients ($\hat{\beta}$s) change when observation $i$ is removed?'' The related DFBETA measure quantifies influence with respect to one predictor ($x_1$): ``How much does the coefficient $\hat{\beta}_1$ change when observation $i$ is removed?''

Cook's distance and DFBETAs can be similarly defined for mixed-effects models, at each level of the model:
\begin{itemize}
\item How much do the fixed-effect coefficients (or a particular coefficient) change when observation $i$ is removed?

\item or: when participant $j$ is removed?

\item or: when item $k$ is removed?
\end{itemize}

That is, there can be influential participants, items (or level of any grouping factor: words, etc.), or observations.  We assume here that fixed-effect coefficients are of interest, but similar measures can be computed to quantify influence on the variance components. These can all be computed for {lme4} models using functions from the {car} package.

We considered influential observations above; typically it is of more interest to check for influential groups (e.g.,\ speakers).  Unlike for non-mixed models, there is no formula to compute Cook's distance (or DFBETA) for this case; the model must be refitted without a given participant/item/etc. to check how much the model changes.  The {influence()} function from the {car} package (see \ttt{?influence.mixed.models})  automates this process, using some optimizations that make it faster than simply refitting the model $J$ times for $J$ speakers.
%\footnote{Nonetheless, 
%\ttt{influence} can take a very long time, and 
%you may want to use parallel processing if your computer has multiple cores; see  \ttt{?influence.mixed.models}.}

For model \ttt{vot\_mod\_3}, we first compute objects which contain all the information to compute by-speaker and by-word influence diagnostics:

<<eval=FALSE>>=
alt_est_spk <- influence(model = vot_mod_3, group = "speaker")
alt_est_word <- influence(model = vot_mod_3, group = "word")
@


<<vot_mod_3_infl, echo=FALSE>>=
f_name_1 <- "alt_est_spk.rds"
f_name_2 <- "alt_est_word.rds"

f_path_1 <- paste("objects/", f_name_1, sep = "")
f_path_2 <- paste("objects/", f_name_2, sep = "")


do_calc <- !file.exists(f_path_1) || !file.exists(f_path_2) 


if (do_calc) {
  alt_est_spk <- influence(model = vot_mod_3, group = "speaker")
  alt_est_word <- influence(model = vot_mod_3, group = "word")
  
  saveRDS(alt_est_spk, file = f_path_1)
  saveRDS(alt_est_word, file = f_path_2)
} else {
  alt_est_spk <- readRDS(f_path_1)
  alt_est_word <- readRDS(f_path_2)
}
@


The following code extracts by-speaker Cook's distance and by-word DFBETA for the \ttt{speaking\_rate\_dev} fixed-effect coefficient (which is of primary interest), and puts them into a legible dataframe:

<<output.lines=1:5>>=
## Extract dfbetas as a numeric vector
dbS <- data.frame(dfbeta(alt_est_spk))[, "speaking_rate_dev"]
spk_infl_df <- data.frame(
  dfbeta = dbS,
  cd = cooks.distance(alt_est_spk),
  speaker = alt_est_spk$deleted
)
@

Similarly for by-word influence measures:

<<output.lines=1:5>>=
dbS <- data.frame(dfbeta(alt_est_word))[, "speaking_rate_dev"]
word_infl_df <- data.frame(
  dfbeta = dbS,
  cd = cooks.distance(alt_est_word),
  word = alt_est_word$deleted
)
@

For example, the speakers with the highest DFBETA values and the words with the highest Cook's distance (CD) values are:

<<output.lines=1:5>>=
arrange(spk_infl_df, -abs(dfbeta))
@
<<output.lines=1:5>>=
arrange(word_infl_df, -abs(cd))
@


<<influence_plot_lmm_2,  echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Dotplots of measures of influence of each speaker (top row) and each word (bottom) on the fixed effects of model \\ttt{vot\\_mod\\_3}: Cook\'s distance (left column) and DFBETA for the \\ttt{speaking\\_rate\\_dev} coefficient (right). Vertical lines are $Q_3 + 3 \\cdot IQR$ cutoffs.'>>=

internalCutoff <- function(k) {
  quantile(k, 0.75) + 3 * IQR(k)
}

wordCdCutoff <- internalCutoff(word_infl_df$cd)
spkCdCutoff <- internalCutoff(spk_infl_df$cd)

wordDfBetaCutoff <- internalCutoff(word_infl_df$dfbeta)
spkDfBetaCutoff <- internalCutoff(spk_infl_df$dfbeta)


spk_infl_df %>% ggplot(aes(x = cd, y = reorder(speaker, cd))) +
  geom_point() +
  geom_vline(aes(xintercept = spkCdCutoff), lty = 2) +
  xlab("Cook's distance") +
  ggtitle("By-speaker") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

spk_infl_df %>% ggplot(aes(x = abs(dfbeta), y = reorder(speaker, abs(dfbeta)))) +
  geom_point() +
  geom_vline(aes(xintercept = spkDfBetaCutoff), lty = 2) +
  xlab("|DFBETA| for Speaking rate") +
  ggtitle("By-speaker") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

# FUTURE: get the by-word plots to look similar / not cut off.  Can we at least get them to have similarly-grey backgrounds, and not have points near the top cut off?

word_infl_df %>% ggplot(aes(x = cd, y = reorder(word, cd))) +
  geom_point() +
  geom_vline(aes(xintercept = wordCdCutoff), lty = 2) +
  xlab("Cook's distance") +
  ggtitle("By-word") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

word_infl_df %>% ggplot(aes(x = abs(dfbeta), y = reorder(word, abs(dfbeta)))) +
  geom_point() +
  geom_vline(aes(xintercept = wordDfBetaCutoff), lty = 2) +
  xlab("|DFBETA| for Speaking rate") +
  ggtitle("By-word") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

#
# spk_infl_df %>% ggplot(aes(x=reorder(speaker,cd), y=cd)) + geom_point() + geom_segment(aes(xend=speaker, y=0, yend=cd)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), axis.title.x = element_blank())
#
# spk_infl_df %>% ggplot(aes(x=reorder(speaker,speaking_rate_dev), y=abs(speaking_rate_dev))) + geom_point() + geom_segment(aes(xend=speaker, y=0, yend=speaking_rate_dev)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), axis.title.x = element_blank()) + ylab("DFBETA for speaking_rate_dev") + ggtitle("By-speaker")
#
# word_infl_df %>% ggplot(aes(x=cd)) + geom_histogram() + xlab("Cook's distance")+ ggtitle("By-word")
#
# word_infl_df %>% ggplot(aes(x=speaking_rate_dev)) + geom_histogram() + xlab("DFBETA for speaking_rate_dev")+ ggtitle("By-word")
@

Figure~\ref{fig:influence_plot_lmm_2} shows the values of CD and $|$DFBETA$|$ (for \ttt{speaking\_rate\_dev}) for speakers and words, using dotplots, which are a useful way to visualize influence measures \citep{HLMdiag}.\footnote{CD values are always positive, while DFBETA values can be negative or positive ($\hat{\beta}_1$ decreases/increases when word $i$ is removed). It is usually more helpful to plot the absolute value of DFBETA values.}
%($|$DFBETA$|$).}  

As for non-mixed models: we can use either visual inspection or a `cutoff' value to decide which speakers/words to flag as overly influential.
%% actually got value from Nieuwenhuis et al. 2012 paper on influence.ME
Some possible cutoffs are \citep{belsley2004regression,HLMdiag}:
\begin{itemize}
\item Cook's distance: $4/k$, where $k$ is the number of levels of the grouping factor
\item $|$DFBETA$|$: $2/\sqrt{k}$ 
\item Either CD or $|$DFBETA$|$: $Q_3 + 3 \cdot IQR$ (e.g., 75th percentile + $3 \cdot$ inter-quartile range of the CD values)
\end{itemize}

Figure~\ref{fig:influence_plot_lmm_2} shows $Q_3 + 3 \cdot IQR$ cutoffs. It is visually clear that the cutoffs aren't very helpful here: they flag no speakers and too many words to practically investigate. Using visual inspection instead and looking for gaps in the distributions, we could flag for further consideration the speakers with the highest CD and $|$DFBETA$|$ values, and the words  with very high CD and/or $|$DFBETA$|$ values:

<<>>=
filter(word_infl_df, cd > 0.12 | abs(dfbeta) > 0.009) %>%
  select(word)

filter(spk_infl_df, cd > 1.0 | abs(dfbeta) > 0.01) %>%
  select(speaker)
@

(One could alternatively use a more conservative visual cutoff and select more words/speakers.)

Note that these include the words we flagged above based on non-normal residuals.

At this point we have flagged data from four levels of \ttt{word} (\tsc{dude}, \tsc{glad}, \tsc{but}, \tsc{drunk}) and two levels of \ttt{speaker} (\tsc{dale}, \tsc{darnell}) as potentially problematic (very influential or non-normal residuals) for model \ttt{vot\_mod\_3}.
Exactly what to do next is harder, as for non-mixed-effects models (Section~\ref{sec:what-to-do-influential}): there are different philosophies, and what makes sense depends on the particular data/research questions, but there are some minimum steps for good model hygiene.

For our VOT example, since the \ttt{speaking\_rate\_dev} fixed-effect coefficient is of primary interest, it makes sense to minimally check how the flagged speakers/words affect this term.

For example, excluding data from either flagged speaker dramatically changes the \ttt{speaking\_rate\_dev} estimate and its $|t|$ value:

<<>>=
## Original model
tidy(vot_mod_3) %>% 
  filter(effect == "fixed" & term == "speaking_rate_dev")
@
<<>>=
## Exclude speaker='dale'
update(vot_mod_3, subset = speaker != "dale") %>%
  tidy() %>%
  filter(effect == "fixed" & term == "speaking_rate_dev")
@
<<>>=
## Exclude speaker='darnell'
update(vot_mod_3, subset = speaker != "darnell") %>%
  tidy() %>%
  filter(effect == "fixed" & term == "speaking_rate_dev")
@

The estimate $\hat{\beta}_1$ varies by about 25\%  and $|t|$ goes from `significant to `not significant' (by the $|t|<2$ rule), depending on which speaker is included. Excluding words also has some effect (Exercise~\ref{ex:exclude-words}). Thus, the qualitative conclusion we would draw from this model about our RQ (`is there a speaking rate effect?') is not robust to excluding influential speakers. This should be reported in any write-up of this data.

%, for example to argue that no firm conclusion about a speaking rate effect one way or another can be made from this data.







% words: *but*, also do, god, day...
% filter(word_infl_df, abs(speaking_rate_dev)>0.2)

% deleting `but' increases speaking_rate_dev effect






%% FUTURE: Michaela - this section feels like it could be condensed.  "I'm not sure the example is necessary here."
\subsubsection{Uncorrelated random effects/predictors}
\label{sec:uncorr-ranef-preds}

Mixed-effects models assume that the random effects are uncorrelated with the predictors.
%(e.g.,\ \citealp{antonakis2021ignoring}, \citealp[][\S4.6, 10.2.1]{snijders2011multilevel}).   
Intuitively, any kind of by-group variability modeled in the random effects (e.g.,\ by-participant intercept) should not be correlated with a predictor (e.g.,\ speaking rate). Such correlations most commonly occur when the predictor differs systematically by group (e.g., speakers differ in mean speaking rate).

For example, consider a simpler version of \ttt{vot\_mod\_3}, where we model the effect of $x_1$ = \ttt{speaking\_rate} on $y$ = VOT, using a fixed effect of $x_1$ and a by-speaker random intercept.\footnote{Note that \ttt{speaking\_rate} is different from \ttt{speaking\_rate\_dev}---see note \ref{fn:speaking-rate-dev}.}
%which is the deviation of \ttt{speaking\_rate} from this speaker's mean.}  
If speakers differ in \ttt{speaking\_rate}, as seems likely, the model can't tell whether the effect of $x_1$ on $y$ is due to change in $x_1$ itself (the fixed effect) or speaker differences (the random effect). Technically this problem is called `endogeneity bias', and is potentially serious for the fixed-effect coefficients: we lose any possibility of a causal interpretation of $\beta_1$, and its estimate can be inaccurate \citep{antonakis2021ignoring}.  We will call this the \emph{non-endogeneity assumption}.
%\footnote{This is sometimes called `the random-effects assumption', but this terminology is confusing given we are discussing several LMM assumptions related to random effects.}

Fortunately, there is a simple solution: adding by-group averages of $y$  as a predictor.  This changes the interpretation of the random effect, in a similar way to other group-level predictors (Section~\ref{sec:changes-variance-components}), so they can no longer be correlated with $x_1$.  For the \ttt{vot\_mod\_3} example we would add the speaker-level predictor \ttt{speaking\_rate\_mean}, which is the speaker's average for \ttt{speaking\_rate} across the dataset:\footnote{The interpretation of each speaker's random intercept is now ``offset in \ttt{log\_vot} from what is expected from their \ttt{speaking\_rate\_mean} and \ttt{gender}''.}
%In this case the dataset happens to already contain a predictor of by-group averages. Typically we wouiked need to code a new predictor.} 

<<>>=
vot_mod_4 <- update(vot_mod_3, . ~ . + speaking_rate_mean)
@

The non-endogeneity assumption can be tested by a model comparison: 

<<output.lines=5:7>>=
anova(vot_mod_4, vot_mod_3)
@
Because \ttt{speaking\_rate\_mean} does not significantly improve the model, we can leave it out, and consider the non-endogeneity assumption satisfied in this case.
% (by-speaker intercepts uncorrelated with \ttt{speaking\_rate}).   If it had improved the model, we would need to leave \ttt{speaking\_rate\_mean} in the model to avoid endogeneity bias---even if we are not interested in its effect per se.
%\footnote{\citet[][\S3.6, 4.6]{snijders2011multilevel} discuss such `within-group' versus `between-group' predictors for the same variable more generally,  and how they can capture qualitatively different aspects of the same phenomenon (e.g., ``how does speech rate affect VOT?'').}

The same problem extends to random slopes of $x_1$, with a similar solution. For our example, by-speaker variability in the effect of \ttt{speaking\_rate} could in fact be due to its effect differing depending on a speaker's mean speaking rate ($\bar{x}_1$).  To check whether this is a problem, we can test whether adding a $x_1:\bar{x}_1$ interaction improves the model.  For model \ttt{vot\_mod\_3}:

% FUTURE: redo with KRmodcomp?
<<output.lines=5:7>>=
vot_mod_5 <- update(vot_mod_3, . ~ . + 
    speaking_rate_mean * speaking_rate_dev)
anova(vot_mod_3, vot_mod_5)
@

The high $p$-value suggests that endogeneity bias is not a problem for interpreting the \ttt{speaking\_rate\_dev} term of interest, so we can proceed with \ttt{vot\_mod\_3}.

% We have just considered one kind of endogeneity bias in a simple model (one random effect, one fixed effect). In a more realistic model there would be many possible sources of endogeneity bias (e.g.,\ \emph{word}-level differences in VOT).   In practice endogeneity bias is rarely checked for linguistic data in current practice, and I am not sure how important it is (Box~\ref{box:ranef-pred-corr}).




\begin{boxedtext}{Broader context: Random effect-predictor correlations, group-level averages}
\label{box:ranef-pred-corr}

%Like heteroskedasticity (Section~\ref{sec:constancy-variance-assumption}), 
Endogeneity bias  is an issue which receives a lot of attention in data analysis in some other fields---such as economics (where it is called the `random effects assumption')---on par with multicollinearity.  Endogenity bias is rarely checked for linguistic data.  Whether endogeneity bias is important for  research questions typically asked of linguistic data seems like a good direction for future work.

Since there are many possible sources of endogeneity in models of realistic data (multiple predictors, for each random-effect term) it seems unrealistic to check for all of them.
%More  or how one would go about really checking for endogeneity bias in practice, given the large  The number of possible kinds of endogeneity
In practice it seems 
%(to me) 
most important to check endogeneity bias for predictors of interest for the research questions, if they can reasonably vary by-group.  For example, for \ttt{vot\_mod\_3} it makes sense to consider by-speaker means of \ttt{speaking\_rate}, but less so by-word means (there is no reason words should vary in speaking rate).

Incorporating group-level averages as predictors is not only a way to check endogeneity assumptions: when adding these predictors is justified, one's findings can change in interesting ways. \citet[][\S3.6, 4.6]{snijders2011multilevel} discuss how the `within-group' and `between-group' predictors for the same variable (e.g.,\ \ttt{speaking\_rate\_dev}, \ttt{speaking\_rate\_mean}) can capture qualitatively different aspects of the same phenomenon (e.g., ``how does speech rate affect VOT?'').  A nice example from recent work explores speaking rate effects on phonetic reduction \citep{priva2018role}: both `faster speakers and `faster speech' matter in explaining degree of reduction (e.g.,\ deleted segments) in spoken English; the authors use this to argue that faster speakers may help initiate sound changes.
%For the vot data considered here: the full model of voiced stops (S et al Table A1) suggests that faster speech and faster speakers both affect VOT for voiced stops, in different ways.

\end{boxedtext}
% 
% 
% - Tie off somehow, with the following points:
% -- (3) could in principle matter for many terms..
% -- In practice endogeneity is usually not considered for linguistic data -- more common in e.g., economics -- not sure whether this is justified.
% -- In practice seems to me most important to do this for predictors of interest for RQs, if they can reasonably vary by-group

% -- Incorporating  group-level averages is not only a way to check endogeneity assumptionl; when adding these predictors *is* justified, can change one's actual findings in interesting ways.  A nice example from recent work is Cohen-Priva \& Gleason, exploring speaking rate effects on phonetic reduction: both ``faster speakers'' and ``faster speech'' matter in explaining degree of reduction (e.g.,\ deleted segments) in spoken English; they use this to argue that faster speakers may help initiate sound changes.
%For the vot data considered here: the full model of voiced stops (S et al Table A1) suggests that faster speech and faster speakers both affect VOT for voiced stops, in different ways.


%% FUTURE: think about whether to go back into this issue in Chapter. 10?
% We will return to (3) in Section~\ref{sec:rand-effects-assumption}; this % assumption has not usually been considered for linguistic data, and requires some introduction.










\section{Other readings}
\label{sec:other-reading-ch8}

Mixed-effects models are widely used in behavioral and social sciences, and there are many full-length treatments and tutorials on specific aspects. Some I have found particularly useful for this chapter are \citet{snijders2011multilevel,zuur2009extensions}, \citet[][Chapter 13]{mcelreath2020statistical}, and especially \citet[][Parts 2--3]{gelman2007data}, and the tutorials below.    Model validation in particular is discussed in  \citet[][chap.~10]{snijders2011multilevel}, \citet[][chap.~8]{fox2019r}, \citet{HLMdiag}.

\citet{meteyard2020best} give a comprehensive best-practices tutorial for using LMMs in psychological sciences in 2020, including a long list of books and tutorials
%, focusing on those which are most 
useful for language scientists.
%psychologists and linguists. 
\citet{brauer2018linear} is another excellent tutorial from this perspective.  (More tutorials in this space appear frequently, such as \citealp{debruine2021understanding,singmann2019introduction}.) Both focus on practical issues, and summarize important primary literature.
%on topics like model selection \citep{barr2013random,matuschek2017balancing}.  
Tutorials by \citet{bolker2009generalized,bolker2015linear} on (G)LMMs, though writtten for ecologists, are excellent.  Many textbooks on statistics for linguists in particular cover mixed models, at different levels of detail (e.g., \citealp[][chap.~7]{baayen2008analyzing}, \citealp[][chap.~6]{gries2021statistics},  \citealp[][chap.~7]{johnson2008quantitative}); \citet{vasishth2021linear} is especially comprehensive, and  \citet[][chap.~14, 15]{winter2019statistics} is especially good for beginning/intermediate readers.   \citet{speelman2018mixed} is an edited volume of case studies which apply mixed-effects models to linguistic data.

  The {lme4} documentation is extensive and contains useful practical advice.
  %such as on model convergence (\ttt{?convergence}) and validity (\ttt{?isSingular}).  
  \citet{pinheiro2000mixed} is an expansive introduction to mixed models in R using the earlier \ttt{lme} package, which typically carries over to {lme4}.  \citet{bates2015fitting} describes the mathematical details and technical implementation of {lme4}. Many questions you may have about {lme4} have been addressed on the
  %somewhere online, often Stack/Stats Overflow or the
  \ttt{r-sig-mixed-models} mailing list.   A very useful page for practical help with {lme4} (and other mixed models in R) is Ben Bolker's `GLMM FAQ' webpage \citep{glmmfaq}, which mostly summarizes answers to questions from this list.




% 
% FUTURE: account for  Appendix 3 of Meteyard \& Davies 2020 which gives a full list of books cited in a survey (!) and suggests that Baayen, PB, GH are indeed most widely used.
% 
% Meteyard \& Davies is a must-read on best practices, state of affairs as of 2020, including lots of details on how LMMs used in practice, and what degrees of freedom to worry about vs not.  Similarly useful: \cite{brauer2018linear}. (Also many, many readings for different levels.)
% 


\section{Exercises}

\exer{\label{ex:neut-1-eff-size}  Calculate effect sizes for all predictors in model \ttt{neut\_mod\_1}, using Cohen's $F$ (e.g., \ttt{cohens\_f} in the {effectsize} package). Does \ttt{voicing} have a small effect, as expected (given the goal of the study)?  If you know a bit about phonetics: Which predictor has the largest effect, and why does this make sense?}

%% In exercise: can calculate Cohen's F for these, e.g., 
%% library(effectsize)
%% cohens_f(neut_mod_1)

\exer{\label{ex:neut-mod3-ranef-dist}
For model \ttt{neut\_mod\_3}, what range of values is the slope of \ttt{voicing} predicted to take across 95\% of participants? Across 95\% of items?}




\exer{\label{ex:vot-mod2-ex} Comparing the output of models \ttt{vot\_mod\_2} and \ttt{vot\_mod2\_more\_slopes}, verify this statement from the text: ``the model with multiple random slopes predicts substantial by-speaker variability in the effect of a following high vowel on VOT, and by-word variability in the effect of speaker gender.  Accordingly,  the standard errors of both fixed-effect terms (\ttt{foll\_high\_vowel}, \ttt{gender}) increase.''}




\exer{\label{ex:vot-rate-slope} Test whether speakers significantly differ in the slope of \ttt{speaking\_rate\_dev} in the VOT dataset, using model \ttt{vot\_mod\_1}.  Try all methods from Section~\ref{sec:lmm-ht-ranef}.  Does it matter which method is used if $\alpha = 0.01$? If $\alpha=0.05$?  Does it seem more likely to you that speakers do or do not differ in the speaking rate effect?}

%% FUTURE: write differently? The point is that if alpha=0.01 then yes it matters -- the most exact test gives p<0.01 .

\exer{\label{ex:vot-mod2-ex2} Check whether adding a by-word random slope of \ttt{voicing} to \ttt{vot\_mod\_2} significantly improves the model (using a likelihood-ratio test on REML fits, with $\alpha=0.2$).
}

\exer{In Exercise~\ref{ex:vot-rate-slope}  we used $\alpha=0.01$ or $\alpha=0.05$, to test a hypothesis of speakers differing in an effect of a predictor. But in Exercise~\ref{ex:vot-mod2-ex2},
%following the text,
we use a higher $\alpha$ value ($\alpha=0.2$) for testing whether a random-slope term should be added to a model. Both tests just involve adding a single random slope term to a model.  Why are these two hypothesis tests conceptually different, and why does it make sense to use a higher $\alpha$ for the second one?}



% anova(vot_mod_1, update(vot_mod_1, . ~ . - (0+speaking_rate_dev|speaker)), refit=FALSE)
% 
% vot_mod_1_justRateSlope <- update(vot_mod_1, . ~ . - (1 | word) -(1|speaker))
% 
% vot_mod_1_noRateSlope <- update(vot_mod_1, . ~ . - (0+speaking_rate_dev|speaker))
% 
% exactRLRT(vot_mod_1_justRateSlope, vot_mod_1, vot_mod_1_noRateSlope)

\exer{\label{ex:kr-pb-repeat} Redo the model comparison in Section~\ref{sec:lmm-fixed-and-random}, assessing whether speaking rate matters for model \ttt{vot\_mod\_2}, using an $F$-test (with Kenward-Roger approximated-$df$) and a parametric bootstrap.  How do the $p$-values from these methods compare to the value from the text, where a less exact method (likelihood-ratio test) is used?}

% This does the problem above:
% 
% <<pb-sim-2, echo=FALSE>>=
% library(parallel)
% nc <- detectCores()
% cl <- makeCluster(rep("localhost", nc))
% 
% f_name4 = 'pb_vot_3.rds'
% f_path4 = paste('objects/', f_name4, sep='')
% do_sim <- !file.exists(f_path4)
% 
% if(do_sim){
%   pb_vot_3 <- PBmodcomp(vot_mod_2, vot_mod_2_noRate, cl=cl)
%   saveRDS(pb_vot_3, file = f_path4)
% } else{
%   pb_vot_3 <- readRDS(f_path4)
% }
% pb_vot_3
% @

%% methods:
%% LRT: p=0.0008
%% PB: p=0.00199

\exer{\label{ex:qq-plot-expl} This exercise addresses the Q-Q plot for model \ttt{vot\_mod\_3} (Section~\ref{sec:lmm-resid-plots}).}

\subexer{Examine a histogram of \ttt{log\_vot} for this data.  You should find a spike in this histogram, around a value of \ttt{vot}=$k$, where the distribution looks different to the left and right of $k$. What is $k$? (Hint: it should be a multiple of 5 msec.)}

\subexer{It turns out that $k$ was an important value in the measurement procedure for this data, which involved VOT being set to $k$ by default, after which annotators could adjust upwards or downwards.  In practice annotators mostly adjusted  upwards, so it is unclear what VOT really is for observations with \ttt{vot}=$k$ or lower.  It is unclear how to deal with this issue, but suppose we took a brute-force approach,  excluding all data with \ttt{vot}$\le k$.  Refit the model for this subset of the data.}

\subexer{Does the Q-Q plot look better for this new model? Do any qualitative conclusions, from the fixed-effect coefficients, differ between the two models?}
% 

% Notes for exercise:
% 
% Examine histogram of log_vot:
% vot_mod_3_df %>% ggplot(aes(x=log_vot)) + geom_histogram()
% 
% why is there a spike, then distribution looks different to the left? trns out this is log(5).  Measurement procedure involved a default of 5, which annotators could adjust up or downwards, but in practice annotators tended to only adjust upwards, so 5 usually meant ``at most 5''.  Unclear how we should deal with this issue, but note that the residuals issue goes away if considering rest of data
% 
% filter(vot_mod_3_df, log_vot>log(5)) %>% ggplot(aes(sample=.resid)) + 
%   geom_qq() + geom_qq_line() + 
%   xlab("Theoretical quantiles") + ylab("Residuals")
% 
% So perhaps this is effectively a good model of VOT for VOT>5

\exer{\label{ex:exclude-words} Follow up on Section \ref{sec:lmm-ranef-distributions}---\ref{sec:lmm-ranef-influence} by checking what effect excluding data from influential words has on \ttt{vot\_mod\_3}. (This includes words whose random intercepts were flagged as outliers.)}

\subexer{As one case, refit the model without observations from the words \tsc{dude} and \tsc{glad}---these were the two words flagged in a Q-Q plot of by-word random intercepts.}
\subexer{Verify this statement from the text: ``the estimated variance of the by-word random intercepts decreases... while its confidence interval narrows... however, the fixed effects (estimates, standard errors) barely change.'' Can you explain why?}
